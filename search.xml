<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>大模型优化--PagedAttention</title>
      <link href="/2024/12/03/da-mo-xing-you-hua-pagedattention/"/>
      <url>/2024/12/03/da-mo-xing-you-hua-pagedattention/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://juejin.cn/post/7290163879287881765">https://juejin.cn/post/7290163879287881765</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> LLM </tag>
            
            <tag> PagedAttention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型优化--KV Cache</title>
      <link href="/2024/12/02/da-mo-xing-you-hua-kv-cache/"/>
      <url>/2024/12/02/da-mo-xing-you-hua-kv-cache/</url>
      
        <content type="html"><![CDATA[<h2 id="KV-Cache"><a href="#KV-Cache" class="headerlink" title="KV Cache"></a>KV Cache</h2><p>KV Cache是Transformer标配的推理加速功能，transformer官方use_cache这个参数默认是True，但是它只能用于Decoder架构的模型，这是因为Decoder有Causal Mask，在推理的时候前面已经生成的字符不需要与后面的字符产生attention，从而使得前面已经计算的K和V可以缓存起来。</p><p>我们先看一下不使用KV Cache的推理过程。假设模型最终生成了“遥遥领先”4个字。</p><p>当模型生成第一个“遥”字时，<code>input="&lt;s&gt;"</code>, <code>"&lt;s&gt;"</code>是起始字符。Attention的计算如下：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202155542.png"></p><p>为了看上去方便，我们暂时忽略scale项$1/\sqrt(d)$， 但是要注意这个scale面试时经常考。</p><p>如上图所示，最终Attention的计算公式如下，（softmaxed 表示已经按行进行了softmax）:<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202155748.png"></p><p>当模型生成第二个“遥”字时，<code>input="&lt;s&gt;遥"</code>, Attention的计算如下：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202155837.png"></p><p>当QK变为矩阵时，softmax 会针对 行 进行计算。写详细一点如下，softmaxed 表示已经按行进行了softmax。</p><p><strong>（关键）由于decoder架构的模型有Causal Mask，所以$Q_1$与$K_2$的计算结果为$-\infty$。</strong><br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202204856.png"></p><p>假设$Att_1$表示 Attention 的第一行， $Att_2$表示 Attention 的第二行，则根据上面推导，其计算公式为：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202205002.png"></p><p>我们发现：</p><ul><li>$Q_1$在第二步参与的计算与第一步是一样的，而且第二步生成的$Att_1$仅仅依赖于$Q_1$，与$Q_2$毫无关系。</li><li>$Att_2$仅仅依赖于$Q_2$，与$Q_1$毫无关系。</li></ul><p>当模型生成第三个“领”字时，<code>input="&lt;s&gt;遥遥"</code>, Attention的计算如下：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202210952.png"><br>详细的推导参考第二步，其计算公式为：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202211018.png"><br>同样的，第三步生成的$Att_3$仅仅依赖于$Q_3$，与$Q_1$和$Q_2$毫无关系。</p><p>当模型生成第四个“先”字时，<code>input="&lt;s&gt;遥遥领"</code>, Attention的计算如下：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202211156.png"><br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202211222.png"><br>和之前类似，不再赘述。</p><p>看上面图和公式，我们可以得出结论：</p><ul><li>当前计算方式存在大量冗余计算</li><li>$Attn_k$只与$Q_k$有关</li><li>推理第$x_k$个字符时，只需要输入字符$x_{k-1}$即可。<br>第三个结论的前提是，我们需要把每一步的K和V缓存起来，这样在推理第$x_k$个字符时，只需要输入字符$x_{k-1}$计算其$Q_k,K_k,V_k$, 结合之前保存的KV Cache即可得到对应的$Attn_k$。</li></ul><p>下图展示了使用KV Cache和不使用KV Cache的过程对比：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202211901.png"></p><p><a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L318C1-L331C97">huggingface 的实现</a></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">if layer_past is not None:        past_key, past_value = layer_past        key = torch.cat((past_key, key), dim=-2)        value = torch.cat((past_value, value), dim=-2)        if use_cache is True:        present = (key, value)    else:        present = None        if self.reorder_and_upcast_attn:        attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)    else:        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)</code></pre><h2 id="MQA与GQA"><a href="#MQA与GQA" class="headerlink" title="MQA与GQA"></a>MQA与GQA</h2><p> 参考：</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/708120479">https://zhuanlan.zhihu.com/p/708120479</a></p></blockquote><blockquote><p>大模型推理加速：看图学KV Cache <a href="https://zhuanlan.zhihu.com/p/662498827">https://zhuanlan.zhihu.com/p/662498827</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> LLM </tag>
            
            <tag> KV Cache </tag>
            
            <tag> MQA </tag>
            
            <tag> GQA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型显存占用分析</title>
      <link href="/2024/11/30/da-mo-xing-xian-cun-zhan-yong-fen-xi/"/>
      <url>/2024/11/30/da-mo-xing-xian-cun-zhan-yong-fen-xi/</url>
      
        <content type="html"><![CDATA[<h1 id="大模型消耗的显存"><a href="#大模型消耗的显存" class="headerlink" title="大模型消耗的显存"></a>大模型消耗的显存</h1><p>在详细说明大模型需要消耗的显存大小之前我们需要先明确几个概念。<br>一个就是大模型在不同阶段对显存的消耗是不同的。但是大致可以分为三个阶段或者说三个场景。即大模型<strong>预训练阶段</strong>、大模型<strong>微调阶段</strong>和大模型<strong>推理阶段</strong>。</p><ul><li>在<strong>预训练阶段</strong>，大模型通常选择较大规模的数据集获取泛化能力，因此需要较大的批次等来保证模型的训练强大。而模型的权重也是从头开始计算，因此通常也会选择高精度（如32位浮点数）进行训练。需要消耗大量的GPU显存资源。</li><li>在<strong>微调阶段</strong>，通常会冻结大部分参数，只训练小部分参数。同时，也会选择非常多的优化技术和较少的高质量数据集来提高微调效果，此时，由于模型已经在预训练阶段进行了大量的训练，微调时的数值误差对模型的影响通常较小。也常常选择16位精度或者混合精度训练。因此通常比预训练阶段消耗更低的显存资源。</li><li>在<strong>推理阶段</strong>，通常只是将一个输入数据经过模型的前向计算得到结果即可，因此需要最少的显存即可运行。</li></ul><h3 id="模型权重"><a href="#模型权重" class="headerlink" title="模型权重"></a>模型权重</h3><p>这部分显存用于存储神经网络模型的参数，包括权重（weights）和偏置（biases）。模型内存是模型在训练和推理过程中都需要的，因为它包含了模型的结构和学习到的知识。在训练过程中，模型内存的大小通常与模型的复杂度和参数数量成正比。</p><h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><p>在模型训练反向传播（Backward）过程中，计算的梯度所占的显存大小。梯度内存的大小与模型的参数数量有关，因为每个参数都需要计算对应的梯度。</p><h3 id="优化器状态"><a href="#优化器状态" class="headerlink" title="优化器状态"></a>优化器状态</h3><p>优化器内存用于存储优化器状态，这通常包括梯度的一阶和二阶矩（如在Adam优化器中使用的均值和方差估计）优化器内存的大小取决于所使用的优化器类型。例如，Adam优化器需要额外的内存来存储梯度的一阶和二阶矩，而SGD只需要存储梯度信息，无其他优化器内存占用。</p><h3 id="激活值"><a href="#激活值" class="headerlink" title="激活值"></a>激活值</h3><p>激活内存用于存储神经网络在前向传播过程中计算的中间激活值。这些激活值在反向传播过程中需要被重用，以计算关于模型参数的梯度。激活内存的大小与网络的深度和输入数据大小（batch size）有关。更深的网络和更大的 batch size 会导致更大的激活内存需求。</p><h1 id="数据精度"><a href="#数据精度" class="headerlink" title="数据精度"></a>数据精度</h1><p>想要计算显存，从“原子”层面来看，就需要知道我们的使用数据的精度，因为精度代表了数据存储的方式，决定了一个数据占多少bit。对于一个1B参数的模型，如果使用FP32精度存储，那么模型权重占用的显存就是1B * 2 = 2GB。</p><h3 id="常见精度类型"><a href="#常见精度类型" class="headerlink" title="常见精度类型"></a>常见精度类型</h3><p>浮点数主要是由符号位（sign）、指数位（exponent）和小数位（mantissa）三部分组成。 符号位都是1位（0表示正，1表示负），指数位影响浮点数范围，小数位影响精度。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202035151.png"></p><ul><li>FP32：32位浮点数，每个数据占4字节</li><li>TF32：<strong>19位浮点数</strong>，每个数据占2字节</li><li>FP16：16位浮点数，每个数据占2字节</li><li>BF16：16位浮点数，每个数据占2字节</li><li>Int8：8位整数，每个数据占1字节</li><li>Int4：4位整数，每个数据占0.5字节</li></ul><h3 id="混合精度训练AMP"><a href="#混合精度训练AMP" class="headerlink" title="混合精度训练AMP"></a>混合精度训练AMP</h3><p>较低模型精度对于运算效率和显存占用都更友好，但是如果直接使用FP16精度在训练过程中会出现很多问题：</p><ul><li>underflow：梯度再乘以学习率会很小，无法用fp16表示</li><li>rounding error：fp16各个区间之间存在gap，即使梯度可以用fp16表示，但是也没有把法加在fp16的权重上（被舍去）</li><li>模型预测准确度降低</li></ul><h4 id="FP32权重备份：解决舍入误差问题"><a href="#FP32权重备份：解决舍入误差问题" class="headerlink" title="FP32权重备份：解决舍入误差问题"></a>FP32权重备份：解决舍入误差问题</h4><p>保留一份FP32的主权重（Master-Weights），同时在训练中使用FP16存储权重、激活、梯度等数据。在参数更新的过程汇总，用FP16更新FP32的主权重。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202040432.png"></p><p>Step1:优化器会先备份一份FP32精度的模型权重，初始化好FP32精度的一阶和二阶动量（用于更新权重）。</p><p>Step2:开辟一块新的存储空间，将FP32精度的模型权重转换为FP16精度的模型权重。</p><p>Step3:运行forward和backward，产生的梯度和激活值都用FP16精度存储。</p><p>Step4:优化器利用FP16的梯度和FP32精度的一阶和二阶动量去更新备份的FP32的模型权重。</p><p>Step5:重复Step2到Step4训练，直到模型收敛。</p><p>我们可以看到训练过程中显存主要被用在四个模块上：</p><ul><li>模型权重本身（FP32+FP16）</li><li>梯度（FP16）</li><li>优化器（FP32）</li><li>激活值（FP16）</li></ul><p>写到这里，我们应该对于分析大模型训练时候的显存问题应该不在话下了（除了动态部分），那么我们就来实测一下，正在阅读的小伙伴也可以先自己尝试计算一下，看看是不是真的懂了。 对于llama3.1 8B模型，FP32和BF16混合精度训练，用的是AdamW优化器，请问模型训练时占用显存大概为多少？</p><p>解：</p><p>模型参数：16（BF16） + 32（PF32）= 48G</p><p>梯度参数：16（BF16）= 16G</p><p>优化器参数：32（PF32） + 32（PF32）= 64G</p><p>不考虑激活值的情况下，总显存大约占用 （48 + 16 + 64） = 128G</p><h4 id="损失缩放：解决数据下溢问题"><a href="#损失缩放：解决数据下溢问题" class="headerlink" title="损失缩放：解决数据下溢问题"></a>损失缩放：解决数据下溢问题</h4><p>当采用FP16而不是FP32更新梯度时，由于值太小，会造成FP16精度下数据下溢的问题，一些梯度会变为0，导致模型不收敛。故采用在前向过程结束后对损失进行放大，在反向过程结束后对梯度进行缩小。<br>损失缩放可以有两种主要方式：静态损失缩放和动态损失缩放。</p><ul><li>静态损失缩放：在训练开始前，设置一个固定的缩放因子，在训练过程中保持不变。</li><li>动态损失缩放：在训练过程中，根据损失值的大小动态调整缩放因子。<ul><li>如果在某轮训练中检测到梯度正常且没有溢出，缩放因子会逐渐增大。</li><li>如果检测到梯度出现 NaN 或 Inf，则缩放因子减小以防止数值不稳定。</li></ul></li></ul><h4 id="精度累加"><a href="#精度累加" class="headerlink" title="精度累加"></a>精度累加</h4><p>此外，研究者还发现，可以在模型训练的过程中，使用FP16进行乘法预算，使用FP32进行累加运算，并将FP32转换为FP16存储。FP32可以弥补损失的精度，减少舍入误差。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202042004.png"><br>如英伟达Volta架构中的Tensor Core可以使用FP16混合精度进行加速，采用的是FP16的矩阵乘法，得出全精度乘积，然后使用FP32累加，将该乘积与其他中间乘积累加，减少因FP16带来的精度损失。</p><h4 id="更为动态的精度缩放方法"><a href="#更为动态的精度缩放方法" class="headerlink" title="更为动态的精度缩放方法"></a>更为动态的精度缩放方法</h4><p>在英伟达最新的Hopper架构GPU中，英伟达的Tensor Core能够自动根据所需的精度进行动态的数据缩放调整，特别是针对Transformer网络架构，能够在数据存入内存前，根据需求改变各种参数精度。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202042340.png"><br>Hopper白皮书内容如下：</p><blockquote><p>在 Transformer 模型的每一层，Transformer Engine 都会分析 Tensor Core 产生的输出值的统计数据。了解了接下来会出现哪种类型的神经网络层以及它需要什么精度后，Transformer Engine 还会决定将张量转换为哪种目标格式，然后再将其存储到内存中。 FP8 的范围比其他数字格式更有限。为了优化使用可用范围，Transformer Engine 还使用从张量统计数据计算的缩放因子动态地将张量数据缩放到可表示的范围内。因此，每一层都在其所需的范围内运行，并以最佳方式加速。</p></blockquote><h1 id="其他显存占用"><a href="#其他显存占用" class="headerlink" title="其他显存占用"></a>其他显存占用</h1><ul><li>KV Cache：在推理过程中，大模型需要缓存一些中间结果，以便在处理下一个输入时重用。这些缓存的结果通常称为KV Cache。KV Cache占用的显存大小与模型的层数、序列长度和每个序列的token数量有关。</li><li>显存碎片：显存碎片是指显存中未被使用的空闲空间，这些空闲空间可能无法被有效利用，导致显存利用率降低。paged attention机制可以有效减少显存碎片。</li></ul><h3 id="推理与KV-cache-显存"><a href="#推理与KV-cache-显存" class="headerlink" title="推理与KV cache 显存"></a>推理与KV cache 显存</h3><p>推理的时候，显存几乎只考虑模型参数本身，除此之外就是现在广泛使用的KV cache也会占用显存。KV cache与之前讲的如何减少显存不一样，KV cache的目的是减少延迟，也就是<strong>为了推理的速度牺牲显存</strong>。</p><h4 id="kv-cache介绍"><a href="#kv-cache介绍" class="headerlink" title="kv cache介绍"></a>kv cache介绍</h4><p>具体可以参考另一篇博客：<a href="https://baoblei.github.io/2024/12/02/da-mo-xing-you-hua-kv-cache/">大模型优化–KV Cache</a><br>KV Cache是Transformer标配的推理加速功能，transformer官方use_cache这个参数默认是True，但是它只能用于Decoder架构的模型，这是因为Decoder有Causal Mask，在推理的时候前面已经生成的字符不需要与后面的字符产生attention，从而使得前面已经计算的K和V可以缓存起来。</p><p>下图展示了使用KV Cache和不使用KV Cache的过程对比：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202211901.png"><br>从图中，我们可以得出结论：</p><ul><li>当前计算方式存在大量冗余计算</li><li>$Attn_k$只与$Q_k$有关</li><li>推理第$x_k$个字符时，只需要输入字符$x_{k-1}$即可。<br>第三个结论的前提是，我们需要把每一步的K和V缓存起来，这样在推理第$x_k$个字符时，只需要输入字符$x_{k-1}$计算其$Q_k,K_k,V_k$, 结合之前保存的KV Cache即可得到对应的$Attn_k$。</li></ul><p><a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L318C1-L331C97">huggingface 的实现</a></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">if layer_past is not None:        past_key, past_value = layer_past        key = torch.cat((past_key, key), dim=-2)        value = torch.cat((past_value, value), dim=-2)        if use_cache is True:        present = (key, value)    else:        present = None        if self.reorder_and_upcast_attn:        attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)    else:        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)</code></pre><h4 id="KV-Cache显存占用"><a href="#KV-Cache显存占用" class="headerlink" title="KV Cache显存占用"></a>KV Cache显存占用</h4><p>当<strong>sequence特别长</strong>的时候，KV Cache其实还是个Memory刺客。</p><p>对于fp16精度保存的KV Cache，其占用的显存大小为：<br>$$<br>memory = batch_size * hidden_size * seq_length * layer * 2 * 2<br>$$<br>其中两个2分别表示K和V，fp16精度字节数。</p><p>比如llama 7B模型，batch_size=32, layer=32, dim_size=4096, seq_length=2048, float32类型，则需要占用的显存为 2 * 32 * 4096 * 2048 * 32 * 4 / 1024/1024/1024 = 64G。</p><p>为了解决KV Cache显存占用问题，研究者提出了<strong>MQA和GQA</strong>。其核心思想是：<strong>共享多头KV Cache</strong>。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241203015549.png"><br>以GQA为例，我们将hidden_size维度切分为head*head_dim，然后将多个head分成group组，每个group共享一个KV。则总的KV Cache显存占用为：<br>$$<br>memory = batch_size * group * head_dim * seq_length * layer *  2 * 2<br>$$<br>而MQA则是group=1，即每个head单独保存一个KV。</p><blockquote><p>大模型推理加速：看图学KV Cache <a href="https://zhuanlan.zhihu.com/p/662498827">https://zhuanlan.zhihu.com/p/662498827</a></p></blockquote><h3 id="LoRA-与-QLoRA-训练显存"><a href="#LoRA-与-QLoRA-训练显存" class="headerlink" title="LoRA 与 QLoRA 训练显存"></a>LoRA 与 QLoRA 训练显存</h3><h1 id="huggingface-显存分析工具"><a href="#huggingface-显存分析工具" class="headerlink" title="huggingface 显存分析工具"></a>huggingface 显存分析工具</h1><p>huggingface 提供了一个工具可以方便的查看大模型在不同阶段消耗的显存大小。<br><a href="https://huggingface.co/docs/accelerate/usage_guides/model_size_estimator">model size estimator</a></p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><blockquote><p><a href="https://blog.zhexuan.org/archives/llm-gpu-memory.html">https://blog.zhexuan.org/archives/llm-gpu-memory.html</a></p></blockquote><blockquote><p><a href="https://juejin.cn/post/7352387675837480995">https://juejin.cn/post/7352387675837480995</a></p></blockquote><blockquote><p><a href="https://gitcode.csdn.net/662a062ca2b051225566cf63.html">https://gitcode.csdn.net/662a062ca2b051225566cf63.html</a></p></blockquote><blockquote><p><a href="https://hub.baai.ac.cn/view/16045">https://hub.baai.ac.cn/view/16045</a></p></blockquote><blockquote><p><a href="https://zhuanlan.zhihu.com/p/624740065">https://zhuanlan.zhihu.com/p/624740065</a></p></blockquote><blockquote><p>NVIDIA H100 Tensor Core GPU Architecture：<a href="https://nvdam.widen.net/s/9bz6dw7dqr/gtc22-whitepaper-hopper">https://nvdam.widen.net/s/9bz6dw7dqr/gtc22-whitepaper-hopper</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> LLM </tag>
            
            <tag> KV Cache </tag>
            
            <tag> AI </tag>
            
            <tag> 大模型 </tag>
            
            <tag> LoRA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo博客搭建</title>
      <link href="/2024/11/28/hexo-zhu-ti-next-pei-zhi/"/>
      <url>/2024/11/28/hexo-zhu-ti-next-pei-zhi/</url>
      
        <content type="html"><![CDATA[<h1 id="基础搭建"><a href="#基础搭建" class="headerlink" title="基础搭建"></a>基础搭建</h1><h2 id="Hexo"><a href="#Hexo" class="headerlink" title="Hexo"></a>Hexo</h2><p>选一个博客框架，hexo是静态网站框架，基于nodejs，可以生成静态网页，部署到github上。<br>需要提前安装<strong>git</strong>，<strong>nodejs</strong>。</p><ul><li>check git version: <code>git --version</code></li><li>check nodejs version: <code>node -v</code></li><li>check npm version: <code>npm -v</code><br>安装 cnmp (optional) </li><li><code>npm install -g cnpm --registry=https://registry.npm.taobao.org</code></li><li>check cnmp version: <code>cnpm -v</code></li></ul><p>安装 <strong>hexo-cli</strong></p><ul><li><code>cnpm install -g hexo-cli</code></li><li>check hexo version: <code>hexo -v</code></li></ul><h2 id="初始化blog目录"><a href="#初始化blog目录" class="headerlink" title="初始化blog目录"></a>初始化blog目录</h2><ul><li><code>mkdir blog | cd blog</code></li><li><code>sudo hexo init</code>    # clone hexo-starter repo</li><li><code>hexo s</code>            # start blog, default port 4000</li></ul><h2 id="添加博客"><a href="#添加博客" class="headerlink" title="添加博客"></a>添加博客</h2><ul><li><code>hexo n "newblog"</code>  # add new blog, save in /source/_posts<ul><li>edit blog in /source/_posts/newblog.md</li></ul></li><li><code>hexo clean</code>        # clean cache</li><li><code>hexo g</code>            # generate static files</li><li><code>hexo s</code>            # start blog, default port 4000</li></ul><h2 id="github部署"><a href="#github部署" class="headerlink" title="github部署"></a>github部署</h2><ul><li>build blog address:<ul><li>add new token in github</li><li>github new repo: https://<username>.github.io/</username></li></ul></li><li><code>cnpm install hexo-deployer-git --save</code>  <pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml">deploy:    type: git    repo: https://&lt;token&gt;@github.com/&lt;username&gt;/&lt;username&gt;.github.io.git    branch: master</code></pre></li><li><code>hexo d</code>            # deploy blog</li></ul><h1 id="主题"><a href="#主题" class="headerlink" title="主题"></a>主题</h1><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><ul><li>theme 推荐：<ul><li><a href="https://github.com/litten/hexo-theme-yilia.git">https://github.com/litten/hexo-theme-yilia.git</a></li><li><a href="https://github.com/theme-next/hexo-theme-next.git">https://github.com/theme-next/hexo-theme-next.git</a></li></ul></li><li>clone theme into themes folder, e.g. <code>git clone https://github.com/theme-next/hexo-theme-next.git themes/next</code></li><li>edit _config.yml, set theme to next  <pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml">theme: next</code></pre></li><li><code>hexo c</code>&amp;&amp;<code>hexo g</code>&amp;&amp;<code>hexo d</code>            # clean cache, generate static files, deploy blog</li></ul><h2 id="配置主题"><a href="#配置主题" class="headerlink" title="配置主题"></a>配置主题</h2><p>大部分配置在<code>_config.yml</code> 文件中，可以参考<a href="https://theme-next.js.org/docs/getting-started/">官方文档</a></p><h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><h4 id="scheme"><a href="#scheme" class="headerlink" title="scheme"></a>scheme</h4><ul><li>Scheme 是 NexT 提供的一种特性，借助于 Scheme，NexT 为你提供多种不同的外观。同时，几乎所有的配置都可以 在 Scheme 之间共用。</li><li>muse - 默认 Scheme，这是 NexT 最初的版本，黑白主调，大量留白</li><li>Mist - Muse 的紧凑版本，整洁有序的单栏外观</li><li>Pisces - 双栏 Scheme，小家碧玉似的清新<br>Scheme 的切换通过更改 <code>_config.yml</code> 文件，搜索 scheme 关键字。 你会看到有三行 scheme 的配置，将你需用启用的 scheme 前面注释 # 去除即可。</li></ul><h4 id="language"><a href="#language" class="headerlink" title="language"></a>language</h4><h4 id="menu"><a href="#menu" class="headerlink" title="menu"></a>menu</h4><p>菜单配置包括三个部分，第一是菜单项（名称和链接），第二是菜单项的显示文本，第三是菜单项对应的图标。 NexT 使用的是 Font Awesome 提供的图标，可以在 <a href="https://fontawesome.com/icons">Font Awesome</a> 查看。</p><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml">menu:  home: / || home  archives: /archives || archive  tags: /tags || tags  categories: /categories || th</code></pre><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>主题美化是个逐渐积累的过程, 后期可以在相册、挂件、评论、搜索、sitemap、rss等方面进行配置。<br>一些工具插件：<br><strong>gallery page</strong>–justified gallery</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> next </tag>
            
            <tag> 静态网站搭建 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
