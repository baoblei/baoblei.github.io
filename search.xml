<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>个人博客加入谷歌元标记与站点地图</title>
      <link href="/2024/12/04/ge-ren-bo-ke-jia-ru-gu-ge-yuan-biao-ji-yu-zhan-dian-di-tu/"/>
      <url>/2024/12/04/ge-ren-bo-ke-jia-ru-gu-ge-yuan-biao-ji-yu-zhan-dian-di-tu/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在建立好我们的博客或者个人网站后，需要再让自己的链接被搜索引擎所收录。这里有两种方法能够被搜索引擎添加自己网站的索引。一个是自己努力提高自己的网站知名度，让搜索引擎主动去添加索引。另外一种就是自己把自己的链接添加到搜索引擎的索引当中。</p><h2 id="查看是否收录"><a href="#查看是否收录" class="headerlink" title="查看是否收录"></a>查看是否收录</h2><p>在google或者百度等搜索引擎中搜索<code>site:website addres</code>，查看是否已经被收录。</p><p>如果搜索不出来自己等网站，那么就需要自己添加到搜索引擎的索引当中。下面以google为例，介绍如何添加自己的网站到google的索引当中。</p><h2 id="添加谷歌元标记"><a href="#添加谷歌元标记" class="headerlink" title="添加谷歌元标记"></a>添加谷歌元标记</h2><p>首先你需要一个谷歌账号，然后使用<a href="https://search.google.com/search-console/about">Google Search Console</a>服务，点击立即使用！</p><p>接下来你可以看到如下的验证界面:<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205035222.png"></p><p>这里选择前缀，然后输入自己网站地址。之后按照提示添加meta标签到自己的网站中。对于hexo博客，可以在<code>themes/next/layout/_partials/head.ejs</code>文件中添加。</p><p>在添加完元标记后，进行网址检查，如果显示网址未收录，点击右下角的请求编入索引，别着急，一般来说一到两天后Google就会收录。</p><h2 id="添加站点地图"><a href="#添加站点地图" class="headerlink" title="添加站点地图"></a>添加站点地图</h2><h3 id="生成站点地图"><a href="#生成站点地图" class="headerlink" title="生成站点地图"></a>生成站点地图</h3><ul><li>安装sitemap插件：<code>npm install hexo-generator-sitemap --save</code></li><li>生成站点地图：<code>hexo generate</code>，然后在<code>public</code>目录下会生成<code>sitemap.xml</code>文件</li><li>修改配置：<ul><li>在<code>_config.yml</code>文件中添加如下配置  <pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml">Plugins:    - hexo-generator-sitemapsitemap:    path: /sitemap.xml</code></pre></li><li>在主题配置文件中添加如下配置  <pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml">menu:...sitemap: /sitemap.xml || fa fa-sitemap...</code></pre></li></ul></li></ul><h4 id="提交站点地图"><a href="#提交站点地图" class="headerlink" title="提交站点地图"></a>提交站点地图</h4><p>还是Google Search Console，点击左侧边栏中的站点地图，添加新的站点地图，在主站地址后面填入sitemap.xml，即与前面生成的站点地图文件名称相同！</p><p>等待Googlec处理，一般来说一到两天后Google就会收录。</p><blockquote><p><a href="https://blog.csdn.net/tzhuwb/article/details/125477001">https://blog.csdn.net/tzhuwb/article/details/125477001</a><br><a href="https://mizeri.github.io/2021/04/18/hexo-sitemap-google/">https://mizeri.github.io/2021/04/18/hexo-sitemap-google/</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> sitemap </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型高效微调方法PEFT--LoRA/QLoRA</title>
      <link href="/2024/12/04/da-mo-xing-gao-xiao-wei-diao-fang-fa-peft-lora-qlora/"/>
      <url>/2024/12/04/da-mo-xing-gao-xiao-wei-diao-fang-fa-peft-lora-qlora/</url>
      
        <content type="html"><![CDATA[<h2 id="参数高效微调PEFT"><a href="#参数高效微调PEFT" class="headerlink" title="参数高效微调PEFT"></a>参数高效微调PEFT</h2><h4 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h4><p>微调（Fine-tuning）是一种迁移学习的技术，用于在一个已经预训练好的模型基础上，通过进一步训练来适应特定的任务或数据集。微调可以在具有相似特征的任务之间共享知识，从而加快训练速度并提高模型性能。</p><p>以下是一般的微调步骤：</p><ul><li><strong>选择择预训练模型</strong>：选择一个在大规模数据集上预训练好的模型，如ImageNet上的预训练的卷积神经网络（如ResNet、VGG等）。这些模型通常具有良好的特征提取能力。</li><li><strong>冻结底层权重</strong>：将预训练模型的底层权重（通常是卷积层）固定住，不进行训练。这是因为底层权重通常学习到了通用的特征，可以被用于许多不同的任务。</li><li><strong>替换顶层分类器</strong>：将预训练模型的顶层分类器（通常是全连接层）替换为适合特定任务的新的分类器。新的分类器的输出节点数量应该与任务的类别数相匹配。</li><li><strong>解冻部分权重（可选）</strong>：根据任务的复杂性和可用的训练数据量，可以选择解冻一些底层权重，以便更好地适应新的任务。这样可以允许底层权重进行微小的调整，以更好地适应新任务的特征。</li><li><strong>进行训练</strong>：使用特定任务的训练数据集对新的分类器进行训练。可以使用<strong>较小的学习率</strong>进行训练，以避免对预训练模型的权重进行过大的更新。</li><li><strong>评估和调整</strong>：在训练完成后，使用验证集或测试集评估模型的性能。根据评估结果，可以进行调整，如调整学习率、调整模型结构等。</li></ul><p>微调的关键是在预训练模型的基础上进行训练，从而将模型的知识迁移到特定任务上。通过这种方式，可以在较少的数据和计算资源下，快速构建和训练高性能的模型。</p><h4 id="PEFT"><a href="#PEFT" class="headerlink" title="PEFT"></a>PEFT</h4><p>PEFT（Parameter-Efficient Fine-Tuning，参数高效微调）是一种在大语言模型（Large Language Model，LLM）上进行微调的新技术，旨在降低微调大模型的计算和存储成本，使得微调过程更为高效。与传统的微调技术相比，PEFT 可以显著减少训练所需的参数量，并保持与标准微调相似的性能。PEFT 技术在优化资源效率、降低硬件需求、并提高开发灵活性方面有显著优势，特别是在需要频繁微调以适应不同任务或领域的场景中尤为有用。</p><p>目前主流的方法包括2019年 Houlsby N 等人提出的 Adapter Tuning，2021年微软提出的 LORA，斯坦福提出的 Prefix-Tuning，谷歌提出的 Prompt Tuning，2022年清华提出的 P-tuning v2。</p><ul><li>Adapter Tuning 在模型的某些层之间插入小的适配器模块，而不对原有的大量参数进行修改。适配器模块通常包含一小部分可训练的参数，并通过这些参数来调整模型的输出；但是增加了模型层数，引入了额外的推理延迟</li><li>Prefix-Tuning 每个 Transformer 层前面添加一个可学习的前缀（prefix）来实现模型的调整。这个前缀是固定长度的，可视作特定任务的提示（prompt）；但是预留给 Prompt 的序列挤占了下游任务的输入序列空间，影响模型性能</li><li>P-tuning v2 使用可学习的嵌入来替代硬编码的提示文本；但是很容易导致旧知识遗忘，微调之后的模型，在之前的问题上表现明显变差</li><li>基于上述背景，LORA 得益于前人的一些关于内在维度（intrinsic dimension）的发现：</li></ul><blockquote><p>模型是过参数化的，它们有更小的内在维度，模型主要依赖于这个低的内在维度（low intrinsic dimension）去做任务适配。</p></blockquote><p>假设模型在任务适配过程中权重的改变量是低秩（low rank）的，由此提出<strong>低秩自适应</strong>（LoRA）方法。</p><p>LoRA 允许我们通过优化适应过程中密集层变化的秩分解矩阵，来间接训练神经网络中的一些密集层，同时保持预先训练的权重不变。</p><h2 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h2><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241204190635.png"><br>LoRA 的思想很简单:</p><ul><li><p>在原始 PLM (Pre-trained Language Model) 旁边增加一个旁路，做一个降维再升维的操作，来模拟所谓的intrinsic rank。</p></li><li><p>训练的时候固定 PLM 的参数，只训练降维矩阵 与升维矩阵。而模型的输入输出维度不变，输出时将与 PLM 的参数叠加。</p></li><li><p>用随机高斯分布初始化 A，用 0 矩阵初始化 B，保证训练的开始此旁路矩阵依然是 0 矩阵。</p><blockquote><p>为什么A用随机高斯分布初始化，B用0矩阵初始化呢？</p></blockquote></li><li><p>首先增量矩阵$\Delta W = BA$ 在训练刚开始肯定是要用0来初始化。</p></li><li><p>如果A用0矩阵初始化，在计算梯度时由于B的梯度有一个因子A，导致B的梯度始终为0，无法被更新，导致梯度传播受阻。</p></li></ul><p>我们也可以理解为因为A是降维矩阵，所以它能够学到一些有用的信息，所以用随机高斯分布初始化；而B是升维矩阵，我们希望它能够保持原始信息，所以用0矩阵初始化。</p><p><strong>秩的选择</strong><br>在 Transformer 中，LoRA 主要作用在 attention 过程的四个权重矩阵W_Q, W_K, W_V, W_O（也可以选择其中部分）。A，B矩阵的秩r也时远远小于原始的权重大小。<br>对于一般的任务， r=1，2，4，8就足够了。而一些领域差距比较大的任务可能需要更大的r。<br>同时，增加r值变大并不能提升微调的效果，这可能是因为参数量增加需要更多的语料.<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241204204545.png"></p><p><strong>实现</strong><br>huggingface:<br><a href="https://huggingface.co/docs/peft/main/conceptual_guides/lora">https://huggingface.co/docs/peft/main/conceptual_guides/lora</a><br>llama_factory:<br><a href="https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/sft.html">https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/sft.html</a><br>关键参数：</p><ul><li>lora_r：秩，控制低秩矩阵的表示能力。</li><li>lora_alpha：缩放因子，控制 LoRA 的影响力。</li><li>lora_dropout：dropout 概率，防止过拟合。</li><li>lora_target_modules：指定微调的目标层。</li><li>lora_trainable_only：是否只训练 LoRA 增加的部分。</li><li>lora_init_scale：控制低秩矩阵初始化的尺度。</li><li>lora_layers_to_freeze：指定冻结哪些层。</li><li>lora_lr：设置 LoRA 的学习率。</li><li>lora_warmup_steps：指定热身阶段的步数。</li></ul><h2 id="QLoRA"><a href="#QLoRA" class="headerlink" title="QLoRA"></a>QLoRA</h2><p>QLoRA（Quantized Low-Rank Adapter）是一种高效的微调方法，是 LoRA 的量化版本（什么是 LoRA？）。该调优方法由华盛顿大学发表于论文《QLORA: Efficient Finetuning of Quantized LLMs》。通过降低内存使用，实现在单个 GPU 上对大型语言模型进行微调。它可以在单个 48GB GPU 上微调 650 亿个参数的模型，并且能够保持完整的 1 6 位微调任务性能。</p><p>QLoRA 核心是使用量化进行微调。其成果有三个。</p><ul><li>4 位标准浮点数量化（4-bit NormalFloat Quantization）</li><li>双重量化（Double Quantization）</li><li>分页优化（Paged Optimizer）就是显存不足时将部分梯度检查点转移到内存上。</li></ul><p>为了降低极端的量化对微调造成的不利影响，<strong>QLoRA 借助 LoRA 对原模型权重进行更新</strong>，且 LoRA 侧的权重保持高精度。这就是 QLoRA 与 LoRA 的关系。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241204231609.png"></p><h3 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h3><h4 id="量化"><a href="#量化" class="headerlink" title="量化"></a>量化</h4><p><strong>量化</strong>指的是将连续或高精度的数值转换为较低精度（比如较少的位数）的表示形式的过程。这通常用于减少模型的存储需求和加快其运算速度。例如，将一个 FP32 的 tensor 转成 Int8:<br>$$X^{\text{Int8}} = \text{round}\left(\frac{127}{\text{absmax}(X^{\text{FP32}})} \cdot X^{\text{FP32}}\right)=round(c^{\text{FP32}} \cdot X^{\text{FP32}})$$ </p><p>其中，c 为量化常数，通常是这个张量的特征的绝对值的最大值。逆量化公式则为：<br>$$\text{dequant}(c^{\text{FP32}}, X^{\text{Int8}}) = \frac{X^{\text{Int8}}}{c^{\text{FP32}}}$$</p><p>按照量化过程是否以0点为对称点量化又可以分为对称量化和非对称量化。其中对称量化将原浮点数的最小或最大值的绝对值作为映射值的范围，而非对称量化是将原浮点数的最小和最大值映射为量化数据的最小和最大值。在非对称量化中，0的映射也可能会有偏移，因此不一定会被映射到0。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205004225.png"></p><h4 id="分位数量化"><a href="#分位数量化" class="headerlink" title="分位数量化"></a>分位数量化</h4><p>分位数量化（Quantile Quantization）是隶属于非线性量化。分位数（Quantile）在数学上的定义指的是把顺序排列的一组数据分割为若干个相等块的分割点的数值。在标准正态分布中，对于分布X给定的概率$\alpha$，如果存在$\mu_\alpha$使得它的累积分布函数（CDF） $P(X &lt; \mu_\alpha)= \alpha$，则称 $\mu_\alpha$是标准正态分布的 $\alpha$分位数，因为CDF表示的是概率值小于$\mu_\alpha$的阴影部分的面积，因此具有严格递增的特性，所以它一定存在反函数。CDF的反函数的一个重要作用是用来生成服从该随机分布的随机变量。假设a是[0,1)区间上均匀分布的一个随机变量，那么 $F_{X}^{-1}(a)$服从分布$X$。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205004852.png"></p><p><strong>分位数量化是通过分位数将张量分成了大小相同的若干个块，这样我们得到更加均匀的量化特征</strong>，对于4比特量化，我们希望需要找到15个分位数来将这个曲线下面的面积（积分）等分成16份。两个分位数的中点便是模型量化到这个区间映射的值 $q_i$。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205005255.png"></p><p>由于大模型参数通常服从正态分布，因此有：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205015121.png"><br>其中$Q$是CSF的反函数。</p><h4 id="分块k位量化"><a href="#分块k位量化" class="headerlink" title="分块k位量化"></a>分块k位量化</h4><p>常规的量化方法的局限性是当 tensor 中有一个非常大的数字（一般称为 outlier）时，导致大多数值被压缩到较小的范围，精度损失大，影响最终的量化结果。</p><p>因此，Block-wise k-bit Quantization 方法就是把 tensor 分割成 B 块，每块有自己的量化常数 c，独自量化。从而解决模型参数的极大极小的异常值的问题。分块量化的另外一个好处是减少了核之间的通信，可以实现更好的并行性，并充分利用硬件的多核的能力。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205010158.png"></p><h3 id="4位标准浮点量化—NF4"><a href="#4位标准浮点量化—NF4" class="headerlink" title="4位标准浮点量化—NF4"></a>4位标准浮点量化—NF4</h3><p>4-bit NormlFLoat 量化是结合了分位数量化和分块量化，<br>使用上面介绍的分位数量化方法我们可以将FP2精度量化到4bit的精度，但是直接这么用的一个问题是不能保证高精度的0一定被映射到低精度的0，但是0点又是深度学习中一个重要的值，例如在模型稀疏化，数据padding的时候一般都是使用0来完成。</p><p>假设offset的值是0.99，我们可以通过下面的代码片段计算出它的16个 $q_i$。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">offset = 0.99num_bins = 16quantile = norm.ppf(torch.linspace(1 - offset, offset, num_bins + 1)).tolist() # 将[1-offset,offset]区间等分为16份tmp = [(quantile[1:][idx] + val) / 2 for idx, val in enumerate(quantile[:-1])] # 计算分位数r_max, r_min = tmp[-1], tmp[0]S = (r_max - r_min)/(1 - (-1))Z = 1 - r_max / SQ = [x/S + Z for x in tmp] # 分位数量化到[-1,1]print (Q)&gt;&gt;&gt; Q = [-1.0, -0.680534899946304, -0.5217156169574965, -0.4015399993912077, -0.299784167882981, -0.20835410767681603, -0.12291223249970012, -0.040639059218818274, 0.04063881956774142, 0.12291199284862328, 0.20835391124150712, 0.2997839714476721, 0.40153976366883704, 0.5217154126647753, 0.6805348056573558, 1.0]</code></pre><p>这种方式的一个问题是0的映射值不是0，如果我们考虑奇数个bin，0是可以有个映射值但是却无法充分利用4比特的16位的信息。为了确保零点映射到0并且使用4位数据类型的全部16位，我们通过估计正负两个范围的分位数来创建一个非对称的数据类型：负数部分映射其中7位，正数部分映射8位，0占据1位，总共用满了4位数的16位。另外我们也可以使用对称的量化，其中正数和负数均使用7位，0占用2个位。我这里和论文介绍的略有不同，论文说的是正数部分取9个值，负数部分取8个值，不过它们都会取到0，所以合并时再去掉一个重复的0，这两个说法其实是一样的，只是实现方式略有差异。</p><p>接下来根据作者的源码来看下量化分位数如何计算的。其中核心代码片段摘抄如下。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">from scipy.stats import normimport torchdef create_normal_map(offset=0.9677083, use_extra_value=True, num_bins=16):    # INT8 : num_bins = 256    # INT4 : num_bins = 16    if use_extra_value:        # one more positive value, this is an asymmetric type        v1 = norm.ppf(torch.linspace(offset, 0.5, 9)[:-1]).tolist() # 正数部分        v2 = [0]*(num_bins-15) ## we have 15 non-zero values in this data type        v3 = (-norm.ppf(torch.linspace(offset, 0.5, 8)[:-1])).tolist() #负数部分        v = v1 + v2 + v3    else:        v1 = norm.ppf(torch.linspace(offset, 0.5, 8)[:-1]).tolist()        v2 = [0]*(num_bins-14) ## we have 14 non-zero values in this data type        v3 = (-norm.ppf(torch.linspace(offset, 0.5, 8)[:-1])).tolist()        v = v1 + v2 + v3    values = torch.Tensor(v)    values = values.sort().values    values /= values.max()    assert values.numel() == num_bins    return values    Q = create_normal_map()&gt;&gt;&gt; Q = [-1.0, -0.6961928009986877, -0.5250730514526367, -0.39491748809814453, -0.28444138169288635, -0.18477343022823334, -0.09105003625154495, 0.0, 0.07958029955625534, 0.16093020141124725,0.24611230194568634, 0.33791524171829224, 0.44070982933044434, 0.5626170039176941, 0.7229568362236023, 1.0]函数create_normal_map有两个入参：offset和use_extra_value。其中offset的作用是确定分位数的始末值。use_extra_value用来控制是使用对称量化还是非对称量化。v1计算正数部分，v3计算负数部分。v2直接将0映射到0，并且根据要量化的单位计算0的个数。源码是使用NF4来表示8比特的量化，如果是使用4比特的量化，我们将里面的256改成16就行。接下来最后几行用来将量化值归一化到[-1,1]。接下来我们举一个具体的实例。假设一个张量有16个值，它的被分成了4块：```pythoninput_blocked_tensor = [[-1.28645003578589, -1.817660483275528, 9.889441349505042, 0.010208034676132627], [ -15.009014631551885, 1.4136255086268115, -7.815595761491153, 10.766760590950263],  [-0.731406153917959, 3.468224595908726, 2.445252541840315, -8.970824523299282],  [-9.641638854625175, 7.696158363188889, -5.323939281255154, 5.97160401402024]]</code></pre><p>根据每个块的特征的绝对值的最大值，我们为每个块保存一个量化常数，它的计算方式是每个块中特征的绝对值中最大的那个：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">c1 = max(|-1.28645003578589|, |-1.817660483275528|, |9.889441349505042|, |0.010208034676132627|) = 9.889441349505042c2 = max(|-15.009014631551885|, |1.4136255086268115|, |-7.815595761491153|, |10.766760590950263|) = 15.009014631551885c3 = max(|-0.731406153917959|, |3.468224595908726|, |2.445252541840315|, |-8.970824523299282|) = 8.970824523299282c4 = max(|-9.641638854625175|, |7.696158363188889|, |-5.323939281255154|, |5.97160401402024|) = 9.641638854625175</code></pre><p>最后我们便可以计算这个张量的量化值了。例如第一个值-1.28645003578589，它除以这个块的量化常数c1后得到-0.13008318572517502，接下来我们要按照分位数的值来量化这个值。-0.13008318572517502在Q中最接近的值是-0.12291223249970012，这个值在Q中对应的索引是6，因此这个值被量化后的值是6。同理我们可以得到这个输入张量所有的值量化后的结果。<strong>在模型保存时，除了要保存量化后的值，我们还要保存每个块对应的量化常数c_i</strong>，因为这个值在我们进行反量化时需要用到。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">[[6, 5, 15, 7],[0, 8, 2, 14],[6, 11, 10, 0],[0, 14, 2, 13]]</code></pre><p>在反量化时，我们以量化结果作为索引，从Q中查找到它对应的分位数，再乘以为每个块保存的量化常数c_i，便可以得到最终结果。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">[[-0.9004339933799617, -1.8273060011889755, 9.889441349505042, 0.0], [-15.009014631551885, 1.1944218804231184,  -7.880829111886221,  10.850869732860506], [-0.816793898052648, 3.0313783372030603, 2.2078302737800004, -8.970824523299282], [-9.641638854625175, 6.970488722350373, -5.062564734402345, 5.424549965245643]]</code></pre><h3 id="双量化"><a href="#双量化" class="headerlink" title="双量化"></a>双量化</h3><p>在上面我们介绍到，当我们保存模型时我们不仅要保存量化后的结果，还要保存每个块的量化常数。虽然量化后的参数只有4bit的精度，但是这个量化常量的精度是float32。</p><p><strong>在QLoRA中，每个块的大小是64，块中的每个值占4比特。这相当于为了存储量化常数，模型要额外占用 32/（64*4）=12.5% 的显存。</strong></p><p>QLoRA的双重量化就是对这个量化常数再做一次8bit的量化，在进行量化常数的量化时，QLoRA以每256个量化常数为一组再做一次量化。因此它额外增加的内存消耗有两部分组成，一部分是量化后的8bit的第一层的量化常数，它额外增加的显存占比是 8/（64x4）= 3.125%， 第二部分是为量化常数做量化的第二层的32bit的量化常数，它额外增加的显存占比是32/（256x64x4）= 0.049%，额外显存增加只有3.17%。</p><h3 id="分页优化器"><a href="#分页优化器" class="headerlink" title="分页优化器"></a>分页优化器</h3><p>分页优化是针对梯度检查点做的进一步优化，以防止在显存使用峰值时发生显存OOM的问题。QLoRA分页优化其实就是当显存不足是，将保存的部分梯度检查点转移到CPU内存上，和计算机的内存数据转移到硬盘上的常规内存分页一个道理。</p><p>参考资料：</p><blockquote><p><a href="https://arxiv.org/pdf/2106.09685">https://arxiv.org/pdf/2106.09685</a><br><a href="https://arxiv.org/abs/2305.14314">https://arxiv.org/abs/2305.14314</a><br><a href="https://zhuanlan.zhihu.com/p/623543497">https://zhuanlan.zhihu.com/p/623543497</a><br><a href="https://mingchao.wang/ShYWOOwr/">https://mingchao.wang/ShYWOOwr/</a><br><a href="https://zhuanlan.zhihu.com/p/690739797">https://zhuanlan.zhihu.com/p/690739797</a><br><a href="https://zhuanlan.zhihu.com/p/666234324">https://zhuanlan.zhihu.com/p/666234324</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 大模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> LoRA </tag>
            
            <tag> PEFT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型优化--PagedAttention</title>
      <link href="/2024/12/03/da-mo-xing-you-hua-pagedattention/"/>
      <url>/2024/12/03/da-mo-xing-you-hua-pagedattention/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://juejin.cn/post/7290163879287881765">https://juejin.cn/post/7290163879287881765</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 大模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> LLM </tag>
            
            <tag> PagedAttention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型优化--KV Cache</title>
      <link href="/2024/12/02/da-mo-xing-you-hua-kv-cache/"/>
      <url>/2024/12/02/da-mo-xing-you-hua-kv-cache/</url>
      
        <content type="html"><![CDATA[<h2 id="KV-Cache-介绍"><a href="#KV-Cache-介绍" class="headerlink" title="KV Cache 介绍"></a>KV Cache 介绍</h2><p>KV Cache是Transformer标配的推理加速功能，transformer官方use_cache这个参数默认是True，但是它只能用于Decoder架构的模型，这是因为Decoder有Causal Mask，在推理的时候前面已经生成的字符不需要与后面的字符产生attention，从而使得前面已经计算的K和V可以缓存起来。</p><p>我们先看一下不使用KV Cache的推理过程。假设模型最终生成了“遥遥领先”4个字。</p><p>当模型生成第一个“遥”字时，<code>input="&lt;s&gt;"</code>, <code>"&lt;s&gt;"</code>是起始字符。Attention的计算如下：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202155542.png"></p><p>为了看上去方便，我们暂时忽略scale项$1/\sqrt(d)$， 但是要注意这个scale面试时经常考。</p><p>如上图所示，最终Attention的计算公式如下，（softmaxed 表示已经按行进行了softmax）:<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202155748.png"></p><p>当模型生成第二个“遥”字时，<code>input="&lt;s&gt;遥"</code>, Attention的计算如下：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202155837.png"></p><p>当QK变为矩阵时，softmax 会针对 行 进行计算。写详细一点如下，softmaxed 表示已经按行进行了softmax。</p><p><strong>（关键）由于decoder架构的模型有Causal Mask，所以$Q_1$与$K_2$的计算结果为$-\infty$。</strong><br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202204856.png"></p><p>假设$Att_1$表示 Attention 的第一行， $Att_2$表示 Attention 的第二行，则根据上面推导，其计算公式为：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202205002.png"></p><p>我们发现：</p><ul><li>$Q_1$在第二步参与的计算与第一步是一样的，而且第二步生成的$Att_1$仅仅依赖于$Q_1$，与$Q_2$毫无关系。</li><li>$Att_2$仅仅依赖于$Q_2$，与$Q_1$毫无关系。</li></ul><p>当模型生成第三个“领”字时，<code>input="&lt;s&gt;遥遥"</code>, Attention的计算如下：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202210952.png"><br>详细的推导参考第二步，其计算公式为：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202211018.png"><br>同样的，第三步生成的$Att_3$仅仅依赖于$Q_3$，与$Q_1$和$Q_2$毫无关系。</p><p>当模型生成第四个“先”字时，<code>input="&lt;s&gt;遥遥领"</code>, Attention的计算如下：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202211156.png"><br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202211222.png"><br>和之前类似，不再赘述。</p><p>看上面图和公式，我们可以得出结论：</p><ul><li>当前计算方式存在大量冗余计算</li><li>$Attn_k$只与$Q_k$有关</li><li>推理第$x_k$个字符时，只需要输入字符$x_{k-1}$即可。<br>第三个结论的前提是，我们需要把每一步的K和V缓存起来，这样在推理第$x_k$个字符时，只需要输入字符$x_{k-1}$计算其$Q_k,K_k,V_k$, 结合之前保存的KV Cache即可得到对应的$Attn_k$。</li></ul><p>下图展示了使用KV Cache和不使用KV Cache的过程对比：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202211901.png"></p><p><a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L318C1-L331C97">huggingface 的实现</a></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">if layer_past is not None:        past_key, past_value = layer_past        key = torch.cat((past_key, key), dim=-2)        value = torch.cat((past_value, value), dim=-2)        if use_cache is True:        present = (key, value)    else:        present = None        if self.reorder_and_upcast_attn:        attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)    else:        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)</code></pre><h2 id="KV-Cache-步骤"><a href="#KV-Cache-步骤" class="headerlink" title="KV Cache 步骤"></a>KV Cache 步骤</h2><p>正是因为 Self Attention 中带 Maske ，因此，在推理的时候，前面已经生成的 token 不需要与后面的 token 计算 Attention ，从而使得前面已经计算的 K 和 V 可以缓存起来。<br>一个典型的带有 KV Cache 的推理过程包含以下两个阶段：</p><ol><li>预填充阶段：输入一个 prompt 序列，为每个 transformer 层生成 Key Cache 和 Value Cache（KV cache）。</li><li>解码阶段：使用并更新 KV Cache，一个接一个地生成 token，当前生成的 token 依赖于之前已经生成的token。</li></ol><h3 id="预填充阶段"><a href="#预填充阶段" class="headerlink" title="预填充阶段"></a>预填充阶段</h3><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241204023238.png"></p><h3 id="解码阶段"><a href="#解码阶段" class="headerlink" title="解码阶段"></a>解码阶段</h3><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241204023814.png"></p><h2 id="MQA与GQA"><a href="#MQA与GQA" class="headerlink" title="MQA与GQA"></a>MQA与GQA</h2><p>在GPU上部署模型时，我们遵循的原则是：能在一张卡上部署的，就不要跨多张卡；能在一台机器上部署的，就不要跨多台机器。这是因为“卡内通信带宽 &gt; 卡间通信带宽 &gt; 机间通信带宽”。由于“木桶效应”，模型部署时跨的设备越多，受到设备间通信带宽的制约就越大。</p><p>因此，减少 KV Cache 的目的是为了在更少的设备上推理更长的 Context，或者在相同的 Context 长度下实现更大的推理 batch size，从而提升推理速度或增加吞吐总量。最终目的都是为了降低推理成本。</p><h3 id="MHA"><a href="#MHA" class="headerlink" title="MHA"></a>MHA</h3><p>MHA（Multi-Head Attention），也就是多头注意力，是 Transformer 中的标准 Attention 形式。在数学上，多头注意力 MHA 等价于多个独立的单头注意力的拼接。其遵循前面所讲的KV Cache 的原理。而后面的 MQA、GQA、都是围绕“如何减少 KV Cache 同时尽可能地保证效果”这个主题发展而来的产物。</p><h3 id="MQA"><a href="#MQA" class="headerlink" title="MQA"></a>MQA</h3><p>MQA，即“Multi-Query Attention”，2019年由 Google 在论文 <a href="chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/1911.02150">Fast Transformer Decoding: One Write-Head is All You Need</a> 中提出。</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241204024825.png"><br>使用 MQA 的模型包括 PaLM、StarCoder、Gemini 等。很明显，MQA 直接将 KV Cache 减少到了原来的 1/head_num。</p><p>效果方面，目前看来大部分任务的损失都比较有限。</p><h3 id="GQA"><a href="#GQA" class="headerlink" title="GQA"></a>GQA</h3><p>也有人担心 MQA 对 KV Cache 的压缩太严重，以至于会影响模型的学习效率以及最终效果。为此，一个 MHA 与 MQA 之间的过渡版本 GQA（Grouped-Query Attention）应运而生，出自 2023 年 Google 的论文 <a href="https://arxiv.org/abs/2305.13245">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints</a>。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241203015549.png"></p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241204025114.png"></p><table><thead><tr><th>模型</th><th>参数量</th><th>非Embedding参数量</th><th>GQA</th><th>上下文长度</th></tr></thead><tbody><tr><td>Qwen2-0.5B</td><td>0.49B</td><td>0.35B</td><td>√</td><td>32K</td></tr><tr><td>Qwen2-1.5B</td><td>1.54B</td><td>1.31B</td><td>√</td><td>32K</td></tr><tr><td>Qwen2-7B</td><td>7.07B</td><td>5.98B</td><td>√</td><td>128K</td></tr><tr><td>Qwen2-57B-A14B</td><td>57.41B</td><td>56.32B</td><td>√</td><td>64K</td></tr><tr><td>Qwen2-72B</td><td>72.71B</td><td>70.21B</td><td>√</td><td>128K</td></tr></tbody></table><p>在 Llama 2/3-70B 中，GQA 的 g=8 ，其他用了 GQA 的同体量模型基本上也保持了这个设置，这并非偶然，而是同样出于推理效率的考虑。我们知道，70B 这个体量的模型，如果不进行极端的量化，那么不可能部署到单卡（A100/H100 80G）上。单卡不行，那么就能单机了，一般情况下一台机可以装 8 张卡，Attention 的每个Head 实际上是独立运算然后拼接起来的，当 g=8 时，正好可以每张卡负责计算一组 K、V 对应的 Attention Head，这样可以在尽可能保证 K、V 多样性的同时最大程度上减少卡间通信。</p><p>下面看一下 GQA 的实验效果。</p><table><thead><tr><th>模型</th><th>推理时间</th><th>效果</th></tr></thead><tbody><tr><td>MHA-Large</td><td>0.37</td><td>46.0</td></tr><tr><td>MHA-XXL</td><td>1.51</td><td>47.2</td></tr><tr><td>MQA-XXL</td><td>0.24</td><td>46.6</td></tr><tr><td>GQA-8-XXL</td><td>0.28</td><td>47.1</td></tr></tbody></table><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241204025823.png"></p><p>参考：</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/708120479">https://zhuanlan.zhihu.com/p/708120479</a></p></blockquote><blockquote><p>大模型推理加速：看图学KV Cache <a href="https://zhuanlan.zhihu.com/p/662498827">https://zhuanlan.zhihu.com/p/662498827</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 大模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> LLM </tag>
            
            <tag> KV Cache </tag>
            
            <tag> MQA </tag>
            
            <tag> GQA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型显存占用分析</title>
      <link href="/2024/11/30/da-mo-xing-xian-cun-zhan-yong-fen-xi/"/>
      <url>/2024/11/30/da-mo-xing-xian-cun-zhan-yong-fen-xi/</url>
      
        <content type="html"><![CDATA[<h1 id="大模型消耗的显存"><a href="#大模型消耗的显存" class="headerlink" title="大模型消耗的显存"></a>大模型消耗的显存</h1><p>在详细说明大模型需要消耗的显存大小之前我们需要先明确几个概念。<br>一个就是大模型在不同阶段对显存的消耗是不同的。但是大致可以分为三个阶段或者说三个场景。即大模型<strong>预训练阶段</strong>、大模型<strong>微调阶段</strong>和大模型<strong>推理阶段</strong>。</p><ul><li>在<strong>预训练阶段</strong>，大模型通常选择较大规模的数据集获取泛化能力，因此需要较大的批次等来保证模型的训练强大。而模型的权重也是从头开始计算，因此通常也会选择高精度（如32位浮点数）进行训练。需要消耗大量的GPU显存资源。</li><li>在<strong>微调阶段</strong>，通常会冻结大部分参数，只训练小部分参数。同时，也会选择非常多的优化技术和较少的高质量数据集来提高微调效果，此时，由于模型已经在预训练阶段进行了大量的训练，微调时的数值误差对模型的影响通常较小。也常常选择16位精度或者混合精度训练。因此通常比预训练阶段消耗更低的显存资源。</li><li>在<strong>推理阶段</strong>，通常只是将一个输入数据经过模型的前向计算得到结果即可，因此需要最少的显存即可运行。</li></ul><h3 id="模型权重"><a href="#模型权重" class="headerlink" title="模型权重"></a>模型权重</h3><p>这部分显存用于存储神经网络模型的参数，包括权重（weights）和偏置（biases）。模型内存是模型在训练和推理过程中都需要的，因为它包含了模型的结构和学习到的知识。在训练过程中，模型内存的大小通常与模型的复杂度和参数数量成正比。</p><h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><p>在模型训练反向传播（Backward）过程中，计算的梯度所占的显存大小。梯度内存的大小与模型的参数数量有关，因为每个参数都需要计算对应的梯度。</p><h3 id="优化器状态"><a href="#优化器状态" class="headerlink" title="优化器状态"></a>优化器状态</h3><p>优化器内存用于存储优化器状态，这通常包括梯度的一阶和二阶矩（如在Adam优化器中使用的均值和方差估计）优化器内存的大小取决于所使用的优化器类型。例如，Adam优化器需要额外的内存来存储梯度的一阶和二阶矩，而SGD只需要存储梯度信息，无其他优化器内存占用。</p><h3 id="激活值"><a href="#激活值" class="headerlink" title="激活值"></a>激活值</h3><p>激活内存用于存储神经网络在前向传播过程中计算的中间激活值。这些激活值在反向传播过程中需要被重用，以计算关于模型参数的梯度。激活内存的大小与网络的深度和输入数据大小（batch size）有关。更深的网络和更大的 batch size 会导致更大的激活内存需求。</p><h1 id="数据精度"><a href="#数据精度" class="headerlink" title="数据精度"></a>数据精度</h1><p>想要计算显存，从“原子”层面来看，就需要知道我们的使用数据的精度，因为精度代表了数据存储的方式，决定了一个数据占多少bit。对于一个1B参数的模型，如果使用FP32精度存储，那么模型权重占用的显存就是1B * 2 = 2GB。</p><h3 id="常见精度类型"><a href="#常见精度类型" class="headerlink" title="常见精度类型"></a>常见精度类型</h3><p>浮点数主要是由符号位（sign）、指数位（exponent）和小数位（mantissa）三部分组成。 符号位都是1位（0表示正，1表示负），指数位影响浮点数范围，小数位影响精度。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202035151.png"></p><ul><li>FP32：32位浮点数，每个数据占4字节</li><li>TF32：<strong>19位浮点数</strong>，每个数据占2字节</li><li>FP16：16位浮点数，每个数据占2字节</li><li>BF16：16位浮点数，每个数据占2字节</li><li>Int8：8位整数，每个数据占1字节</li><li>Int4：4位整数，每个数据占0.5字节</li></ul><h3 id="混合精度训练AMP"><a href="#混合精度训练AMP" class="headerlink" title="混合精度训练AMP"></a>混合精度训练AMP</h3><p>较低模型精度对于运算效率和显存占用都更友好，但是如果直接使用FP16精度在训练过程中会出现很多问题：</p><ul><li>underflow：梯度再乘以学习率会很小，无法用fp16表示</li><li>rounding error：fp16各个区间之间存在gap，即使梯度可以用fp16表示，但是也没有把法加在fp16的权重上（被舍去）</li><li>模型预测准确度降低</li></ul><h4 id="FP32权重备份：解决舍入误差问题"><a href="#FP32权重备份：解决舍入误差问题" class="headerlink" title="FP32权重备份：解决舍入误差问题"></a>FP32权重备份：解决舍入误差问题</h4><p>保留一份FP32的主权重（Master-Weights），同时在训练中使用FP16存储权重、激活、梯度等数据。在参数更新的过程汇总，用FP16更新FP32的主权重。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202040432.png"></p><p>Step1:优化器会先备份一份FP32精度的模型权重，初始化好FP32精度的一阶和二阶动量（用于更新权重）。</p><p>Step2:开辟一块新的存储空间，将FP32精度的模型权重转换为FP16精度的模型权重。</p><p>Step3:运行forward和backward，产生的梯度和激活值都用FP16精度存储。</p><p>Step4:优化器利用FP16的梯度和FP32精度的一阶和二阶动量去更新备份的FP32的模型权重。</p><p>Step5:重复Step2到Step4训练，直到模型收敛。</p><p>我们可以看到训练过程中显存主要被用在四个模块上：</p><ul><li>模型权重本身（FP32+FP16）</li><li>梯度（FP16）</li><li>优化器（FP32）</li><li>激活值（FP16）</li></ul><p>写到这里，我们应该对于分析大模型训练时候的显存问题应该不在话下了（除了动态部分），那么我们就来实测一下，正在阅读的小伙伴也可以先自己尝试计算一下，看看是不是真的懂了。 对于llama3.1 8B模型，FP32和BF16混合精度训练，用的是AdamW优化器，请问模型训练时占用显存大概为多少？</p><p>解：</p><p>模型参数：16（BF16） + 32（PF32）= 48G</p><p>梯度参数：16（BF16）= 16G</p><p>优化器参数：32（PF32） + 32（PF32）= 64G</p><p>不考虑激活值的情况下，总显存大约占用 （48 + 16 + 64） = 128G</p><h4 id="损失缩放：解决数据下溢问题"><a href="#损失缩放：解决数据下溢问题" class="headerlink" title="损失缩放：解决数据下溢问题"></a>损失缩放：解决数据下溢问题</h4><p>当采用FP16而不是FP32更新梯度时，由于值太小，会造成FP16精度下数据下溢的问题，一些梯度会变为0，导致模型不收敛。故采用在前向过程结束后对损失进行放大，在反向过程结束后对梯度进行缩小。<br>损失缩放可以有两种主要方式：静态损失缩放和动态损失缩放。</p><ul><li>静态损失缩放：在训练开始前，设置一个固定的缩放因子，在训练过程中保持不变。</li><li>动态损失缩放：在训练过程中，根据损失值的大小动态调整缩放因子。<ul><li>如果在某轮训练中检测到梯度正常且没有溢出，缩放因子会逐渐增大。</li><li>如果检测到梯度出现 NaN 或 Inf，则缩放因子减小以防止数值不稳定。</li></ul></li></ul><h4 id="精度累加"><a href="#精度累加" class="headerlink" title="精度累加"></a>精度累加</h4><p>此外，研究者还发现，可以在模型训练的过程中，使用FP16进行乘法预算，使用FP32进行累加运算，并将FP32转换为FP16存储。FP32可以弥补损失的精度，减少舍入误差。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202042004.png"><br>如英伟达Volta架构中的Tensor Core可以使用FP16混合精度进行加速，采用的是FP16的矩阵乘法，得出全精度乘积，然后使用FP32累加，将该乘积与其他中间乘积累加，减少因FP16带来的精度损失。</p><h4 id="更为动态的精度缩放方法"><a href="#更为动态的精度缩放方法" class="headerlink" title="更为动态的精度缩放方法"></a>更为动态的精度缩放方法</h4><p>在英伟达最新的Hopper架构GPU中，英伟达的Tensor Core能够自动根据所需的精度进行动态的数据缩放调整，特别是针对Transformer网络架构，能够在数据存入内存前，根据需求改变各种参数精度。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202042340.png"><br>Hopper白皮书内容如下：</p><blockquote><p>在 Transformer 模型的每一层，Transformer Engine 都会分析 Tensor Core 产生的输出值的统计数据。了解了接下来会出现哪种类型的神经网络层以及它需要什么精度后，Transformer Engine 还会决定将张量转换为哪种目标格式，然后再将其存储到内存中。 FP8 的范围比其他数字格式更有限。为了优化使用可用范围，Transformer Engine 还使用从张量统计数据计算的缩放因子动态地将张量数据缩放到可表示的范围内。因此，每一层都在其所需的范围内运行，并以最佳方式加速。</p></blockquote><h1 id="其他显存占用"><a href="#其他显存占用" class="headerlink" title="其他显存占用"></a>其他显存占用</h1><ul><li>KV Cache：在推理过程中，大模型需要缓存一些中间结果，以便在处理下一个输入时重用。这些缓存的结果通常称为KV Cache。KV Cache占用的显存大小与模型的层数、序列长度和每个序列的token数量有关。</li><li>显存碎片：显存碎片是指显存中未被使用的空闲空间，这些空闲空间可能无法被有效利用，导致显存利用率降低。paged attention机制可以有效减少显存碎片。</li></ul><h3 id="推理与KV-cache-显存"><a href="#推理与KV-cache-显存" class="headerlink" title="推理与KV cache 显存"></a>推理与KV cache 显存</h3><p>推理的时候，显存几乎只考虑模型参数本身，除此之外就是现在广泛使用的KV cache也会占用显存。KV cache与之前讲的如何减少显存不一样，KV cache的目的是减少延迟，也就是<strong>为了推理的速度牺牲显存</strong>。</p><h4 id="kv-cache介绍"><a href="#kv-cache介绍" class="headerlink" title="kv cache介绍"></a>kv cache介绍</h4><p>具体可以参考另一篇博客：<a href="https://baoblei.github.io/2024/12/02/da-mo-xing-you-hua-kv-cache/">大模型优化–KV Cache</a><br>KV Cache是Transformer标配的推理加速功能，transformer官方use_cache这个参数默认是True，但是它只能用于Decoder架构的模型，这是因为Decoder有Causal Mask，在推理的时候前面已经生成的字符不需要与后面的字符产生attention，从而使得前面已经计算的K和V可以缓存起来。</p><p>下图展示了使用KV Cache和不使用KV Cache的过程对比：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202211901.png"><br>从图中，我们可以得出结论：</p><ul><li>当前计算方式存在大量冗余计算</li><li>$Attn_k$只与$Q_k$有关</li><li>推理第$x_k$个字符时，只需要输入字符$x_{k-1}$即可。<br>第三个结论的前提是，我们需要把每一步的K和V缓存起来，这样在推理第$x_k$个字符时，只需要输入字符$x_{k-1}$计算其$Q_k,K_k,V_k$, 结合之前保存的KV Cache即可得到对应的$Attn_k$。</li></ul><p><a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L318C1-L331C97">huggingface 的实现</a></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">if layer_past is not None:        past_key, past_value = layer_past        key = torch.cat((past_key, key), dim=-2)        value = torch.cat((past_value, value), dim=-2)        if use_cache is True:        present = (key, value)    else:        present = None        if self.reorder_and_upcast_attn:        attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)    else:        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)</code></pre><h4 id="KV-Cache显存占用"><a href="#KV-Cache显存占用" class="headerlink" title="KV Cache显存占用"></a>KV Cache显存占用</h4><p>当<strong>sequence特别长</strong>的时候，KV Cache其实还是个Memory刺客。</p><p>对于fp16精度保存的KV Cache，其占用的显存大小为：<br>$$<br>memory = batch_size * hidden_size * seq_length * layer * 2 * 2<br>$$<br>其中两个2分别表示K和V，fp16精度字节数。</p><p>比如llama 7B模型，batch_size=32, layer=32, dim_size=4096, seq_length=2048, float32类型，则需要占用的显存为 2 * 32 * 4096 * 2048 * 32 * 4 / 1024/1024/1024 = 64G。</p><p>为了解决KV Cache显存占用问题，研究者提出了<strong>MQA和GQA</strong>。其核心思想是：<strong>共享多头KV Cache</strong>。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241203015549.png"><br>以GQA为例，我们将hidden_size维度切分为head*head_dim，然后将多个head分成group组，每个group共享一个KV。则总的KV Cache显存占用为：<br>$$<br>memory = batch_size * group * head_dim * seq_length * layer *  2 * 2<br>$$<br>而MQA则是group=1，即每个head单独保存一个KV。</p><blockquote><p>大模型推理加速：看图学KV Cache <a href="https://zhuanlan.zhihu.com/p/662498827">https://zhuanlan.zhihu.com/p/662498827</a></p></blockquote><h3 id="LoRA-与-QLoRA-训练显存"><a href="#LoRA-与-QLoRA-训练显存" class="headerlink" title="LoRA 与 QLoRA 训练显存"></a>LoRA 与 QLoRA 训练显存</h3><h4 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h4><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241204190635.png"><br>LoRA是在原来的权重矩阵的旁路新建一对低秩的可训练权重，训练的时候只训练旁路，大大降低了训练的权重数量，参数量从 dxd 降为 2xdxr。</p><p>有了前面的全参情况下训练的显存分析，现在分析起来就比较通顺了，我们一步一步来，还是以BF16半精度模型Adamw优化器训练为例子，lora部分的参数精度也是BF16，并且设1字节模型参数对应的显存大小 $\Phi$。</p><ul><li>首先是模型权重本身的权重，这个肯定是要加载原始模型和lora旁路模型的，因为lora部分占比小于2个数量级，所以显存分析的时候忽略不计，显存占用 $2\Phi$。</li><li>然后就是优化器部分，优化器也不需要对原模型进行备份了，因为优化器是针对于需要更新参数的模型权重部分进行处理，也就是说优化器只包含Lora模型权重相关的内容，考虑到数量级太小，也忽略不计，故优化器部分占用显存 0。</li><li>原始模型都不更新梯度，肯定只需要Lora部分的梯度显存，而这部分占用显存也可以近似为 0。<br>想深入探究的可以去看了一篇<a href="https://baoblei.github.io/2024/12/04/da-mo-xing-gao-xiao-wei-diao-fang-fa-peft-lora-qlora/">博文</a>和<a href="https://zhuanlan.zhihu.com/p/702629428">大模型高效微调-LoRA原理详解和训练过程深入分析</a>。</li></ul><p>总的来说，不考虑激活值的情况下，Lora微调训练的显存占用只有$2\Phi$，一个7B的模型Lora训练只需要占用显存大约14G左右。验证一下，我们来看Llama Factory里给出训练任务的显存预估表格：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205031352.png"></p><h4 id="QLoRA"><a href="#QLoRA" class="headerlink" title="QLoRA"></a>QLoRA</h4><p>QLoRA本质上还是对模型的主体进行了量化，以4Bit量化为例，Qlora占用的显存主要就是4Bit量化后的模型本身也就是$0.5\Phi$，由于A、B矩阵的参数量很小，故忽略不计。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><table><thead><tr><th>部分显存对应精度（训练）</th><th>全参微调（全FP16）</th><th>全参微调（BF16混合精度）</th><th>LoRA</th><th>QLoRA</th></tr></thead><tbody><tr><td>主干模型（模型存储/计算参数）</td><td>FP16/FP16</td><td>BF16/BF16</td><td>BF16/BF16</td><td>NF4/BF16</td></tr><tr><td>主干模型（梯度）</td><td>FP16</td><td>BF16</td><td>Null</td><td>Null</td></tr><tr><td>主干模型（adamw优化器）</td><td>2 x FP16</td><td>3 x FP32</td><td>Null</td><td>Null</td></tr><tr><td>LoRA部分（可忽略不计）</td><td>Null</td><td>Null</td><td>BF16</td><td>BF16</td></tr><tr><td>总和（大约）</td><td>8Byte</td><td>16Byte</td><td>2Byte</td><td>0.5Byte</td></tr></tbody></table><h1 id="huggingface-显存分析工具"><a href="#huggingface-显存分析工具" class="headerlink" title="huggingface 显存分析工具"></a>huggingface 显存分析工具</h1><p>huggingface 提供了一个工具可以方便的查看大模型在不同阶段消耗的显存大小。<br><a href="https://huggingface.co/docs/accelerate/usage_guides/model_size_estimator">model size estimator</a></p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><blockquote><p><a href="https://blog.zhexuan.org/archives/llm-gpu-memory.html">https://blog.zhexuan.org/archives/llm-gpu-memory.html</a></p></blockquote><blockquote><p><a href="https://juejin.cn/post/7352387675837480995">https://juejin.cn/post/7352387675837480995</a></p></blockquote><blockquote><p><a href="https://gitcode.csdn.net/662a062ca2b051225566cf63.html">https://gitcode.csdn.net/662a062ca2b051225566cf63.html</a></p></blockquote><blockquote><p><a href="https://hub.baai.ac.cn/view/16045">https://hub.baai.ac.cn/view/16045</a></p></blockquote><blockquote><p><a href="https://zhuanlan.zhihu.com/p/624740065">https://zhuanlan.zhihu.com/p/624740065</a></p></blockquote><blockquote><p>NVIDIA H100 Tensor Core GPU Architecture：<a href="https://nvdam.widen.net/s/9bz6dw7dqr/gtc22-whitepaper-hopper">https://nvdam.widen.net/s/9bz6dw7dqr/gtc22-whitepaper-hopper</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 大模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> LLM </tag>
            
            <tag> KV Cache </tag>
            
            <tag> AI </tag>
            
            <tag> 大模型 </tag>
            
            <tag> LoRA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo博客搭建</title>
      <link href="/2024/11/28/hexo-zhu-ti-next-pei-zhi/"/>
      <url>/2024/11/28/hexo-zhu-ti-next-pei-zhi/</url>
      
        <content type="html"><![CDATA[<h1 id="基础搭建"><a href="#基础搭建" class="headerlink" title="基础搭建"></a>基础搭建</h1><h2 id="Hexo"><a href="#Hexo" class="headerlink" title="Hexo"></a>Hexo</h2><p>选一个博客框架，hexo是静态网站框架，基于nodejs，可以生成静态网页，部署到github上。<br>需要提前安装<strong>git</strong>，<strong>nodejs</strong>。</p><ul><li>check git version: <code>git --version</code></li><li>check nodejs version: <code>node -v</code></li><li>check npm version: <code>npm -v</code><br>安装 cnmp (optional) </li><li><code>npm install -g cnpm --registry=https://registry.npm.taobao.org</code></li><li>check cnmp version: <code>cnpm -v</code></li></ul><p>安装 <strong>hexo-cli</strong></p><ul><li><code>cnpm install -g hexo-cli</code></li><li>check hexo version: <code>hexo -v</code></li></ul><h2 id="初始化blog目录"><a href="#初始化blog目录" class="headerlink" title="初始化blog目录"></a>初始化blog目录</h2><ul><li><code>mkdir blog | cd blog</code></li><li><code>sudo hexo init</code>    # clone hexo-starter repo</li><li><code>hexo s</code>            # start blog, default port 4000</li></ul><h2 id="添加博客"><a href="#添加博客" class="headerlink" title="添加博客"></a>添加博客</h2><ul><li><code>hexo n "newblog"</code>  # add new blog, save in /source/_posts<ul><li>edit blog in /source/_posts/newblog.md</li></ul></li><li><code>hexo clean</code>        # clean cache</li><li><code>hexo g</code>            # generate static files</li><li><code>hexo s</code>            # start blog, default port 4000</li></ul><h2 id="github部署"><a href="#github部署" class="headerlink" title="github部署"></a>github部署</h2><ul><li>build blog address:<ul><li>add new token in github</li><li>github new repo: https://<username>.github.io/</username></li></ul></li><li><code>cnpm install hexo-deployer-git --save</code>  <pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml">deploy:    type: git    repo: https://&lt;token&gt;@github.com/&lt;username&gt;/&lt;username&gt;.github.io.git    branch: master</code></pre></li><li><code>hexo d</code>            # deploy blog</li></ul><h1 id="主题"><a href="#主题" class="headerlink" title="主题"></a>主题</h1><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><ul><li>theme 推荐：<ul><li><a href="https://github.com/litten/hexo-theme-yilia.git">https://github.com/litten/hexo-theme-yilia.git</a></li><li><a href="https://github.com/theme-next/hexo-theme-next.git">https://github.com/theme-next/hexo-theme-next.git</a></li></ul></li><li>clone theme into themes folder, e.g. <code>git clone https://github.com/theme-next/hexo-theme-next.git themes/next</code></li><li>edit _config.yml, set theme to next  <pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml">theme: next</code></pre></li><li><code>hexo c</code>&amp;&amp;<code>hexo g</code>&amp;&amp;<code>hexo d</code>            # clean cache, generate static files, deploy blog</li></ul><h2 id="配置主题"><a href="#配置主题" class="headerlink" title="配置主题"></a>配置主题</h2><p>大部分配置在<code>_config.yml</code> 文件中，可以参考<a href="https://theme-next.js.org/docs/getting-started/">官方文档</a></p><h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><h4 id="scheme"><a href="#scheme" class="headerlink" title="scheme"></a>scheme</h4><ul><li>Scheme 是 NexT 提供的一种特性，借助于 Scheme，NexT 为你提供多种不同的外观。同时，几乎所有的配置都可以 在 Scheme 之间共用。</li><li>muse - 默认 Scheme，这是 NexT 最初的版本，黑白主调，大量留白</li><li>Mist - Muse 的紧凑版本，整洁有序的单栏外观</li><li>Pisces - 双栏 Scheme，小家碧玉似的清新<br>Scheme 的切换通过更改 <code>_config.yml</code> 文件，搜索 scheme 关键字。 你会看到有三行 scheme 的配置，将你需用启用的 scheme 前面注释 # 去除即可。</li></ul><h4 id="language"><a href="#language" class="headerlink" title="language"></a>language</h4><h4 id="menu"><a href="#menu" class="headerlink" title="menu"></a>menu</h4><p>菜单配置包括三个部分，第一是菜单项（名称和链接），第二是菜单项的显示文本，第三是菜单项对应的图标。 NexT 使用的是 Font Awesome 提供的图标，可以在 <a href="https://fontawesome.com/icons">Font Awesome</a> 查看。</p><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml">menu:  home: / || home  archives: /archives || archive  tags: /tags || tags  categories: /categories || th</code></pre><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>主题美化是个逐渐积累的过程, 后期可以在相册、挂件、评论、搜索、sitemap、rss等方面进行配置。<br>一些工具插件：<br><strong>gallery page</strong>–justified gallery</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> next </tag>
            
            <tag> 静态网站搭建 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
