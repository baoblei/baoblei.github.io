<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>硬菜收集</title>
      <link href="/2025/01/11/ying-cai-shou-ji/"/>
      <url>/2025/01/11/ying-cai-shou-ji/</url>
      
        <content type="html"><![CDATA[<h2 id="菜肴"><a href="#菜肴" class="headerlink" title="菜肴"></a>菜肴</h2><h3 id="酱汁扒鸡"><a href="#酱汁扒鸡" class="headerlink" title="酱汁扒鸡"></a>酱汁扒鸡</h3><p>(6.97 <a href="mailto:F@u.Sl">F@u.Sl</a> 03/28 Rxs:/ 2025年夜饭第二道：酱汁扒鸡！吃肉又喝汤不浪费一点点！ 主页搜“酱汁扒鸡”有更详细的版本，这个版本更适合下载转发到“一家亲”哦~# 老饭骨 # 舌尖上的抖音 # 北方美食  <a href="https://v.douyin.com/iysoJKdm/">https://v.douyin.com/iysoJKdm/</a> 复制此链接，打开Dou音搜索，直接观看视频！)<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250111202836.png"></p><h4 id="食材"><a href="#食材" class="headerlink" title="食材"></a>食材</h4><ul><li>鸡肉</li><li>甜面酱</li><li>葱姜蒜、八角、盐、糖</li><li>香油</li></ul><h4 id="做法"><a href="#做法" class="headerlink" title="做法"></a>做法</h4><ul><li>炖<ul><li>鸡洗净切块，加入两片姜放入锅中炖一小时。</li><li>炖好的鸡肉捞出，鸡汤留着，鸡进行拆骨</li></ul></li><li>蒸<ul><li>准备酱料：甜面酱+鸡汤+糖 混合均匀</li><li>鸡肉放入准备好的酱料中装盘，再放入两片姜、两块葱、八角、香油、盐少许（根据买的甜面酱的咸度）</li><li>放入锅中蒸15-20分钟</li></ul></li><li>煎<ul><li>将蒸好的鸡肉取出拿掉其中的大料</li><li>放入锅中收汁，可以加入适量的水淀粉勾芡，也可以加入一些香油，尽量不要拿勺子炒</li><li>一面煎好后翻面继续煎，煎到两面金黄即可</li></ul></li><li>装盘：可以再加入一些青蒜点缀<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250111202744.png"></li></ul><h3 id="蒜蓉粉丝虾"><a href="#蒜蓉粉丝虾" class="headerlink" title="蒜蓉粉丝虾"></a>蒜蓉粉丝虾</h3><p>7.12 odN:/ 06/30 <a href="mailto:J@i.pq">J@i.pq</a> 年夜饭大菜！蒜蓉粉丝虾，寓意腰缠万贯！家里来客人一定少不了！# 蒜蓉粉丝虾 # 年夜饭 # 年夜饭菜谱 # 腰缠万贯虾  <a href="https://v.douyin.com/iyGesNSt/">https://v.douyin.com/iyGesNSt/</a> 复制此链接，打开Dou音搜索，直接观看视频！<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250111205942.png"></p><h4 id="食材-1"><a href="#食材-1" class="headerlink" title="食材"></a>食材</h4><ul><li>基围虾</li><li>粉丝</li><li>蒜蓉酱（也可以自制）</li><li>蒸鱼豉油</li><li>葱花、盐、白糖、料酒、白胡椒粉</li></ul><h4 id="做法-1"><a href="#做法-1" class="headerlink" title="做法"></a>做法</h4><ul><li><p>蒜蓉酱做法</p><ul><li>蒜剁碎</li><li>一半放入水中浸泡</li><li>剩下一半大蒜中放一点盐进行腌制（这个时候可以先去处理虾了）</li><li>锅中放油（需要没过准备的蒜蓉），烧至三成热</li><li>水中浸泡的蒜蓉先捞出下锅，大火搅动至蒜蓉变色，捞出</li><li>盐腌过的蒜蓉不放锅炒，加入一下白糖、白胡椒粉</li><li>炸锅的蒜放入腌过的蒜中，抄拌</li><li>油重新烧热，倒入蒜中</li></ul></li><li><p>白虾处理：</p><ul><li>去头，后背平片一刀，尾巴可以顺着腹部传过来做个造型</li><li>放入少许盐、料酒、白糖、白胡椒粉简单腌制一下</li></ul></li><li><p>蒸：</p><ul><li>粉丝泡软，剪短</li><li>粉丝可以卷成圈，圈中间部分放虾</li><li>蒜蓉酱浇在虾上</li><li>蒸锅中蒸10分钟</li></ul></li><li><p>装盘：撒上葱花、倒入蒸鱼豉油，最后浇上热油</p></li></ul><h3 id="红烧肉"><a href="#红烧肉" class="headerlink" title="红烧肉"></a>红烧肉</h3><p>9.41 <a href="mailto:n@q.eb">n@q.eb</a> fod:/ 06/01 红烧肉一次成功！中秋节团圆菜单！最好吃的家常菜做法！不用炒糖色！超下饭的红烧五花肉！# 妈呀太香了 # 红烧肉 # 家常菜 # 下饭菜 # 抖音美食推荐官 # 遇见懂你的心动好味 # 美好生活尽在抖音电商 # 提前祝大家中秋节快乐 # 中秋节 @抖音小助手 @DOU+小助手   <a href="https://v.douyin.com/iyGNQA37/">https://v.douyin.com/iyGNQA37/</a> 复制此链接，打开Dou音搜索，直接观看视频！</p><h4 id="食材-2"><a href="#食材-2" class="headerlink" title="食材"></a>食材</h4><ul><li>五花三层瘦六肥四肉</li><li>大葱、葱姜</li><li>冰糖</li><li>老抽、生抽、黄酒</li><li>八角、香叶、桂皮</li></ul><h4 id="做法-2"><a href="#做法-2" class="headerlink" title="做法"></a>做法</h4><ul><li>猪肉处理<ul><li>把肉皮部分放入热锅里，在锅底烫皮，大约两分钟后拿出</li><li>用刀把烫皮刮干净</li><li>肉倒入冷水的锅里定型，放入葱姜、黄酒等水沸腾后五分钟后取出</li><li>肉进行切块</li></ul></li><li>红烧<ul><li>锅中少许油（抹在锅面就行），五花肉块大火煎至四面金黄，煎出来的油倒出</li><li>一点花生油放入冰糖（10颗左右，勇敢放甜口的好吃），最小火炒到红色冒泡</li><li>把肉放入锅中，稍微炒个糖色，再加入半碗黄酒，以及三勺生抽、三勺老抽</li><li>加入香料：八角3个、桂皮一段、香叶2片、姜片和一个葱结</li><li>倒入冷水（基本满锅）盖上锅盖焖煮45分钟，中间如果水干了要再加入水防止煮干了</li></ul></li><li>收汁<ul><li>把香料取出，加少许盐提味</li><li>大火收汁</li><li>最后转小火慢炒</li></ul></li></ul><h3 id="可乐鸡翅"><a href="#可乐鸡翅" class="headerlink" title="可乐鸡翅"></a>可乐鸡翅</h3><p>2.56 <a href="mailto:y@t.eb">y@t.eb</a> 11/30 Wzt:/ 爷俩儿版的可乐鸡翅，看看和你做的有什么不同？# 美食趣胃计划# 可乐鸡翅# 抖音小助手  <a href="https://v.douyin.com/iyG6DNCv/">https://v.douyin.com/iyG6DNCv/</a> 复制此链接，打开Dou音搜索，直接观看视频！</p><h4 id="食材-3"><a href="#食材-3" class="headerlink" title="食材"></a>食材</h4><ul><li>鸡翅</li><li>可乐</li><li>姜</li><li>生抽、老抽、黄酒</li><li>盐、白糖</li><li>芝麻</li></ul><h4 id="做法-3"><a href="#做法-3" class="headerlink" title="做法"></a>做法</h4><ul><li>鸡翅改刀（片切或者牙签扎洞）</li><li>煎鸡翅至双面微黄</li><li>放入姜片，大火，放入些许黄酒去腥</li><li>倒入一瓶可乐，两勺生抽，半勺盐，一勺白糖</li><li>煮沸后撇去浮沫，盖上盖子中小火焖20分钟</li><li>开锅拿掉姜片，中小火收汁，最后也可以撒上一把芝麻，然后出锅</li></ul><h2 id="糕点"><a href="#糕点" class="headerlink" title="糕点"></a>糕点</h2><h3 id="戚风蛋糕（空气炸锅版）"><a href="#戚风蛋糕（空气炸锅版）" class="headerlink" title="戚风蛋糕（空气炸锅版）"></a>戚风蛋糕（空气炸锅版）</h3><p>3.33 dNj:/ <a href="mailto:D@h.Ox">D@h.Ox</a> 12/03 想做蛋糕又没烤箱的看过来啦！今天做的是空气炸锅版戚风！很详细的步骤分享给大家！# 戚风蛋糕 # 烘焙新手 # 家庭烘焙  <a href="https://v.douyin.com/iyGhDMX4/">https://v.douyin.com/iyGhDMX4/</a> 复制此链接，打开Dou音搜索，直接观看视频！</p><h4 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h4><ul><li>空气炸锅</li><li>电动打蛋器、刮刀、蛋抽</li><li>鸡蛋、面粉、白糖、食用油</li><li>两个器皿，一个蛋糕模具（4寸/6寸）</li></ul><h4 id="做法-4"><a href="#做法-4" class="headerlink" title="做法"></a>做法</h4><ul><li>25克玉米油、35克牛奶搅拌至乳化</li><li>低筋面粉45g加入其中，搅拌至无干粉的状态</li><li>分离三个蛋清蛋黄，蛋黄加入刚刚的面糊中搅拌均匀，至滴落有流动性，但滴落下去形状不会立刻消失</li><li>空气炸锅放入一碗温水，140度预热15min。</li><li>蛋清中加入30克糖，打发，要求最后提起后有直立的尖角</li><li>去一部分蛋白和蛋糊混合均匀，混合好后倒入剩余的蛋白霜中</li><li><strong>翻拌（不能搅）</strong>，倒入模具中，整理表面平整</li><li>包上锡纸，上方用牙签扎几个洞，放入炸锅，200度烤40min，然后快速揭掉锡纸180度再烤5分钟表面上色</li><li>烤好后拿出倒扣，凉了后再脱模</li></ul>]]></content>
      
      
      <categories>
          
          <category> 生活 </category>
          
          <category> 美食 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 菜谱 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pip和conda的镜像配置</title>
      <link href="/2025/01/10/pip-he-conda-de-jing-xiang-pei-zhi/"/>
      <url>/2025/01/10/pip-he-conda-de-jing-xiang-pei-zhi/</url>
      
        <content type="html"><![CDATA[<h2 id="pip"><a href="#pip" class="headerlink" title="pip"></a>pip</h2><ul><li>文件路径：<code>~/.pip/pip.conf</code></li><li>配置内容：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple[install]trusted-host = https://pypi.tuna.tsinghua.edu.cn</code></pre></li><li>查看 镜像地址：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">pip config list</code></pre></li></ul><h2 id="conda"><a href="#conda" class="headerlink" title="conda"></a>conda</h2><ul><li>文件路径：<code>~/.condarc</code></li><li>配置内容：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">channels:  - defaultsshow_channel_urls: truedefault_channels:  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2custom_channels:  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud  pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud  deepmodeling: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/</code></pre></li><li>查看 镜像地址：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">conda config --show</code></pre></li></ul><p><strong>注意</strong></p><p>镜像地址有可能会变化，当无法使用时记得最新的镜像地址。</p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pip </tag>
            
            <tag> conda </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>在docker容器中使用宿主机密钥</title>
      <link href="/2025/01/10/zai-docker-rong-qi-zhong-shi-yong-su-zhu-ji-mi-yao/"/>
      <url>/2025/01/10/zai-docker-rong-qi-zhong-shi-yong-su-zhu-ji-mi-yao/</url>
      
        <content type="html"><![CDATA[<p>在 Docker 容器中与远程 GitHub 建立安全连接时，直接在容器内生成新的 SSH 密钥存在一定的泄漏风险，特别是如果容器生命周期较短或需要共享容器的情况下。以下是几种安全的方法来处理这种场景：</p><h2 id="1-使用宿主机已有的-SSH-密钥"><a href="#1-使用宿主机已有的-SSH-密钥" class="headerlink" title="1. 使用宿主机已有的 SSH 密钥"></a>1. 使用宿主机已有的 SSH 密钥</h2><p>将宿主机中已经配置好的 SSH 密钥挂载到容器中，而不是在容器内生成新的密钥。这种方式可以避免密钥泄漏。</p><ul><li>在宿主机上确认已有的 SSH 密钥，通常路径是 ~/.ssh/id_rsa 和 ~/.ssh/id_rsa.pub。</li><li>在启动容器时将宿主机的 SSH 密钥挂载到容器中：  <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker run -it \    -v ~/.ssh:/root/.ssh:ro \     your_image_name</code></pre>  -v ~/.ssh:/root/.ssh:ro：将宿主机的 .ssh 目录挂载到容器中的 /root/.ssh，并设置为只读（ro），避免容器篡改密钥文件。</li><li>在容器中验证 SSH 连接：  <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">ssh -T git@github.com</code></pre></li></ul><h2 id="2-使用-SSH-Agent-Forwarding"><a href="#2-使用-SSH-Agent-Forwarding" class="headerlink" title="2. 使用 SSH Agent Forwarding"></a>2. 使用 SSH Agent Forwarding</h2><p>通过 SSH Agent 转发（SSH_AUTH_SOCK），让容器使用宿主机的 SSH Agent，而不直接暴露密钥文件。</p><ul><li>启动 SSH Agent，确保宿主机中 SSH Agent 正在运行：  <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">eval "$(ssh-agent -s)"ssh-add ~/.ssh/id_rsa</code></pre></li><li>添加密钥到 SSH Agent：  <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">ssh-add ~/.ssh/id_rsa</code></pre></li><li>启动容器时，将宿主机的 SSH Agent Socket 挂载到容器：  <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker run -it --rm \    -v $SSH_AUTH_SOCK:/ssh-agent \    -e SSH_AUTH_SOCK=/ssh-agent \    your_image_name</code></pre></li></ul><h2 id="3-使用环境变量传递-GIT-Token"><a href="#3-使用环境变量传递-GIT-Token" class="headerlink" title="3.使用环境变量传递 GIT Token"></a>3.使用环境变量传递 GIT Token</h2><p>如果只需要使用 Git 而不需要完整的 SSH 功能，可以使用 GitHub 提供的 Personal Access Token (PAT)。</p><ul><li>Settings &gt; Developer settings &gt; Personal access tokens。在 GitHub 上生成 Personal Access Token，勾选 repo 权限。</li><li>在运行容器时，将 Token 作为环境变量传递：  <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker run -it --rm \    -e GITHUB_TOKEN=your_personal_access_token \    your_image_name</code></pre></li><li>在容器中，使用 HTTPS URL 结合 Token：  <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">git clone https://&lt;your_username&gt;:${GITHUB_TOKEN}@github.com/&lt;repo_owner&gt;/&lt;repo_name&gt;.git</code></pre></li></ul>]]></content>
      
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> git </tag>
            
            <tag> github </tag>
            
            <tag> ssh </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git&amp;GitHub日常使用方法</title>
      <link href="/2025/01/09/git-github-ri-chang-shi-yong-fang-fa/"/>
      <url>/2025/01/09/git-github-ri-chang-shi-yong-fang-fa/</url>
      
        <content type="html"><![CDATA[<p>应该学过计算机的人都知道git和github，基本上伴随我们的整个编程生涯。但是博主由于并非科班出身，虽然日常都在用，但是一直对里面的一些概念和操作不是很清楚，每次遇到问题都是百度一下，解决了就忘记了。<br>所以想通过这篇文章来梳理记录一下git和github的日常使用方法，方便自己查阅，也希望能帮助到有需要的人。</p><h2 id="Git和GitHub的概念"><a href="#Git和GitHub的概念" class="headerlink" title="Git和GitHub的概念"></a>Git和GitHub的概念</h2><ul><li>Git 是一个分布式版本控制系统，用于管理代码版本和协作开发。</li><li>GitHub 是一个基于 Git 的代码托管平台，提供了代码托管、版本管理、协作开发等功能。同时增加了界面化的功能，如 Pull Requests、Issues 和 Wiki。</li></ul><h2 id="git与github的联合配置"><a href="#git与github的联合配置" class="headerlink" title="git与github的联合配置"></a>git与github的联合配置</h2><ol><li>安装Git</li></ol><ul><li>访问 <a href="https://git-scm.com/">Git 官方网站</a> 下载并安装适合操作系统的版本。</li><li>验证安装：在命令行输入 <code>git --version</code>，显示版本号则安装成功。</li></ul><ol start="2"><li>配置Git</li></ol><ul><li>配置用户名：<code>git config --global user.name "Your Name"</code></li><li>配置邮箱：<code>git config --global user.email "your_email@example.com"</code></li></ul><ol start="3"><li>创建 GitHub 账户并设置 SSH 密钥</li></ol><ul><li>注册 GitHub 账户</li><li>生成 SSH 密钥：<code>ssh-keygen -t ed25519 -C "your_email@example.com"</code></li><li>将 SSH 密钥添加到 GitHub 账户：将 <code>~/.ssh/id_ed25519.pub</code> 文件中的内容添加到 GitHub 账户的 SSH keys 中。</li><li>验证GitHub连接：<code>ssh -T git@github.com</code><br><strong>如果你是在docker容器中连接github，为了避免SSH 密钥的泄漏风险应该<a href="">使用宿主机已有的 SSH 密钥</a></strong><br><strong>注意.ssh目录中的权限不能修改，如果出现权限太过开放，则需要修改.ssh目录的权限</strong><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">chmod 700 ~/.sshchmod 600 ~/.ssh/id_ed25519chmod 644 ~/.ssh/id_ed25519.pub</code></pre></li></ul><ol start="4"><li>Git 与 GitHub 的连接</li></ol><ul><li>在 GitHub 上创建仓库，获取仓库地址（https或ssh地址）</li><li>在项目的本地目录中初始化仓库：<code>git init</code></li><li>添加远程仓库地址：<code>git remote add origin &lt;仓库地址&gt;  # 添加远程仓库</code></li><li>基本命令：  <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">git add .          # 添加所有文件到暂存区git commit -m "提交信息"  # 提交到本地仓库git push origin main  # 推送到远程仓库</code></pre></li></ul><h2 id="典型工作流程"><a href="#典型工作流程" class="headerlink" title="典型工作流程"></a>典型工作流程</h2><ol><li>克隆仓库：<code>git clone &lt;仓库地址&gt;</code></li><li>同步更新：<code>git pull origin main</code></li><li>分支管理：<ul><li>创建分支：<code>git branch &lt;分支名&gt;</code></li><li>切换分支：<code>git checkout &lt;分支名&gt;</code></li><li>合并分支：<code>git merge &lt;分支名&gt;</code> # 要先切换到主分支</li><li>删除分支：<code>git branch -d &lt;分支名&gt;</code></li></ul></li><li>提交更改： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">git add .git commit -m "提交信息"git push origin main</code></pre></li></ol><h2 id="GitHub高级功能"><a href="#GitHub高级功能" class="headerlink" title="GitHub高级功能"></a>GitHub高级功能</h2><h3 id="fork"><a href="#fork" class="headerlink" title="fork"></a>fork</h3><p>在 GitHub 上，如果你直接clone了一个别人的仓库，会没有原仓库的写入权限（例如为他人开源项目贡献代码）。即使有权限，但团队页也可能有明确要求或想避免在原仓库中直接创建分支。<br>所以fork就是为了解决这个问题的，fork的作用是在你的仓库中创建一个原仓库的副本，你可以在这个副本上自由修改，然后通过pull request向原仓库提交修改。</p><ol><li>在 GitHub 上点击仓库页面右上角的 Fork 按钮，创建一个你的副本仓库。</li><li>将 Fork 的仓库克隆到本地： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">git clone &lt;你的 Fork 仓库地址&gt;</code></pre></li><li>创建新分支： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">git checkout -b feature-branch</code></pre></li><li>提交更改： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">git add .git commit -m "添加新功能"git push origin feature-branch</code></pre></li><li>在 GitHub 上进入你的 Fork 仓库，点击 “New Pull Request”，选择将你的分支合并到原仓库的主分支（如 main）。</li></ol><p><strong>Fork 的仓库能否跟踪原仓库的更改？</strong><br>Fork 仓库可以跟踪原仓库的更改，但需要手动同步更新。</p><ol><li>添加原仓库的远程地址： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">git remote add upstream &lt;原仓库地址&gt;</code></pre></li><li>验证是否添加成功：<code>git remote -v</code><br> 输出类似： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">origin    &lt;你的 Fork 仓库地址&gt; (fetch)upstream  &lt;原仓库地址&gt; (fetch)</code></pre></li><li>同步原仓库的更新：</li></ol><ul><li>从 upstream 拉取最新代码：  <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">git fetch upstream</code></pre></li><li>合并原仓库的主分支代码到你的主分支：  <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">git checkout maingit merge upstream/main</code></pre></li><li>推送更新到你的 Fork 仓库：  <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">git push origin main</code></pre></li></ul><h3 id="Pull-Requests"><a href="#Pull-Requests" class="headerlink" title="Pull Requests"></a>Pull Requests</h3><p>Pull Request (PR) 是协作开发的重要工具，用于在分支之间或不同开发者的仓库之间合并代码改动，同时提供审阅、讨论和测试的机会。</p><ol><li>创建 Pull Request</li></ol><ul><li>确保你的改动已经推送到远程仓库中的某个分支（如 feature-branch）。</li><li>在 GitHub 网站上，进入目标仓库，点击 “Pull Requests”。</li><li>点击 “New Pull Request”。</li><li>选择：<ul><li>Base Branch：要合并到的主分支（如 main）</li><li>Compare Branch：你的开发分支（如 feature-branch）</li></ul></li></ul><ol start="2"><li>审阅和讨论</li></ol><ul><li>代码审阅者会看到你的 PR，提出评论或修改建议。</li><li>你可以根据反馈进行改动，并通过提交新的代码到分支自动更新 PR。</li></ul><ol start="3"><li>合并 PR</li></ol><ul><li>审阅通过后，仓库管理员或 PR 创建者可以点击 “Merge Pull Request”。</li><li>GitHub 提供多种合并方式：<ul><li>Merge Commit（默认方式）：保留所有提交历史。</li><li>Squash and Merge：将所有提交合并为一个提交。</li><li>Rebase and Merge：将分支历史与主分支合并，保留线性历史。</li></ul></li></ul><ol start="4"><li>关闭 PR</li></ol><ul><li>如果 PR 不再需要，可以选择关闭 PR，而不合并改动。</li></ul><h3 id="Issues"><a href="#Issues" class="headerlink" title="Issues"></a>Issues</h3><p>Issues 是用于跟踪任务、记录 Bug 或建议新功能的工具。它是协作和项目管理的重要部分。</p><ol><li>创建 Issue</li></ol><ul><li>在仓库主页，点击 “Issues”。</li><li>点击 “New Issue”。</li><li>填写 Issue 标题和描述。</li><li>添加标签（Labels）标记类型（如 Bug、Enhancement）、分配人员（Assignees）和里程碑（Milestones）。</li></ul><ol start="2"><li>管理 Issue</li></ol><ul><li>可以对 Issue 进行评论和讨论。</li><li>可以将 Issue 关联到 PR 或者 Issue。</li></ul><ol start="3"><li>关闭 Issue</li></ol><ul><li>当 Issue 解决后，可以点击 “Close Issue” 关闭。</li><li>如果与 PR 关联，PR 合并时可以自动关闭 Issue（在 PR 描述中写 Fixes #&lt;Issue编号&gt;）。</li></ul><h3 id="Actions"><a href="#Actions" class="headerlink" title="Actions"></a>Actions</h3><p>GitHub Actions 是 GitHub 提供的持续集成/持续部署（CI/CD）工具，用于自动化构建、测试和部署流程。</p><ol><li>创建 Workflow</li></ol><ul><li>在仓库中创建 <code>.github/workflows</code> 目录。</li><li>在目录中创建 <code>.yml</code> 文件，定义 Workflow。示例：  <pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml">name: CIon:push:    branches:    - mainjobs:build:    runs-on: ubuntu-latest    steps:    - name: Checkout code        uses: actions/checkout@v2    - name: Set up Node.js        uses: actions/setup-node@v3        with:        node-version: 16    - name: Install dependencies        run: npm install    - name: Run tests        run: npm test</code></pre></li></ul><ol start="2"><li>触发 Workflow</li></ol><ul><li>每次推送代码或创建 PR，Actions 会自动运行。</li><li>运行结果可以在仓库的 “Actions” 标签页中查看。</li></ul><ol start="3"><li>使用现成模板</li></ol><ul><li>GitHub 提供了许多现成的 Workflow 模板，如代码格式检查、构建部署等，可以直接使用。</li></ul><h3 id="Projects"><a href="#Projects" class="headerlink" title="Projects"></a>Projects</h3><p>Projects 是 GitHub 提供的项目管理工具，用于组织和跟踪任务、Issue 和 PR。</p><ul><li>创建项目<ul><li>在仓库页面，点击 “Projects”</li><li>点击 “New Project”。</li><li>选择模板（如 Kanban Board）或自定义项目布局。</li></ul></li><li>添加任务<ul><li>将 Issues 或 Pull Requests 直接添加到项目中。</li><li>可以在项目中拖动任务，调整任务状态或优先级。</li></ul></li><li>管理任务<ul><li>通过拖放操作移动任务到不同阶段（如 “To Do” -&gt; “In Progress” -&gt; “Done”）。</li><li>设置优先级、截止日期和责任人。</li></ul></li></ul><h3 id="Wiki"><a href="#Wiki" class="headerlink" title="Wiki"></a>Wiki</h3><p>Wiki 是一个仓库附带的文档存储工具，适用于记录项目文档、API 说明或团队协作指南。</p><ul><li>在仓库页面，点击 “Wiki”。</li><li>点击 “Create the first page”。</li><li>编辑页面内容，支持 Markdown 语法。</li></ul>]]></content>
      
      
      <categories>
          
          <category> git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker常见命令</title>
      <link href="/2025/01/09/docker-chang-jian-ming-ling/"/>
      <url>/2025/01/09/docker-chang-jian-ming-ling/</url>
      
        <content type="html"><![CDATA[<ol><li><p>拉取镜像</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker pull [镜像地址]#【镜像地址]一般由[镜像名]:[标签]组成，标签一般指的版本号，默认为latest。例如：ubuntu:20.04</code></pre></li><li><p>创建并运行新容器</p></li></ol><p>使用拉取的镜像创建并运行容器：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker run -d --name &lt;容器名&gt; -p &lt;主机端口&gt;:&lt;容器端口&gt; -v &lt;宿主机目录或卷名&gt;:&lt;容器目录&gt; &lt;镜像名&gt; sleep infinity# -d: 后台运行容器# --name: 指定容器名称# -p: 将主机端口映射到容器端口，格式为 主机端口:容器端口# -v: 将宿主机目录或数据卷挂载到容器目录。宿主机目录用于直接挂载本地文件系统，而数据卷提供更高的灵活性和隔离性。# sleep infinity: 保持容器运行# 如果容器以创建可以使用以下命令启动容器：docker start &lt;容器名或ID&gt;</code></pre><ol start="3"><li><p>查看容器</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker ps # 查看正在运行的容器docker ps -a # 查看所有容器，包括已停止的</code></pre></li><li><p>进入容器</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker exec -it &lt;容器名&gt; /bin/bash # 或者 /bin/sh # it: 是两个选项的组合: --interactive --tty# -i: 交互式操作# -t: 为容器分配一个伪终端</code></pre></li><li><p>停止和删除容器</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker stop &lt;容器名或ID&gt;docker rm &lt;容器名或ID&gt;docker stop $(docker ps -aq) # 停止所有容器docker rm $(docker ps -aq) # 删除所有容器</code></pre></li><li><p>保存容器状态为新镜像</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker commit &lt;容器名或ID&gt; &lt;新镜像名&gt;:&lt;标签&gt;</code></pre></li><li><p>查看镜像</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker images</code></pre></li><li><p>删除镜像</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker rmi &lt;镜像名或ID&gt;docker rmi -f &lt;镜像名或ID&gt; # 强制删除</code></pre></li><li><p>导出和导入镜像</p></li></ol><ul><li>导出镜像为文件：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker save -o &lt;文件名&gt;.tar &lt;镜像名或ID&gt;</code></pre></li><li>从文件导入镜像：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker load -i &lt;文件名&gt;.tar</code></pre></li></ul><ol start="10"><li>检查容器日志</li></ol><ul><li>查看容器运行日志：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker logs &lt;容器名或ID&gt;</code></pre></li><li>实时查看日志：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker logs -f &lt;容器名或ID&gt;</code></pre></li><li>查看容器内进程：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker top &lt;容器名或ID&gt;</code></pre></li><li>检查容器的详细信息：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker inspect &lt;容器名或ID&gt;</code></pre></li></ul><ol start="11"><li>清理无用的资源</li></ol><ul><li>清理未使用的镜像、容器、网络和卷：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker system prune</code></pre></li><li>清理未使用的卷：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker volume prune</code></pre></li></ul>]]></content>
      
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker的存储</title>
      <link href="/2025/01/09/docker-de-cun-chu/"/>
      <url>/2025/01/09/docker-de-cun-chu/</url>
      
        <content type="html"><![CDATA[<p><a href="https://docs.docker.com/engine/storage/">Docker 的存储功能</a>分为几个主要部分，每个部分都有不同的作用和适用场景，以下是对这些部分的详细说明：</p><h2 id="Volumes（卷）"><a href="#Volumes（卷）" class="headerlink" title="Volumes（卷）"></a>Volumes（卷）</h2><p>Volumes 是 Docker 推荐的持久化存储方式，可以将容器数据独立存储在主机文件系统中。即使容器被删除，数据也不会丢失。<br><strong>设置卷的存储位置</strong><br>卷会被存储在 Docker 的默认数据目录中：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">/var/lib/docker/volumes/&lt;volume_name&gt;/_data # LinuxC:\ProgramData\Docker\volumes\&lt;volume_name&gt;\_data # Windows</code></pre><p>运行以下命令查看某个卷的具体路径：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker volume inspect my_volume</code></pre><p>如果希望将卷存储到非默认路径，可以通过修改 Docker 的配置文件（通常位于 /etc/docker/daemon.json，没有的话可以新建）来实现：</p><pre class="line-numbers language-json" data-language="json"><code class="language-json">{    # 将 Docker 数据目录修改为 /home/docker。    "data-root": "/home/docker"}</code></pre><p>其实这里本质上是修改了 Docker 的数据目录，卷的数据也会存储在这个目录中，我们可以通过<code>docker info</code>查看 Docker 的根目录。</p><p><strong>切记</strong>如果之前已经有数据，需要将数据从<code>/var/lib/docker</code>迁移到新的目录<code>/home/docker</code>中。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">systemctl stop docker # 停止 Docker 服务rsync -avz /var/lib/docker /home/docker # 迁移数据systemctl start docker # 启动 Docker 服务</code></pre><p>设置后重启 Docker 服务:<code>sudo systemctl restart docker</code>，所有新创建的卷都会存储在新的数据目录<code>/home/docker/volumes</code>中。可以在通过<code>docker volume inspect my_volume</code>查看卷的具体路径，或者<code>docker info</code>查看 Docker 的根目录。</p><p><strong>卷可以被不同的容器共享，也可以在多个容器之间传递数据。</strong></p><p><strong>注意</strong></p><ol><li>不要直接修改卷目录中的文件：如果直接修改主机上的卷目录中的数据，可能导致数据损坏或无法被容器识别。</li><li>备份与迁移：可以通过主机上的路径对卷数据进行备份或迁移。</li></ol><p><strong>使用方式</strong></p><ul><li><p>创建一个 volume</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker volume create my_volume</code></pre></li><li><p>在容器中挂载 volume，<strong>通过挂载卷可以实现数据持久化，即使容器被删除，卷中的数据仍然保留。</strong></p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker run -d --name my_container -v my_volume:/app/data my_image# -d: 分离模式，后台运行容器# --name: 容器名称# -v: 挂载 volume# my_image: 镜像名称</code></pre></li><li><p>查看 volume</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker volume ls</code></pre></li><li><p>删除 volume</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker volume rm my_volume</code></pre></li></ul><h2 id="Bind-mounts（绑定挂载）"><a href="#Bind-mounts（绑定挂载）" class="headerlink" title="Bind mounts（绑定挂载）"></a>Bind mounts（绑定挂载）</h2><p>Bind Mount 将主机上的文件或目录直接挂载到容器中，主机上的数据可以实时更新到容器中，反之亦然。</p><p><strong>使用方式</strong>-在运行容器时指定绑定挂载：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker run -d --name my_container -v /path/on/host:/path/in/container my_image</code></pre><p><strong>适用场景</strong>：</p><ol><li>开发环境中需要实时同步主机和容器数据。</li><li>在主机上有特定目录需要容器访问（如配置文件）。</li></ol><p><strong>注意</strong>：<br>绑定挂载没有卷的隔离性和安全性，需谨慎操作主机文件。</p><h2 id="tmpfs-Mounts（临时文件系统挂载）"><a href="#tmpfs-Mounts（临时文件系统挂载）" class="headerlink" title="tmpfs Mounts（临时文件系统挂载）"></a>tmpfs Mounts（临时文件系统挂载）</h2><p>tmpfs 挂载将数据存储在内存中，而不是磁盘上。数据是临时的，容器停止后数据会丢失。</p><p><strong>使用方式</strong>-在运行容器时使用 –tmpfs 选项：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker run -d --name my_container --tmpfs /app/data my_image# /app/data 是容器内部的路径，应用程序在该路径下操作数据。</code></pre><p><strong>适用场景：</strong></p><ol><li>临时数据存储，例如缓存或临时文件。</li><li>对性能要求较高且数据无需持久化的场景。</li></ol><h2 id="Storage-Drivers（存储驱动）"><a href="#Storage-Drivers（存储驱动）" class="headerlink" title="Storage Drivers（存储驱动）"></a>Storage Drivers（存储驱动）</h2><p>存储驱动管理容器和镜像的底层存储。每种存储驱动适用于不同的文件系统和操作场景。<br>可以详情查看<a href="https://docs.docker.com/engine/storage/drivers/">官方文档</a>，有具体的图示和说明，对镜像和容器的存储方式有更深入的了解。</p><p><strong>常用驱动：</strong></p><ol><li>OverlayFS：默认驱动，适用于大多数 Linux 发行版。</li><li>BTRFS：支持高级功能，如快照和压缩。</li><li>ZFS：高性能存储驱动，适用于复杂存储需求。</li><li>Device Mapper 和 AUFS：较老的存储驱动。</li></ol><p><strong>使用方式：</strong>- 检查当前存储驱动：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker info | grep Storage</code></pre><h2 id="建议"><a href="#建议" class="headerlink" title="建议"></a>建议</h2><p>Docker 提供了一些存储管理的建议：</p><ol><li>优先使用 Volumes：推荐用 Volumes 来替代 Bind Mount，确保数据隔离性和安全性。</li><li>避免直接使用容器层存储：容器层的存储是临时的，适用于短期任务，不建议用于持久化数据。</li><li>定期清理未使用的卷：<code>docker volume prune</code> 命令可以清理未使用的卷，释放磁盘空间。</li></ol>]]></content>
      
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用CodeWithGPU实现模型的开箱即用</title>
      <link href="/2025/01/08/shi-yong-codewithgpu-shi-xian-mo-xing-de-kai-xiang-ji-yong/"/>
      <url>/2025/01/08/shi-yong-codewithgpu-shi-xian-mo-xing-de-kai-xiang-ji-yong/</url>
      
        <content type="html"><![CDATA[<h1 id="CodeWithGPU简介"><a href="#CodeWithGPU简介" class="headerlink" title="CodeWithGPU简介"></a>CodeWithGPU简介</h1><p><a href="https://www.codewithgpu.com/image">CodeWithGPU</a>是一个AI镜像、模型技术社区（今后文档中简称CG)，它以GitHub为基础，在GitHub托管的代码下游，提供额外的镜像环境、算法模型等，解决以往构建环境难、管理与分享模型难的问题，为算法开发者、研究者等提供开箱即用的内容体验。</p><blockquote><p>CodeWithGPU和AutoDL同属于视拓云研发和运营的产品，两者账户通用，可以互相登录使用。</p></blockquote><h2 id="镜像"><a href="#镜像" class="headerlink" title="镜像"></a>镜像</h2><p>CG上的镜像指能运行Github上某一具体项目代码所需的docker image环境。主要目的为方便其他用户免去构建环境安装依赖的过程，而直接在此基础之上训练模型。镜像来自用户在AutoDL上保存的docker image。(AutoDL是一个GPU算力租用平台，它和CG共用账户，即一个帐号可在两个平台中登录)</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>CG上的模型指用户使用算法在某数据集上训练出来的模型文件。<br>使用的话首先需要安装：<code>pip install codewithgpu</code><br>可以由以下两种方式下载模型：</p><ol><li>cli<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">cg down GuoFeng3/GuoFeng3.3.safetensors</code></pre></li><li>代码中集成<pre class="line-numbers language-python" data-language="python"><code class="language-python">import codewithgpu as cg cg.model.download("模型文件的KEY", target_directory="下载目标路径，可选"))</code></pre></li></ol><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><h2 id="cuda检查"><a href="#cuda检查" class="headerlink" title="cuda检查"></a>cuda检查</h2><p>检查电脑的nvidia驱动以及cuda toolkit。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">nvidia-sminvcc -V</code></pre><p>如果显示有：<code>Failed to initialize NVML: Driver/library version mismatch。</code>则说明驱动和cuda toolkit版本不匹配，需要重新安装驱动或者cuda toolkit。</p><p>或者你的cuda版本过低，建议也升级一下，不然CG很多新的镜像都无法使用。</p><p>如果你的cuda版本ok且驱动都正常的话，那么跳过这里直接看下面的安装cg-client。</p><h2 id="卸载旧版本nvida驱动和cuda-toolkit"><a href="#卸载旧版本nvida驱动和cuda-toolkit" class="headerlink" title="卸载旧版本nvida驱动和cuda toolkit"></a>卸载旧版本nvida驱动和cuda toolkit</h2><ul><li><p>卸载nvidia驱动：<code>sudo nvidia-uninstall</code></p></li><li><p>卸载cuda toolkit：找到 CUDA 工具包的安装目录（例如 /usr/local/cuda-10.1/），然后执行：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">cd /usr/local/cuda-10.1/bin/sudo ./cuda-uninstaller</code></pre><p>如果你忘记了 CUDA 工具包的安装目录，可以使用以下命令找到：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">where nvcc</code></pre><p>或者通过之前安装cuda设置的环境变量查看：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">echo $PATH | tr ':' '\n' | grep cudaecho $LD_LIBRARY_PATH | tr ':' '\n' | grep cuda</code></pre><p>如果没有找到cuda-uninstaller，可以手动删除：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo rm -rf /usr/local/cuda-11.8/   # 删除目录# 清理其他残留文件：sudo apt-get remove --purge '^nvidia-.*'sudo apt-get remove cuda*sudo apt-get autoremove# 查看剩余残留sudo dpkg -l |grep cuda# 继续卸载哈哈sudo dpkg -P 残留文件全称# 环境变量删除：编辑～/.bashrc 或者  /etc/profile文件中是否有 CUDA 相关路径， 删除以下内容export PATH=/usr/local/cuda-11.8/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH# 保存source ~/.bashrc</code></pre></li><li><p>完成卸载后，删除 CUDA 工具包的目录：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo rm -rf /usr/local/cuda-10.1/</code></pre></li></ul><h2 id="安装新版本nvidia驱动"><a href="#安装新版本nvidia驱动" class="headerlink" title="安装新版本nvidia驱动"></a>安装新版本nvidia驱动</h2><ul><li>前往 NVIDIA <a href="https://www.nvidia.cn/Download/index.aspx">官方网站</a>，根据显卡型号（例如 2080Ti）下载最新的驱动程序。</li><li>将下载的驱动程序（例如 NVIDIA-Linux-x86_64-550.54.14.run）上传到服务器的某个目录，然后执行：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo sh NVIDIA-Linux-x86_64-550.54.14.run</code></pre></li><li>安装完成后，重启服务器：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo reboot</code></pre></li><li>重启后，检查驱动是否安装成功：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">nvidia-smi</code></pre></li></ul><h2 id="安装新版本cuda-toolkit"><a href="#安装新版本cuda-toolkit" class="headerlink" title="安装新版本cuda toolkit"></a>安装新版本cuda toolkit</h2><p>注意安装的cuda toolkit版本要和nvidia驱动版本匹配：</p><pre class="line-numbers language-none"><code class="language-none">- nvidia-smi属于driver API、nvcc属于runtime API。- 用于支持driver API的必要文件(如libcuda.so)是由GPU driver installer安装的。- 用于支持runtime API的必要文件(如libcudart.so以及nvcc)是由CUDA Toolkit installer安装的。- 如果只安装driver API，不安装runtime API（cuda toolkit），也能正常使用pytorch，但涉及到一些需要编译安装的更底层的工具，比如apex、deepspeed，就会报错，这时还需要床runtime API。</code></pre><ul><li>前往 NVIDIA <a href="https://developer.nvidia.com/cuda-downloads">开发者网站</a>，根据操作系统和需求下载与新驱动兼容的 CUDA 工具包（例如 CUDA 12.1.1）。</li><li>将下载的 CUDA 安装包（例如 cuda_12.1.1_530.30.02_linux.run）上传到服务器的某个目录，然后执行：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo sh cuda_12.1.1_530.30.02_linux.run</code></pre><strong>注意</strong>：由于已经有CUDA Driver12.4了，因此在安装过程中不需要勾选CUDA Driver。</li><li>安装完成后，设置环境变量：<br>（如果之前安装过旧版本记得删除旧版本的环境变量）<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">echo 'export PATH=/usr/local/cuda-12.1/bin:$PATH' &gt;&gt; ~/.bashrcecho 'export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH' &gt;&gt; ~/.bashrcsource ~/.bashrc</code></pre></li><li>检查CUDA是否安装成功：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">nvcc -V</code></pre></li></ul><h2 id="cg-client"><a href="#cg-client" class="headerlink" title="cg-client"></a>cg-client</h2><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"># 下载客户端并添加可执行权限wget -O cg-client https://codewithgpu.ks3-cn-beijing.ksyuncs.com/cg-clientchmod +x cg-client# 启动客户端，需要使用到sudo权限否则有docker使用上的异常sudo ./cg-client</code></pre><h2 id="web上使用"><a href="#web上使用" class="headerlink" title="web上使用"></a>web上使用</h2><p>启动<code>cg-client</code>后会输出类似如下包含访问地址的日志：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">**************cg-client is running on http://localhost:2022/container?token=E5PR11vT9MMbrpwFeIdLcltRNnUmaY**************</code></pre><p>在浏览器中打开该地址后，客户端会自动检测是否有Docker环境，点击安装nvidia-docker按钮自动安装相关环境</p><p><strong>docker的权限配置</strong></p><p>在使用docker时，如果不想每次都使用sudo，可以将当前用户加入docker组：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo groupadd docker # 添加docker组sudo gpasswd -a ${USER} docker # 将当前用户加入docker组sudo systemctl restart docker # 重启docker服务sudo chmod a+rw /var/run/docker.sock # 修改docker.sock权限</code></pre><p><strong>如果使用ssh连接的服务器使用则需要设置端口转发</strong></p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">ssh -CNg -L 2022:localhost:2022 user@host -p port</code></pre><p>参数说明：<br>-C:<br>启用压缩。这对于减少传输数据量有帮助，特别是在带宽较低的情况下。</p><p>-N:<br>不执行远程命令，只建立隧道。适合仅用于端口转发的场景。</p><p>-g:<br>允许其他主机通过本机（运行此命令的机器）的 7890 端口访问隧道。<br>如果不需要其他机器访问，可以省略这个参数。</p><p>-L:<br>本地端口转发。格式为 -L [本地端口]:[目标地址]:[目标端口]</p><p>CG开始使用时需要安装docker/nvidia-docker环境，web界面会自动检测是否有Docker环境，点击安装nvidia-docker按钮自动安装相关环境，但是我使用时发现我原本安装的docker环境在使用CG提供的脚本安装后就失效了，所以建议还是卸载原有的docker环境后再使用CG提供的脚本安装</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250108033158.png"><br>这样就成功啦</p><h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><p>在web界面可以实现容器的创建、启动、停止、删除等操作，也可以上传文件到容器中，也可以在容器中打开终端，执行命令，查看日志等操作。</p><h2 id="docker的存储"><a href="#docker的存储" class="headerlink" title="docker的存储"></a>docker的存储</h2><p>在容器内我们会发现系统盘大小为本地根目录的大小，这是因为Docker 默认安装的情况下，会使用<code>/var/lib/docker/</code>目录作为存储目录，用以存放拉取的镜像和创建的容器等。</p><p>而autodl-tmp未挂载，虽然CG提供了网盘的支持，但是如果我们想适用宿主机中的磁盘分区的话，还得会一些docker的存储知识（见博客-<a href="https://baoblei.github.io/2025/01/09/docker-de-cun-chu/">docker的存储</a>）</p><p>在了解了docker的存储后，我们可以通过<code>docker run</code>命令的<code>-v</code>参数来挂载宿主机的磁盘分区到容器中，这样就可以在容器中使用宿主机的磁盘分区了。在web界面创建容器时也提供了挂载的选项，可以直接在web界面中挂载。</p><h2 id="镜像与容器的基本用法"><a href="#镜像与容器的基本用法" class="headerlink" title="镜像与容器的基本用法"></a>镜像与容器的基本用法</h2><p>具体见博客-<a href="https://baoblei.github.io/2025/01/09/docker-chang-jian-ming-ling/">docker的常见命令</a>，这里简单通过本地终端创建并运行一个容器来说明：</p><ul><li>拉取镜像<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker pull registry.cn-beijing.aliyuncs.com/codewithgpu2/hvision-nku-storydiffusion:n3iFx8w7UJ</code></pre></li><li>查看镜像<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker images</code></pre></li><li>创建并运行容器<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo docker run -d --name quizzical_wilson -v data80:/root/autodl-tmp registry.cn-beijing.aliyuncs.com/codewithgpu2/hvision-nku-storydiffusion:n3iFx8w7UJ sleep infinity# 这里我挂载了一个卷data80到容器的/root/autodl-tmp目录sudo docker exec -it quizzical_wilson /bin/bash</code></pre>到这里就可以在容器中进行操作了，可以使用<code>exit</code>退出容器。</li><li>停止容器<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker stop quizzical_wilson</code></pre></li></ul><p><strong>参考:</strong></p><blockquote><p>CG官方文档：<a href="https://www.codewithgpu.com/docs/">https://www.codewithgpu.com/docs/</a></p></blockquote><blockquote><p>docker engine文档：<a href="https://docs.docker.com/engine/">https://docs.docker.com/engine/</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> CG </category>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CG </tag>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>通过公网访问局域网内服务器</title>
      <link href="/2025/01/07/tong-guo-gong-wang-fang-wen-ju-yu-wang-nei-fu-wu-qi/"/>
      <url>/2025/01/07/tong-guo-gong-wang-fang-wen-ju-yu-wang-nei-fu-wu-qi/</url>
      
        <content type="html"><![CDATA[<p>分为两种情况：</p><h2 id="拥有路由器管理权限"><a href="#拥有路由器管理权限" class="headerlink" title="拥有路由器管理权限"></a>拥有路由器管理权限</h2><ol><li><p>MAC地址绑定：在路由器中绑定服务器的MAC地址，使其获得固定的内网IP地址。</p></li><li><p>端口映射：在路由器中设置端口映射，将公网IP的某个端口映射到服务器的内网IP的某个端口上。</p></li></ol><ul><li>找到路由器管理页面中的 转发规则、NAT 设置 或类似选项（名称因品牌而异）。</li><li>添加一条新的端口转发规则：<ul><li>外部端口（External Port）：在公网访问时使用的端口（例如 80）。</li><li>内部端口（Internal Port）：服务器监听的端口（例如 80）。</li><li>内部 IP 地址（Internal IP）：服务器的 IP 地址（例如</li><li>协议类型（Protocol）：选择 TCP、UDP 或 TCP/UDP（通常选择 TCP 或 TCP/UDP）。</li><li>保存设置。</li></ul></li></ul><ol start="3"><li><p>查看公网ip地址：在路由器状态页或通过IP 检测网站查看路由器的公网 IP 地址。</p></li><li><p>测试，例如ssh登录：<code>ssh -p &lt;公网端口&gt; user@&lt;公网IP&gt;</code>（端口转发设置的内网端口为22）。</p></li></ol><p><strong>注意</strong></p><ul><li>如果你的公网IP是一个私有IP地址（如 192.168.x.x、172.16.x.x、10.x.x.x），那么你的路由器可能处于双重 NAT 环境中。在这种情况下，你需要拥有公网IP地址的路由器才能进行端口映射。</li><li>一般路由器重启后公网ip会变化。<br>可以使用动态域名解析服务（DDNS）或者联系互联网提供商ISP来解决这个问题。<br>或者通过一个脚本定期检查公网IP地址并通过某个外部服务器或邮件通知自己。<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">#!/bin/bashIP=$(curl -s ifconfig.me) # 获取公网IPecho "当前公网IP是：$IP"# 发送邮件或更新外部服务curl -X POST -d "ip=$IP" http://yourserver.com/update_ip</code></pre></li></ul><blockquote><p>动态域名解析（DDNS）服务<br>动态域名解析（DDNS）允许你将动态公网 IP 绑定到一个固定的域名，当 IP 变化时，DDNS 服务会自动更新域名指向的 IP 地址。</p></blockquote><p>配置步骤：<br>1.注册 DDNS 服务</p><ul><li>选择一个 DDNS 提供商（通常是免费的或附带在路由器服务中），如：<ul><li>No-IP</li><li>DynDNS</li><li>路由器品牌自带的 DDNS 服务（如 TP-Link、华为等）。</li></ul></li><li>注册一个域名，例如 myserver.ddns.net。</li></ul><ol start="2"><li>   配置路由器的 DDNS</li></ol><ul><li>登录路由器管理界面。</li><li>找到 DDNS 设置（通常在网络或高级设置下）。</li><li>输入 DDNS 提供商信息：<ul><li>服务提供商（例如 No-IP、DynDNS）。</li><li>注册的域名（例如 myserver.ddns.net）。</li><li>登录凭据（用户名和密码）。</li><li>启用 DDNS。</li></ul></li></ul><ol start="3"><li>   验证配置</li></ol><ul><li>检查DDNS状态，确认域名已成功绑定到当前公网IP。</li><li>之后，通过 <a href="http://myserver.ddns.net/">http://myserver.ddns.net</a>:&lt;端口号&gt; 访问你的服务器。</li></ul><h2 id="无路由器管理权限"><a href="#无路由器管理权限" class="headerlink" title="无路由器管理权限"></a>无路由器管理权限</h2><p><strong>使用第三方内网穿透服务</strong>，绕过公网 IP 限制。例如：</p><ul><li><strong>Ngrok</strong>：通过创建一个临时的公网地址，将流量转发到你的服务器。</li><li><strong>Cloudflare Tunnel（Argo Tunnel）</strong>：直接通过 Cloudflare 提供的域名访问你的服务器。</li><li><strong>Tailscale / ZeroTier</strong>：创建虚拟私有网络（VPN）实现远程访问。<br>这些服务可以帮助你在无法访问路由器设置的情况下，实现公网访问内网服务器的需求。但一般需要收费。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 网络配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 端口转发 </tag>
            
            <tag> 内网穿透 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux文件、磁盘以及扩容方式</title>
      <link href="/2025/01/07/linux-wen-jian-ci-pan-yi-ji-kuo-rong-fang-shi/"/>
      <url>/2025/01/07/linux-wen-jian-ci-pan-yi-ji-kuo-rong-fang-shi/</url>
      
        <content type="html"><![CDATA[<h2 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h2><h3 id="文件系统类型"><a href="#文件系统类型" class="headerlink" title="文件系统类型"></a>文件系统类型</h3><p>通过<br><code>df -T /home</code>或者<code>lsblk -f</code>命令可以查看文件系统类型。</p><ul><li><strong>ext4</strong>：Linux 常用的文件系统，支持大文件和高性能。</li><li><strong>XFS</strong>：用于高性能、高并发的场景。</li><li><strong>btrfs</strong>：支持高级功能，如快照和子卷。</li><li><strong>FAT32 和 NTFS</strong>：可用作跨平台兼容，但通常用于外部设备。</li></ul><h3 id="挂载点"><a href="#挂载点" class="headerlink" title="挂载点"></a>挂载点</h3><ul><li><strong>挂载点</strong>：将磁盘分区与目录关联，使得目录中的文件实际存储在对应的磁盘分区中。</li></ul><h4 id="挂载与卸载"><a href="#挂载与卸载" class="headerlink" title="挂载与卸载"></a>挂载与卸载</h4><ul><li><strong>挂载</strong>：<code>sudo mount /dev/sdXn /mnt/new_home</code>, /dev/sdXn 是分区路径，/mnt/new_home 是临时挂载点。</li><li><strong>卸载</strong>：<code>sudo umount /mnt/new_home</code>。</li></ul><h4 id="查看当钱挂载情况"><a href="#查看当钱挂载情况" class="headerlink" title="查看当钱挂载情况"></a>查看当钱挂载情况</h4><p><code>mount | grep /home</code></p><p><code>lsblk</code></p><h4 id="持久化挂载配置"><a href="#持久化挂载配置" class="headerlink" title="持久化挂载配置"></a>持久化挂载配置</h4><p>编辑 /etc/fstab，添加新分区的信息，例如：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">UUID=xxxxx-xxxxx-xxxxx /home ext4 defaults 0 2</code></pre><p>使用 <code>blkid</code> 获取分区 UUID。<br>UUID=xxxxx-xxxxx-xxxxx 是分区的 UUID，/home 是挂载点，ext4 是文件系统类型，defaults 是默认挂载选项，0 是备份级别，2 是文件系统检查顺序。</p><h3 id="分区表类型"><a href="#分区表类型" class="headerlink" title="分区表类型"></a>分区表类型</h3><p>分区表类型决定了磁盘分区的布局方式</p><ul><li>MBR（Master Boot Record）：<ul><li>支持最大 2TB 磁盘，分区数量限制为 4 个主分区（或使用扩展分区）。</li><li>较旧的系统中常见。</li></ul></li><li>GPT（GUID Partition Table）：<ul><li>支持大于 2TB 的磁盘，分区数量几乎无限制。</li><li>推荐使用 GPT，尤其是现代系统和大容量磁盘。</li></ul></li></ul><p>如何查看分区表类型：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo fdisk -l</code></pre><p>输出中会显示磁盘使用的分区表类型（MBR 或 GPT）。</p><h3 id="逻辑卷管理（LVM）"><a href="#逻辑卷管理（LVM）" class="headerlink" title="逻辑卷管理（LVM）"></a>逻辑卷管理（LVM）</h3><p>LVM（Logical Volume Manager）提供了灵活的存储管理能力，是现代 Linux 系统推荐的存储管理工具。</p><h4 id="LVM-组成："><a href="#LVM-组成：" class="headerlink" title="LVM 组成："></a>LVM 组成：</h4><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250107042326.png"></p><ul><li>Physical Volume (PV)：物理卷，对应实际磁盘分区（如 /dev/sda1）。</li><li>Volume Group (VG)：卷组，将多个 PV 组合成一个存储池。</li><li>Logical Volume (LV)：逻辑卷，在卷组中划分出的存储块，类似于分区。</li></ul><h4 id="LVM-的优势："><a href="#LVM-的优势：" class="headerlink" title="LVM 的优势："></a>LVM 的优势：</h4><ul><li><strong>动态调整</strong>：支持动态调整（扩展或缩小）逻辑卷。</li><li><strong>存储池</strong>：可以将多个物理磁盘组合成一个逻辑存储池。</li><li><strong>快照和备份</strong>：提供快照和备份功能。</li></ul><h4 id="如何查看-LVM-配置"><a href="#如何查看-LVM-配置" class="headerlink" title="如何查看 LVM 配置"></a>如何查看 LVM 配置</h4><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo vgdisplaysudo lvdisplay</code></pre><h3 id="块设备路径"><a href="#块设备路径" class="headerlink" title="块设备路径"></a>块设备路径</h3><p>块设备路径是 Linux 用于标识磁盘和分区的名称。</p><h4 id="路径格式"><a href="#路径格式" class="headerlink" title="路径格式"></a>路径格式</h4><ul><li>/dev/sdX：传统磁盘路径，X 是磁盘序号。</li><li>/dev/nvmeXnY：NVMe 磁盘路径，X 是设备号，Y 是分区号。</li><li>/dev/mapper/vg_name-lv_name：LVM 的逻辑卷路径。</li></ul><h4 id="如何查看块设备路径："><a href="#如何查看块设备路径：" class="headerlink" title="如何查看块设备路径："></a>如何查看块设备路径：</h4><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">lsblk 查看磁盘和分区的结构。blkid 查看分区的文件系统和 UUID。</code></pre><p>可以通过<code>fdisk / parted</code> 等工具管理分区。</p><h2 id="Linux和Windows在磁盘分区管理上的区别"><a href="#Linux和Windows在磁盘分区管理上的区别" class="headerlink" title="Linux和Windows在磁盘分区管理上的区别"></a>Linux和Windows在磁盘分区管理上的区别</h2><h3 id="分区类型和布局"><a href="#分区类型和布局" class="headerlink" title="分区类型和布局"></a>分区类型和布局</h3><table><thead><tr><th><strong>系统</strong></th><th><strong>分区类型</strong></th><th><strong>分区结构</strong></th><th><strong>挂载点</strong></th></tr></thead><tbody><tr><td><strong>Linux</strong></td><td>- 使用主分区（Primary）、扩展分区（Extended）和逻辑分区（Logical）的结构<br>- GPT（GUID分区表）支持 128 个以上的分区</td><td>- 分区通常用设备文件表示，如 <code>/dev/sda1</code>（表示第一个硬盘的第一个分区）<br>- 文件系统可以跨越多个分区（如通过 LVM 实现逻辑卷管理）</td><td>- 无固定的驱动器符号（如 Windows 的 C:、D:）<br>- 所有分区整合到一个目录树下，例如根目录 <code>/</code></td></tr><tr><td><strong>Windows</strong></td><td>- 支持 MBR（最多 4 个主分区）和 GPT（支持更多分区）</td><td>- 分区通常表示为独立的驱动器符号，如 C:、D:</td><td>- 每个分区是独立的逻辑单元<br>- 支持通过挂载点功能将分区挂载为 NTFS 文件系统下的文件夹，但不常用</td></tr></tbody></table><h3 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h3><table><thead><tr><th><strong>系统</strong></th><th><strong>常见文件系统</strong></th><th><strong>灵活性</strong></th></tr></thead><tbody><tr><td><strong>Linux</strong></td><td>- <strong>ext4</strong>: 现代 Linux 的默认文件系统，支持大文件和高性能<br>- <strong>XFS</strong>: 用于高性能、高并发的场景<br>- <strong>btrfs</strong>: 支持高级功能，如快照和子卷<br>- <strong>FAT32 和 NTFS</strong>: 可用作跨平台兼容，但通常用于外部设备</td><td>- 多种文件系统可选，支持格式化为几乎任何文件系统<br>- 支持网络文件系统（如 NFS、CIFS）</td></tr><tr><td><strong>Windows</strong></td><td>- <strong>NTFS</strong>: 默认的主文件系统，支持高性能和安全功能<br>- <strong>FAT32 和 exFAT</strong>: 用于外部设备或某些特殊用途</td><td>- 不支持直接格式化为 Linux 原生文件系统（如 ext4）<br>- 文件系统选项相对较少</td></tr></tbody></table><h3 id="引导加载"><a href="#引导加载" class="headerlink" title="引导加载"></a>引导加载</h3><table><thead><tr><th><strong>系统</strong></th><th><strong>特点</strong></th></tr></thead><tbody><tr><td><strong>Linux</strong></td><td>- 使用 GRUB（或其他引导程序）管理启动项<br>- 支持从多种分区或系统启动，灵活性高<br>- 根分区（/）和引导分区（如 /boot）可以分离<br>- 支持双系统或多系统并存</td></tr><tr><td><strong>Windows</strong></td><td>- 使用 Windows Boot Manager 管理引导<br>- 通常依赖特定的系统分区（System Reserved 或 EFI 分区）<br>- 双系统支持有限，需第三方引导管理器（如 GRUB）协助</td></tr></tbody></table><h3 id="分区工具"><a href="#分区工具" class="headerlink" title="分区工具"></a>分区工具</h3><table><thead><tr><th><strong>系统</strong></th><th><strong>特点</strong></th></tr></thead><tbody><tr><td><strong>Linux</strong></td><td>- 命令行工具丰富：<br>  - <strong>fdisk</strong>: 适用于 MBR 分区表<br>  - <strong>gdisk</strong>: 适用于 GPT 分区表<br>  - <strong>parted 和 gparted</strong>: 高级分区管理工具<br>- 灵活支持命令行和图形界面<br>- 提供 LVM（逻辑卷管理）支持，便于动态调整分区大小</td></tr><tr><td><strong>Windows</strong></td><td>- 图形化工具为主：<br>  - <strong>磁盘管理</strong>（Disk Management）是主要的分区工具<br>  - 命令行工具如 <strong>diskpart</strong><br>- 相较 Linux，动态调整分区大小的灵活性较低</td></tr></tbody></table><h3 id="文件路径和符号"><a href="#文件路径和符号" class="headerlink" title="文件路径和符号"></a>文件路径和符号</h3><table><thead><tr><th><strong>系统</strong></th><th><strong>特点</strong></th></tr></thead><tbody><tr><td><strong>Linux</strong></td><td>- 文件路径区分大小写，例如 <code>/home</code> 和 <code>/Home</code> 是不同的路径<br>- 使用正斜杠 <code>/</code> 作为路径分隔符<br>- 支持符号链接和硬链接</td></tr><tr><td><strong>Windows</strong></td><td>- 文件路径不区分大小写，例如 <code>C:\Users</code> 和 <code>C:\users</code> 是同一路径<br>- 使用反斜杠 <code>\</code> 作为路径分隔符<br>- 支持快捷方式（功能类似于符号链接）</td></tr></tbody></table><h2 id="Linux-根目录"><a href="#Linux-根目录" class="headerlink" title="Linux 根目录"></a>Linux 根目录</h2><p>根据 **Filesystem Hierarchy Standard (FHS)**，Linux 的根目录 <code>/</code> 下存在一系列标准化的目录，以下是常见目录及其作用说明。</p><h3 id="1-bin-（Binary）"><a href="#1-bin-（Binary）" class="headerlink" title="1. /bin （Binary）"></a>1. <code>/bin</code> （Binary）</h3><ul><li>存放系统启动时或单用户模式下的基本用户命令。</li><li>包括常用的命令，如 <code>ls</code>、<code>cp</code>、<code>mv</code>、<code>cat</code> 等。</li><li>可供所有用户访问。</li><li></li></ul><hr><h3 id="2-boot-（Boot）"><a href="#2-boot-（Boot）" class="headerlink" title="2. /boot （Boot）"></a>2. <code>/boot</code> （Boot）</h3><ul><li>存放系统启动所需的文件，包括引导加载程序（如 GRUB）的配置文件和内核文件。</li><li>常见文件：<ul><li><code>vmlinuz</code>：压缩内核文件。</li><li><code>initrd.img</code>：初始 RAM 磁盘镜像。</li><li><code>grub</code> 目录：GRUB 引导加载程序的配置文件。</li></ul></li></ul><hr><h3 id="3-dev-（Device）"><a href="#3-dev-（Device）" class="headerlink" title="3. /dev （Device）"></a>3. <code>/dev</code> （Device）</h3><ul><li>包含设备文件，表示系统中的硬件设备。</li><li>设备文件通常通过 <code>/dev</code> 提供接口，例如：<ul><li><code>/dev/sda</code>：第一个 SATA 硬盘。</li><li><code>/dev/null</code>：空设备。</li><li><code>/dev/tty</code>：终端接口。</li></ul></li></ul><hr><h3 id="4-etc-（Et-Cetera）"><a href="#4-etc-（Et-Cetera）" class="headerlink" title="4. /etc （Et Cetera）"></a>4. <code>/etc</code> （Et Cetera）</h3><ul><li>存放系统配置文件。</li><li>通常只由系统管理员（<code>root</code> 用户）修改。</li><li>常见文件和目录：<ul><li><code>/etc/passwd</code>：用户信息。</li><li><code>/etc/fstab</code>：文件系统挂载信息。</li><li><code>/etc/network/interfaces</code>：网络配置。</li></ul></li></ul><hr><h3 id="5-home"><a href="#5-home" class="headerlink" title="5. /home"></a>5. <code>/home</code></h3><ul><li>存放普通用户的主目录，每个用户一个子目录。</li><li>示例：<ul><li><code>/home/user1</code>：用户 <code>user1</code> 的主目录。</li></ul></li><li>用户可以在其主目录中存储文件、配置数据等。</li></ul><hr><h3 id="6-lib-（Library）"><a href="#6-lib-（Library）" class="headerlink" title="6. /lib （Library）"></a>6. <code>/lib</code> （Library）</h3><ul><li>存放系统运行所需的共享库（类似于 Windows 的 <code>.dll</code> 文件）。</li><li>包括 <code>/bin</code> 和 <code>/sbin</code> 中的可执行文件所依赖的库。</li></ul><hr><h3 id="7-media"><a href="#7-media" class="headerlink" title="7. /media"></a>7. <code>/media</code></h3><ul><li>挂载点目录，用于自动挂载外部设备（如 USB 驱动器、光盘）。</li><li>当插入外部存储设备时，系统会在 <code>/media</code> 下创建一个挂载点。</li></ul><hr><h3 id="8-mnt"><a href="#8-mnt" class="headerlink" title="8. /mnt"></a>8. <code>/mnt</code></h3><ul><li>临时挂载点目录。</li><li>系统管理员可以手动挂载文件系统到该目录下。</li><li>用于临时挂载而非自动化。</li></ul><hr><h3 id="9-opt-（Optional）"><a href="#9-opt-（Optional）" class="headerlink" title="9. /opt （Optional）"></a>9. <code>/opt</code> （Optional）</h3><ul><li>存放可选的第三方软件包。</li><li>一些软件会安装在 <code>/opt</code> 下，例如 <code>/opt/myapp</code>。</li></ul><hr><h3 id="10-proc"><a href="#10-proc" class="headerlink" title="10. /proc"></a>10. <code>/proc</code></h3><ul><li>虚拟文件系统，存储系统内核和进程相关信息。</li><li>动态生成，不占用实际磁盘空间。</li><li>示例：<ul><li><code>/proc/cpuinfo</code>：CPU 信息。</li><li><code>/proc/meminfo</code>：内存使用信息。</li></ul></li></ul><hr><h3 id="11-root"><a href="#11-root" class="headerlink" title="11. /root"></a>11. <code>/root</code></h3><ul><li><code>root</code> 用户的主目录，与普通用户的 <code>/home/username</code> 对应。</li><li>通常只有 <code>root</code> 用户有权限访问。</li><li>需要通过<code>sudo passwd root</code>设置密码才能登录。</li></ul><hr><h3 id="12-run"><a href="#12-run" class="headerlink" title="12. /run"></a>12. <code>/run</code></h3><ul><li>临时文件系统，用于存储系统运行时生成的文件（如进程 ID 文件、套接字）。</li><li>系统重启后该目录会被清空。</li></ul><hr><h3 id="13-sbin-（System-Binary）"><a href="#13-sbin-（System-Binary）" class="headerlink" title="13. /sbin （System Binary）"></a>13. <code>/sbin</code> （System Binary）</h3><ul><li>存放系统管理员使用的基本命令，例如：<ul><li><code>fsck</code>：文件系统检查。</li><li><code>reboot</code>：重启系统。</li></ul></li><li>一般普通用户无权直接执行。</li></ul><hr><h3 id="14-srv-（Service）"><a href="#14-srv-（Service）" class="headerlink" title="14. /srv （Service）"></a>14. <code>/srv</code> （Service）</h3><ul><li>存储与系统提供的服务（如 HTTP、FTP）相关的数据。</li><li>示例：<code>/srv/www</code> 可能存放一个 Web 服务器的数据。</li></ul><hr><h3 id="15-sys"><a href="#15-sys" class="headerlink" title="15. /sys"></a>15. <code>/sys</code></h3><ul><li>虚拟文件系统，提供内核和硬件设备的信息。</li><li>和 <code>/proc</code> 类似，但更注重设备相关的信息。</li></ul><hr><h3 id="16-tmp-（Temporary）"><a href="#16-tmp-（Temporary）" class="headerlink" title="16. /tmp （Temporary）"></a>16. <code>/tmp</code> （Temporary）</h3><ul><li>用于存储临时文件。</li><li>文件通常会在系统重启时清空。</li></ul><hr><h3 id="17-usr-（User-System-Resources）"><a href="#17-usr-（User-System-Resources）" class="headerlink" title="17. /usr （User System Resources）"></a>17. <code>/usr</code> （User System Resources）</h3><ul><li>包含与用户相关的文件和资源：<ul><li><code>/usr/bin</code>：用户可用的命令。</li><li><code>/usr/lib</code>：共享库。</li><li><code>/usr/share</code>：共享数据（如文档、图标）。</li><li><code>/usr/local</code>：本地安装的软件。</li></ul></li></ul><hr><h3 id="18-var-（Variable）"><a href="#18-var-（Variable）" class="headerlink" title="18. /var （Variable）"></a>18. <code>/var</code> （Variable）</h3><ul><li>存放经常变化的文件。</li><li>示例：<ul><li><code>/var/log</code>：日志文件。</li><li><code>/var/spool</code>：任务队列（如打印任务）。</li></ul></li></ul><hr><h2 id="Ubuntu-系统分区建议"><a href="#Ubuntu-系统分区建议" class="headerlink" title="Ubuntu 系统分区建议"></a>Ubuntu 系统分区建议</h2><h3 id="基本分区方案"><a href="#基本分区方案" class="headerlink" title="基本分区方案"></a>基本分区方案</h3><p>通常情况下，建议以下几个分区：</p><h4 id="1-（根分区）"><a href="#1-（根分区）" class="headerlink" title="1. /（根分区）"></a>1. <code>/</code>（根分区）</h4><ul><li><strong>用途</strong>：存放操作系统的核心文件和默认安装的软件。</li><li><strong>建议大小</strong>：<ul><li><strong>20-30GB</strong>：普通桌面用户。</li><li><strong>40-50GB</strong>：需要安装大量软件或开发环境的用户。</li></ul></li><li><strong>文件系统</strong>：<code>ext4</code>（推荐）。</li><li><strong>注意</strong>：所有未分配到其他分区的内容都会默认存储在根分区中。</li></ul><h4 id="2-swap（交换分区）"><a href="#2-swap（交换分区）" class="headerlink" title="2. swap（交换分区）"></a>2. <code>swap</code>（交换分区）</h4><ul><li><strong>用途</strong>：虚拟内存，用于内存不足时提供临时存储；也用于休眠功能。</li><li><strong>建议大小</strong>：<ul><li>如果 <strong>不使用休眠功能</strong>：<ul><li>RAM &lt; 4GB：<strong>等于2倍RAM大小</strong>。</li><li>RAM 在 4-8GB：<strong>等于RAM大小</strong>。</li><li>RAM &gt; 8GB：<strong>4-8GB</strong> 通常足够。</li></ul></li><li>如果 <strong>使用休眠功能</strong>：<ul><li>至少等于RAM大小。</li></ul></li></ul></li><li><strong>文件系统</strong>：专用交换空间，无文件系统。</li><li><strong>注意</strong>：现代 Ubuntu 支持使用交换文件（无需单独分区），可在安装后配置。</li></ul><h4 id="3-home（用户目录）"><a href="#3-home（用户目录）" class="headerlink" title="3. /home（用户目录）"></a>3. <code>/home</code>（用户目录）</h4><ul><li><strong>用途</strong>：存储用户数据（如文档、配置、下载等），便于系统重装时保留用户数据。</li><li><strong>建议大小</strong>：<ul><li>普通桌面用户：<strong>50GB 或更多</strong>。</li><li>如果硬盘空间充裕，可分配 **100GB+**，以支持长期使用。</li><li>服务器或开发用户：按实际需求分配。</li></ul></li><li><strong>文件系统</strong>：<code>ext4</code>（推荐）。</li></ul><h4 id="4-boot（可选）"><a href="#4-boot（可选）" class="headerlink" title="4. /boot（可选）"></a>4. <code>/boot</code>（可选）</h4><ul><li><strong>用途</strong>：存放引导加载程序和内核文件。</li><li><strong>建议大小</strong>：<ul><li><strong>500MB-1GB</strong>：普通用户。</li><li><strong>1-2GB</strong>：如果需要保留多个内核版本。</li></ul></li><li><strong>文件系统</strong>：<code>ext4</code>。</li></ul><h4 id="5-efi（如果是-UEFI-系统）"><a href="#5-efi（如果是-UEFI-系统）" class="headerlink" title="5. /efi（如果是 UEFI 系统）"></a>5. <code>/efi</code>（如果是 UEFI 系统）</h4><ul><li><strong>用途</strong>：存放 UEFI 引导程序。</li><li><strong>建议大小</strong>：<ul><li><strong>300-500MB</strong> 通常足够。</li></ul></li><li><strong>文件系统</strong>：<code>FAT32</code>。</li><li><strong>注意</strong>：如果是多系统环境，EFI 分区可以共享。</li></ul><h4 id="6-var（可选）"><a href="#6-var（可选）" class="headerlink" title="6. /var（可选）"></a>6. <code>/var</code>（可选）</h4><ul><li><strong>用途</strong>：存放日志文件、缓存数据、邮件队列等（特别是服务器环境）。</li><li><strong>建议大小</strong>：<ul><li><strong>5-10GB</strong>：普通用户。</li><li><strong>20-50GB</strong>：如果运行数据库、Web服务器或其他高日志生成服务。</li></ul></li><li><strong>文件系统</strong>：<code>ext4</code>。</li></ul><h4 id="7-tmp（可选）"><a href="#7-tmp（可选）" class="headerlink" title="7. /tmp（可选）"></a>7. <code>/tmp</code>（可选）</h4><ul><li><strong>用途</strong>：存放临时文件。</li><li><strong>建议大小</strong>：<ul><li><strong>2-4GB</strong>：普通用户。</li><li><strong>10GB 或更多</strong>：需要处理大量临时文件（如视频编辑、编译大型项目）。</li></ul></li><li><strong>文件系统</strong>：<code>ext4</code> 或 <code>tmpfs</code>（内存文件系统）。</li></ul><h4 id="8-数据分区（可选）"><a href="#8-数据分区（可选）" class="headerlink" title="8. 数据分区（可选）"></a>8. 数据分区（可选）</h4><ul><li><strong>用途</strong>：独立存储大文件或共享数据（如 <code>/mnt/data</code>）。</li><li><strong>建议大小</strong>：根据需求调整。</li></ul><hr><h3 id="分区建议"><a href="#分区建议" class="headerlink" title="分区建议"></a>分区建议</h3><ol><li><p><strong>根分区大小要合理</strong>：</p><ul><li>根分区存放系统和软件文件，除单独分配空间的挂载点，其余内容都存储在根分区中。</li><li>如果空间不足，可能导致系统无法更新或安装新软件。</li></ul></li><li><p><strong>优先为 <code>/home</code> 留出空间</strong>：</p><ul><li><code>/home</code> 是用户数据的主要存储位置，建议分配尽可能大的空间。</li></ul></li><li><p><strong>使用 LVM</strong>：</p><ul><li>如果磁盘使用量不确定，使用 LVM 便于后续扩展。</li></ul></li><li><p><strong>EFI 分区共享</strong>：</p><ul><li>如果有多系统（如 Windows 和 Linux），可以共享一个 <code>/boot/efi</code> 分区。</li></ul></li><li><p><strong>根据用途优化分区</strong>：</p><ul><li>桌面用户可以简化分区结构。</li><li>服务器或开发环境可以根据需要细化分区。</li></ul></li></ol><hr><h1 id="扩容方式"><a href="#扩容方式" class="headerlink" title="扩容方式"></a>扩容方式</h1><h2 id="新建磁盘分区并挂载到新的挂载点上"><a href="#新建磁盘分区并挂载到新的挂载点上" class="headerlink" title="新建磁盘分区并挂载到新的挂载点上"></a>新建磁盘分区并挂载到新的挂载点上</h2><p>主要使用的命令：<code>fdisk</code>、<code>mkfs</code>、<code>mount</code>、<code>umount</code>、<code>df</code>、<code>lsblk</code></p><ol><li><p><strong>查看磁盘</strong>：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">lsblk</code></pre><p> <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250107172719.png"><br> 可以看出里面有三个硬盘：sda/nvme0n1/nvme1n1，目前我的Linux文件系统全部都在sda中的部分分区中，另外nvme0n1只设置了一个/boot/efi的挂载点。现在我想从nvme1n1中分配一部分空间给sda/ada6的/home分区。</p></li><li><p><strong>磁盘分区并挂载</strong>：<br> 首先可以查看一下这个磁盘的使用情况</p> <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo fdisk -l /dev/nvme1n1</code></pre><p> <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250107173236.png"><br> 这个盘之前在windows系统上使用，目前的分区也已经占满了全部空间，所以需要删除一些分区或者直接使用之前的分区，如果要删除的话我们可以重新对这个盘进行分区，借助fdisk或者parted应该都可以：</p><ul><li>删除分区：  <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo fdisk /dev/nvme1n1</code></pre><ul><li>输入 d， 逐一删除所有分区。</li><li>输入 w 保存更改。</li></ul></li><li>新建分区  <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo fdisk /dev/nvme1n1</code></pre><ul><li>输入 n 创建新分区</li><li>按提示选择分区类型（通常是主分区）。</li><li>提供起始扇区和结束扇区（默认值即可使用未分配的全部空间）。</li><li>确认并完成创建。</li></ul></li><li>格式化分区  <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo mkfs.ext4 /dev/nvme1n1p1</code></pre></li><li>挂载分区  <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo mount /dev/nvme1n1 /mnt/new_home</code></pre>  注意前面我们说过/mnt是临时挂载点，我们可以新建一个/home/data挂载点来实现对/home目录的扩容，但是其实这样在系统重启后依旧是临时的，每次都重启需要手动挂载才能正常使用分区内容。</li><li>持久化挂载<ul><li>获取分区 UUID：  <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo blkid /dev/nvme1n1p1</code></pre></li><li>编辑 /etc/fstab 文件：<br>  添加以下行，将新分区挂载到 /home/data：  <pre class="line-numbers language-none"><code class="language-none">UUID=xxxx-xxxx-xxxx-xxxx /home/data ext4 defaults 0 2</code></pre></li><li>测试 /etc/fstab 配置：  <pre class="line-numbers language-none"><code class="language-none">sudo mount -a</code></pre></li><li>验证是否挂载：<code>df -h</code></li></ul></li></ul></li><li><p>新挂载点权限设置<br> 上面的案例中/home 下的子目录（ /home/data）会独立使用新分区的存储空间，不会直接扩展 /home 主目录的可用空间。<br> 默认情况下，挂载后的 /home/data 可能由 root 用户拥有。根据需要修改目录权限：</p> <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo chown -R &lt;username&gt;:&lt;group&gt; /home/data 或者chmod -R 权限 /home/data   （权限由三位数字组成，分别对应所有者/组/其他用户的权限，单个数字权限为r=4，w=2，x=1的和）</code></pre></li><li><p>数据迁移<br> 如果 /home 中已有需要迁移到 /home/data 的数据，可以使用 rsync 进行迁移：</p> <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo rsync -av /home/old_data /home/data</code></pre></li></ol><h2 id="LVM扩容"><a href="#LVM扩容" class="headerlink" title="LVM扩容"></a>LVM扩容</h2><p>更新中。。。。</p><h2 id="直接扩容现有分区"><a href="#直接扩容现有分区" class="headerlink" title="直接扩容现有分区"></a>直接扩容现有分区</h2><h3 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h3><ul><li>备份数据</li></ul><p>调整分区可能会导致数据丢失，建议备份 /home 分区和其他重要数据。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo rsync -aXS /home/ /backup/home/</code></pre><ul><li>查看分区布局<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">lsblksudo fdisk -l</code></pre>看看 /home 分区（如 /dev/sda6）的后方存在未分配空间。</li><li>确保分区未挂载：<ul><li>如果 /home 是活动分区，需要在单用户模式或通过 Live CD/USB 执行操作。</li><li>在 Live 环境中挂载根分区，并使用 chroot 进入系统环境。</li></ul></li></ul><h3 id="如果-home-分区（如-dev-sda6）的后方位置存在未分配空间"><a href="#如果-home-分区（如-dev-sda6）的后方位置存在未分配空间" class="headerlink" title="如果/home 分区（如 /dev/sda6）的后方位置存在未分配空间"></a>如果/home 分区（如 /dev/sda6）的后方位置存在未分配空间</h3><p>因为我是想在在另一个磁盘上扩充，所以后面的方法不适用于我的这个需求，如果满足挂载/home的后方还有连续空间的话优先推荐这种方式，可以直接扩容 /dev/sda6 分区大小即可，有以下两种方式：</p><h4 id="使用gparted（图形化界面）"><a href="#使用gparted（图形化界面）" class="headerlink" title="使用gparted（图形化界面）"></a>使用gparted（图形化界面）</h4><p>gparted 是调整分区的图形化工具，操作简单且更安全。</p><ul><li>安装：<code>sudo apt install gparted</code></li><li>启动：<code>sudo gparted</code></li><li>选择 /dev/sda6 分区，右键选择 Resize/Move。</li></ul><h4 id="使用fdisk"><a href="#使用fdisk" class="headerlink" title="使用fdisk"></a>使用fdisk</h4><ul><li>启动 fdisk：<br> <code>sudo fdisk /dev/sda</code></li><li>删除并重新创建分区：<ul><li>输入 d，选择 /dev/sda6 分区。</li><li>输入 n，创建新分区，起始扇区与旧分区相同，结束扇区扩大到未分配空间末尾。</li><li>输入 w 保存更改。</li></ul></li><li>刷新分区表：<br>  <code>sudo partprobe</code></li></ul><h4 id="扩展文件系统"><a href="#扩展文件系统" class="headerlink" title="扩展文件系统"></a>扩展文件系统</h4><p>在采用上面的某一种方式后调整分区后，文件系统大小仍然是原来的大小，需要扩展文件系统以使用新的空间。</p><ul><li>扩展 ext4 文件系统</li></ul><p>如果 /home 分区是 ext4，使用 resize2fs 扩展：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo resize2fs /dev/sda6```   - 扩展 xfs 文件系统如果 /home 分区是 xfs，确保分区已挂载，然后使用 xfs_growfs 扩展：```shellsudo mount /dev/sda6 /homesudo xfs_growfs /home</code></pre><h4 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h4><ul><li>检查分区大小是否扩展成功：<code>df -h /home</code></li><li>确认分区的文件系统健康状态：<code>sudo fsck /dev/sda6</code></li></ul><h3 id="如果-home-分区（如-dev-sda6）的后方位置不存在未分配空间"><a href="#如果-home-分区（如-dev-sda6）的后方位置不存在未分配空间" class="headerlink" title="如果/home 分区（如 /dev/sda6）的后方位置不存在未分配空间"></a>如果/home 分区（如 /dev/sda6）的后方位置不存在未分配空间</h3><p>如果原有的留给/home挂载点的分区已经空间不足且原来空间很小，可以考虑换一个更大的硬盘然后对/home进行迁移。</p><h4 id="切换用户"><a href="#切换用户" class="headerlink" title="切换用户"></a>切换用户</h4><p>因为我们现在要扩充/home分区，所以需要切换到root用户下进行操作，使用命令<code>sudo -i</code>切换到root用户。</p><p>另外，Live环境或者单用户模式也都可以。</p><h4 id="准备新分区"><a href="#准备新分区" class="headerlink" title="准备新分区"></a>准备新分区</h4><ul><li>如果新硬盘还没有新分区的话可以新建一个分区：<code>sudo fdisk /dev/nvme1n1</code>, 输入n新建分区，输入w保存更改。</li><li>格式化分区：<code>sudo mkfs.ext4 /dev/nvme1n1p1</code></li><li>挂载分区：<code>sudo mount /dev/nvme1n1p1 /mnt/new_home</code>, 将其挂载到/mnt/new_home目录下。</li></ul><h4 id="迁移数据"><a href="#迁移数据" class="headerlink" title="迁移数据"></a>迁移数据</h4><ul><li>迁移数据：<code>sudo rsync -aXS /home/ /mnt/new_home/</code><br><strong>非常重要，迁移完成后一定要检查一下数据是否完整，确认无误后再进行下一步操作。</strong><br>-a：归档模式，保留权限、时间戳等。-X：保留扩展属性。-S：处理稀疏文件。</li></ul><h4 id="更新挂载点"><a href="#更新挂载点" class="headerlink" title="更新挂载点"></a>更新挂载点</h4><ul><li>卸载原有 /home 分区：<code>sudo umount /home</code><ul><li>此时可能遇到/home忙碌的问题，可以使用<code>lsof /home</code>查看一下/home目录下的进程，但是如果有系统进程的话就无法卸载了，所以还是Live环境或者单用户模式比较方便。</li></ul></li><li>如果之前临时挂载点设置了永久挂载，需要卸载临时的new_home：<code>sudo umount /mnt/new_home</code>，这个如果不卸载的话会可能导致/home和new_home都在同一个分区下，非常不安全。</li><li>挂载新分区到 /home：<code>sudo mount /dev/nvme1n1p1 /home</code></li><li>获取新分区 UUID：<code>sudo blkid /dev/nvme1n1p1</code></li><li>持久化挂载：编辑 /etc/fstab，添加新分区的 UUID 信息。<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">UUID=xxxx-xxxx-xxxx-xxxx /home ext4 defaults 0 2</code></pre></li></ul><h4 id="验证-1"><a href="#验证-1" class="headerlink" title="验证"></a>验证</h4><ul><li>/home 分区是否正常挂载：<code>df -h /home</code><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo mount -adf -h /home</code></pre></li><li>旧的临时目录下内容和新的/home目录下内容是否是在一个分区下：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">df /mnt/new_home</code></pre>检查清楚后可以删除/mnt/new_home目录下的内容。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 磁盘 </tag>
            
            <tag> 扩容 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python-模块/包的导入</title>
      <link href="/2024/12/27/python-mo-kuai-bao-de-dao-ru/"/>
      <url>/2024/12/27/python-mo-kuai-bao-de-dao-ru/</url>
      
        <content type="html"><![CDATA[<h1 id="模块Module和包Package"><a href="#模块Module和包Package" class="headerlink" title="模块Module和包Package"></a>模块Module和包Package</h1><p>在Python中，一个 <strong>.py文件</strong> 就称之为一个模块（Module）。Module的目的是代码的划分、管理以及代码重用，在一个Module中可以存在多个类、函数甚至是需要预执行的脚本。<br>模块一共三种：python标准库、第三方模块、应用程序自定义模块。<br>相同名字的函数和变量完全可以分别存在不同的模块中，不必考虑名字会与其他模块冲突。但是<strong>不要与内置函数名字冲突</strong>。</p><p>相关概念：</p><ul><li><code>_pycache__</code>：Python3.2版本引入的，用于存放模块的编译后的字节码文件，以提高模块的加载速度。</li><li><code>__init__.py</code>：模块的初始化文件，可以为空，也可以有代码。<strong>当一个文件夹下有__init__.py文件时，这个文件夹就会被当作一个包来处理。</strong></li><li><code>dir()</code>：查看模块的所有属性和方法。比如：  <pre class="line-numbers language-python" data-language="python"><code class="language-python">import mathprint(dir(math))</code></pre></li><li><code>__name__</code>：模块的内置属性，用于判断模块是被导入还是直接当作脚本执行。比如：  <pre class="line-numbers language-python" data-language="python"><code class="language-python">if __name__ == '__main__':    print('This is a module')</code></pre></li></ul><h2 id="模块的查找路径"><a href="#模块的查找路径" class="headerlink" title="模块的查找路径"></a>模块的查找路径</h2><p>在 Python 中，当运行一个项目时，模块的查找路径遵循特定的顺序。这是由 <code>sys.path</code> 列表定义的，该列表按顺序列出了 Python 在查找模块时会搜索的路径。</p><p>模块查找路径的顺序</p><ul><li>当前目录：运行脚本所在的目录。如果你执行了一个脚本，那么 Python 会首先尝试从该脚本所在的目录加载模块。</li><li>环境变量 PYTHONPATH 指定的目录：如果设置了 PYTHONPATH 环境变量，其值中列出的目录将被加入查找路径。</li><li>标准库目录：Python 自带的标准库路径（例如 lib 或 Lib 目录）。</li><li>第三方包路径：通常是安装在 site-packages 目录下的路径。</li><li>内置模块：如果以上路径都找不到，Python 会尝试加载内置模块（如 sys、os）。</li></ul><p>示例及详细解释:<br>比如一个项目的目录结构如下：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">project/│├── script.py          # 主运行脚本├── module1.py         # 自定义模块└── subdir/    ├── module2.py     # 自定义模块    └── __init__.py    # 将 subdir 标记为包</code></pre><p>其中 <code>script.py</code> 代码如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import sysprint("Module search paths:")for path in sys.path:    print(path)import module1         # 自定义模块，位于当前目录from subdir import module2  # 从子目录中导入模块</code></pre><p>模块查找过程如下：</p><ol><li>当前目录：<ul><li>运行 python script.py 时，Python 首先查看当前目录（project/）。</li><li>找到 module1.py，并成功导入。</li></ul></li><li>   子目录导入：<ul><li>对于 from subdir import module2，Python 进入 project/subdir/ 并找到 module2.py。</li><li>因为 subdir 包含 <strong>init</strong>.py，它被认为是一个包。</li></ul></li><li>   标准库：<ul><li>如果你尝试导入如 os、sys 等模块，Python 会从标准库路径中加载。</li></ul></li><li>   第三方包：<ul><li>如果尝试导入第三方库（如 numpy），Python 会在 site-packages 目录中查找。</li></ul></li></ol><h2 id="修改模块查找路径"><a href="#修改模块查找路径" class="headerlink" title="修改模块查找路径"></a>修改模块查找路径</h2><p>可以动态修改 sys.path 来影响模块的查找路径。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import sysimport os# 查看当前路径print("Before modification:", sys.path)# 添加新路径new_path = os.path.join(os.getcwd(), 'subdir')sys.path.insert(0, new_path)print("After modification:", sys.path)# 导入模块import module2</code></pre><h2 id="与包package的关系"><a href="#与包package的关系" class="headerlink" title="与包package的关系"></a>与包package的关系</h2><p>包所在的目录需要包含一个空文件__init__.py来表明这个是一个包，所以说<strong>包是一个包含了多个模块的目录</strong>。包的目的是为了组织模块，以便更好地管理和维护代码。</p><p>在<code>__init__.py</code>文件中可以声明一些描述性的代码来变更package的特性。比如针对import *的__all__，如果某个package下有你明确不希望被引用的py文件，可以通过__all__明确说明哪些是希望引入的，这样在python处理import *时会忽略掉不在__all__列表的内容。比如：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">__all__ = ['module1', 'module2'] # 只允许导入module1和module2</code></pre><h2 id="模块的导入"><a href="#模块的导入" class="headerlink" title="模块的导入"></a>模块的导入</h2><h3 id="同级目录下的模块导入"><a href="#同级目录下的模块导入" class="headerlink" title="同级目录下的模块导入"></a>同级目录下的模块导入</h3><p>如果你要使用的模块（py文件）和当前模块在同一目录，只要import相应的文件名就好，比如在当前目录下有一个module1.py文件，那么在另一个文件中可以直接使用import module1来导入。</p><ul><li><strong>一个模块可以在当前位置import多次，但是只有第一次导入会执行内容，其他的都为引用内存</strong></li><li><strong>更改调用名称：<code>import module1 as m1</code></strong></li><li><strong>只导入模块中的部分内容：<code>from module1 import func1</code></strong></li></ul><h3 id="不同目录下的模块导入"><a href="#不同目录下的模块导入" class="headerlink" title="不同目录下的模块导入"></a>不同目录下的模块导入</h3><h4 id="子目录下的模块导入"><a href="#子目录下的模块导入" class="headerlink" title="子目录下的模块导入"></a>子目录下的模块导入</h4><p>假设目录结构如下：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">project/├── main.py├── subdir/│   ├── module2.py│   └── __init__.py</code></pre><p>方法 1：直接导入（如果子目录是一个包）<br>如果子目录包含 <code>__init__.py</code> 文件（即被视为包），可以使用以下方式导入：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"># main.pyfrom subdir import module2  # 导入子目录中的模块module2.some_function()    # 调用 module2 中的函数</code></pre><p>方法 2：修改 sys.path<br>如果子目录中没有 <code>__init__.py</code>，或者希望在没有包的情况下导入，可以通过添加子目录到 sys.path：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"># main.pyimport sysimport os# 将子目录路径添加到 sys.pathsys.path.append(os.path.join(os.getcwd(), 'subdir'))import module2  # 现在可以直接导入module2.some_function()</code></pre><h4 id="父目录下的模块导入"><a href="#父目录下的模块导入" class="headerlink" title="父目录下的模块导入"></a>父目录下的模块导入</h4><p>假设目录结构如下：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">project/├── parent_module.py└── subdir/    ├── main.py    ├── module3.py    └── __init__.py</code></pre><p>方法 1：使用 sys.path</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"># main.pyimport sysimport os# 将父目录路径添加到 sys.pathsys.path.append(os.path.dirname(os.getcwd()))import parent_module  # 现在可以导入父目录的模块parent_module.some_function()</code></pre><p>方法 2：相对导入（仅适用于包结构）</p><p>如果 subdir 是一个包（包含 <strong>init</strong>.py），可以使用相对导入：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"># main.pyfrom .. import parent_module  # 相对导入父目录的模块parent_module.some_function()</code></pre><p><strong>注意：相对导入仅在包结构中有效，且不能直接运行该脚本（需要通过 python -m 的方式运行）。</strong></p><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>1.<br><strong>当使用 <code>from module import func_b</code> 时，整个模块会被加载和执行一次(包括下划线开头的私有对象)，但只有你导入的部分会被绑定到当前的命名空间</strong><br>例如：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"># module.pyvalue = 1def func_a():    print("Function func_a is being executed")    return 0def func_b():    print("Function func_b is being executed")    b = func_a()    return b</code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python"># main.pyfrom module import func_bprint("Calling func_b() in main.py")result = func_b()print(f"Result: {result}")</code></pre><p>执行过程：<br>1.<br>from module import func_b</p><p>Python 会先找到 module.py 文件并加载整个模块：</p><ul><li>Python 会解释并执行 module.py 文件的顶层代码。</li><li>value = 1 会被执行并存储到 module 的命名空间中。</li><li>函数 func_a 和 func_b 的定义会被加载，并绑定到 module 的命名空间。</li><li>然后，func_b 会被绑定到当前模块（main.py）的命名空间。</li></ul><p><strong>注意：</strong></p><ul><li>虽然整个模块被加载，但只有 func_b 被绑定到 main.py 的命名空间中。</li><li>在加载过程中，module 的顶层代码（如赋值语句、类定义等）都会被执行一次。</li></ul><ol start="2"><li>调用 func_b()</li></ol><ul><li>Python 跳转到 module.py 中 func_b 的定义，并开始执行该函数的内容。</li><li>在 func_b 中，func_a 被调用，导致 func_a 的代码被执行。<br>完整输出：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">Calling func_b() in main.pyFunction func_b is being executedFunction func_a is being executedResult: 0</code></pre></li></ul><p>2.<br><strong><strong>init</strong>.py文件的的规范</strong><br>加入存在以下目录结构：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">project/├── my_package/│   ├── module1.py│   ├── module2.py│   └── __init__.py└── main.py</code></pre><p><code>__init__.py</code> 文件的一些写法的解释：</p><ul><li>空：仅标记为包，导入需要显式地指明模块和内容。所以使用时也是<code>import my_package.module1</code>，本质上是模块，包中的定义的__all__列表不会生效。</li><li><code>from . import module2</code>: 导入完整模块到包命名空间，使用时需通过模块名访问，本质上是模块，包中的定义的__all__列表不会生效。</li><li><code>from my_package import *</code>: 导入完整模块到包命名空间，使用时无需模块名访问，</li><li><code>from .module2 import *</code>: 将整个模块内容导入到包命名空间，使用时无需模块名。</li><li><code>from my_package.module2 import *</code>: 将整个模块内容导入到包命名空间，使用时无需模块名。</li><li><code>from module2 import *</code>: 这是种错误写法，无法找到模块。</li></ul><ol start="3"><li><code>__all__</code>在<code>__init__.py</code>中的作用：</li></ol><ul><li>当你使用 from my_package import * 时，<code>__all__</code> 定义了哪些名称会被导入到当前命名空间。</li><li>如果__all__没有定义，那么默认会导入所有非以下划线 _ 开头的对象。</li></ul><p>调用时的区别：</p><ul><li>如果在 <code>main.py</code> 中需要写模块名（例如 my_package.module2.func2()），那么 <code>__all__</code>不会限制模块中的内容。</li><li><code>__all__</code> 只在 from my_package import * 时生效，而不会影响显式导入<br>例如：<br>目录结构：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">my_package/├── __init__.py├── module1.py├── module2.py # 包含 func2 函数</code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python">#__init__.py：from .module2 import *__all__ = []  # 不对外暴露任何内容</code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python"># main.pyfrom my_package import *func2()  # 报错，因为 __all__=[] 限制了导出的内容from my_package.module2 import func2 func2()  # 正常运行from my_package import func2func2()  # 正常运行， 比较奇怪哈，但是测试了这样子是可以的</code></pre></li></ul><p>4.<br><strong>通过<code>-m</code>运行一个模块</strong><br>对于一些项目，我们可能需要通过命令行来运行一个模块，这时候可以使用<code>-m</code>参数，以支持模块的相对导入。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">project/├── my_package/│   ├── module1.py│   ├── module2.py│   └── __init__.py└── subdir/    ├── module3.py    └── __init__.py</code></pre><p>对于moulde3.py中的内容，如果写成：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">from my_package import func1 # module1.py中的函数 func1()</code></pre><p>需要通过<code>-m</code>参数来运行：<code>python -m subdir.module3</code>，这样才能正确导入my_package中的内容。</p><p>参考：</p><blockquote><p><a href="https://blog.csdn.net/Uncle_GUO/article/details/80867086">https://blog.csdn.net/Uncle_GUO/article/details/80867086</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> ImportError </tag>
            
            <tag> python模块 </tag>
            
            <tag> python包 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>好用的大模型prompt分享</title>
      <link href="/2024/12/21/hao-yong-de-da-mo-xing-prompt-fen-xiang/"/>
      <url>/2024/12/21/hao-yong-de-da-mo-xing-prompt-fen-xiang/</url>
      
        <content type="html"><![CDATA[<h1 id="中文写作"><a href="#中文写作" class="headerlink" title="中文写作"></a>中文写作</h1><h2 id="润色"><a href="#润色" class="headerlink" title="润色"></a>润色</h2><pre class="line-numbers language-none"><code class="language-none">作为一名中文学术论文写作改进助理，请用学术化的语言重写以下句子，请分条阐述，你的任务是改进所提供文本的拼写、语法、清晰、简洁和整体可读性，同时分解长句，减少重复，并提供改进建议。请只提供文本的更正版本，避免包括解释。且要求重新组织段落中的句子，使其具有逻辑性，并分条阐述。请编辑以下文本：</code></pre><h2 id="校对与完善"><a href="#校对与完善" class="headerlink" title="校对与完善"></a>校对与完善</h2><pre class="line-numbers language-none"><code class="language-none">假设你是一位专注于学术写作的校审专家。你的主要工作是检查和修改一篇题为“[PAPER TITLE]”的论文段落，目标是让论文的表达更加清晰、条理清楚。找出所有不明确、复杂或冗余的句子或短语，并提供更加精确和简洁的选择方案。特别注意文章的逻辑流动，确保每个句子都能有效支持主旨或叙述。全文要保持术语、风格和语调的统一。对于可能需要对预期读者进行说明的行业术语或专业词汇，要给出清晰的解释。完成校审后，确保该段落符合学术规范，使得论文整体上更加易读，从而提高其影响力。接下来我会逐个给你展示这篇论文的段落，你需要根据上述要求对每个段落进行校审，注意你要保留原有的引用。请在每个段落的下方写下你的修改建议。</code></pre><h2 id="公式分析生成"><a href="#公式分析生成" class="headerlink" title="公式分析生成"></a>公式分析生成</h2><pre class="line-numbers language-none"><code class="language-none">假设你是一名学术研究者，请使用学术化的用语，尽量使用数学公式表达，请帮我分析以下公式具有什么性质，有什么深入推导。请一步一步阐述：</code></pre><h2 id="改写（查重后用）"><a href="#改写（查重后用）" class="headerlink" title="改写（查重后用）"></a>改写（查重后用）</h2><ul><li>示例一：<pre class="line-numbers language-none"><code class="language-none">1.假设你是一个论文润色写作员，具有学术研究相关知识。请给改写以下文段，要求给出的文段与原文段重复率低于10%，要求新文段的任意连续40个字与原文段中任意连续40个字中重复的字小于5个。要求尽可能替换文段中词汇使其更符合学术论文表达，要求尽可能更换说法避免与原文段的重复但是要保留原文段的含义，允许重新组织句子顺序、词语顺序、段落顺序，允许改写时对句子扩写或缩减。请给出改完后的文段、给出与原文段的对比，请一步一步阐述。文段如下：</code></pre></li><li>示例二：<pre class="line-numbers language-none"><code class="language-none">假设你是一个学术研究者，正在撰写毕业论文，具有学术研究相关知识。请简要总结以下文段的主要内容观点，并按条目输出，再根据每条总结扩写句子，最后组合成新文段，要求给出的文段与原文段重复率低于5%，要求输出的新文段中任意抽取连续40个字与原文段中任意连续40个字比较，重复的字小于5个。允许重新组织句子顺序、词语顺序、段落顺序，允许改写时对句子扩写或缩减。请给出改完后的文段、给出与原文段的对比，请一步一步阐述。文段如下：</code></pre></li></ul><h2 id="综述协助"><a href="#综述协助" class="headerlink" title="综述协助"></a>综述协助</h2><ul><li>全职版<pre class="line-numbers language-none"><code class="language-none">假设你是正在对某一学术领域的[RESEARCH TOPIC]进行深入研究的研究者。你的任务是从广泛的文献中识别、分析和综合关键的发现、理论和研究方法。首先，确定一个简单的研究问题或论点，以此为基础指导你的文献回顾。寻找相关的学术期刊、会议论文、书籍以及可靠的网络资源。对每项资源的可信度、相关性和对领域贡献进行评估。摘要每个资源的主要论点、证据和结论，注意观察文献中的一致性、差异或空缺。批判性地审视所研究的方法、其局限性及其发现的意义。根据主题或时间顺序安排文献综述，确保逻辑连贯，围绕研究主题构建出一篇有条理的综述文章。最后，强调你的研究是如何填补已识别的空白或对该学科领域提供新见解的。确保你的文献综述遵循所在机构要求的引用风格和学术写作标准。</code></pre></li><li>润色版<pre class="line-numbers language-none"><code class="language-none">假设你是正在对某一学术领域的[RESEARCH TOPIC]进行深入研究的研究者。你的任务是将我给你的相关段落进行完善与展开。你需要批判性地审视所研究的方法、其局限性及其发现的意义，确保逻辑连贯，保持术语、风格和语调的统一。对于可能需要对预期读者进行说明的行业术语或专业词汇，要给出清晰的解释。</code></pre></li></ul><h1 id="英文写作"><a href="#英文写作" class="headerlink" title="英文写作"></a>英文写作</h1><h2 id="中翻英"><a href="#中翻英" class="headerlink" title="中翻英"></a>中翻英</h2><pre class="line-numbers language-none"><code class="language-none">假设你是一名美式英语为母语的翻译员，同时你还是一名理学的学术研究者，你的任务是将以下中文学术论文段落翻译为英文。要求严格保持语法正确，要求英文词语拼写正确，要求单复数使用正确，要求采用学术的表达，不使用口语化表达，参考其他类似文献中的句子和段落结构，参考其他类似文献中的专有名词和习惯用词，可以根据句子内容扩展添加词汇使句子结构完整，可以根据句子内容调整句子的先后位置。最后提供最后翻译完的英文文段以及调整后的中文文段，同时展示调整后的中文文段与原文段对比在哪些位置做了修改。请一步一步阐述。请翻译以下文段：</code></pre><blockquote><p><a href="https://domyweb.org/chatgpt/#act-as-an-essay-writer">https://domyweb.org/chatgpt/#act-as-an-essay-writer</a></p></blockquote><blockquote><p><a href="https://zhuanlan.zhihu.com/p/654520254">https://zhuanlan.zhihu.com/p/654520254</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Prompt </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLMs </tag>
            
            <tag> Prompt </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>在宇宙最强编辑器上使用latex</title>
      <link href="/2024/12/18/zai-yu-zhou-zui-qiang-bian-ji-qi-shang-shi-yong-latex/"/>
      <url>/2024/12/18/zai-yu-zhou-zui-qiang-bian-ji-qi-shang-shi-yong-latex/</url>
      
        <content type="html"><![CDATA[<h2 id="安装准备"><a href="#安装准备" class="headerlink" title="安装准备"></a>安装准备</h2><ul><li>TeXLive / MacTeX / MiKTeX等LaTeX发行版</li><li>VSCode</li></ul><h2 id="Vscode-配置"><a href="#Vscode-配置" class="headerlink" title="Vscode 配置"></a>Vscode 配置</h2><h3 id="安装插件："><a href="#安装插件：" class="headerlink" title="安装插件："></a>安装插件：</h3><ul><li><p>LaTeX Workshop （必装）</p><p>  <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241218201018.png"><br>  LaTeX Workshop 支持 LaTeX 的编译、预览、语法检查等功能。</p></li><li><p>English Word Hint</p><p>  <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241218201116.png"><br>  English Word Hint 是一个英语单词提示插件，可以在编写英语文档时，自动提示相关英语单词，并显示对应的中文翻译，提高英文文档编写效率。</p></li><li><p>Path Auto Complete</p><p>  <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241218201156.png"><br>  Path Auto Complete 可以自动补全路径，方便快速插入图片。</p></li><li><p>indent rainbow</p><p>  <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241218201245.png"><br>  indent rainbow 可以为不同层级的缩进添加不同的颜色，方便阅读。</p></li><li><p>Word Count CJK</p><p>  Word Count CJK 可以统计中文文档的字数。</p></li><li><p>Material Icon Theme</p><p>  Material Icon Theme 可以为不同类型的文件添加不同的图标，方便区分。</p></li></ul><h3 id="LaTeX-Workshop-配置"><a href="#LaTeX-Workshop-配置" class="headerlink" title="LaTeX Workshop 配置"></a>LaTeX Workshop 配置</h3><h4 id="基本配置"><a href="#基本配置" class="headerlink" title="基本配置"></a>基本配置</h4><pre class="line-numbers language-json" data-language="json"><code class="language-json">// 鼠标悬停，预览公式时，支持 boldsymbol 宏"latex-workshop.hover.preview.mathjax.extensions": [    "boldsymbol"],// 是否启用 IntelliSense，自动补全引用的包中的环境和命令"latex-workshop.intellisense.package.enabled": true,// 编译后的文件输出目录"latex-workshop.latex.outDir": "./tmp",// 默认编译引擎为上次使用的"latex-workshop.latex.recipe.default": "lastUsed",// 预览复杂公式，使用时需要通过 command palette (命令面板) 打开"latex-workshop.mathpreviewpanel.cursor.enabled": true,// 不允许弹窗显示错误信息"latex-workshop.message.error.show": false,// 不允许弹窗显示警告信息"latex-workshop.message.warning.show": false,// 预览 PDF 时，反转颜色"latex-workshop.view.pdf.invert": 1,// 预览 PDF 时，自动检测是否需要反转颜色"latex-workshop.view.pdf.invertMode.enabled": "auto",</code></pre><h4 id="编译工具链配置"><a href="#编译工具链配置" class="headerlink" title="编译工具链配置"></a>编译工具链配置</h4><p>推荐使用 latexmk 进行编译，latexmk 可以自动检测文档中的变化，自动进行编译，并且同时支持多种编译引擎，包括 XeLaTeX、PdfLaTeX。</p><p>在 settings.json 文件中找到 latex-workshop.latex.tools 和 latex-workshop.latex.recipes 配置项，将其全部删除，并修改为如下配置</p><pre class="line-numbers language-json" data-language="json"><code class="language-json">"latex-workshop.latex.recipes": [    {        "name": "XeLaTeX",        "tools": [            "xelatexmk"        ]    },    {        "name": "PdfLaTeX",        "tools": [            "pdflatexmk"        ]    }],"latex-workshop.latex.tools": [    {        "args": [            "-synctex=1",            "-pdfxe",            "-interaction=nonstopmode",            "-file-line-error",            "-outdir=%OUTDIR%",            "%DOC%"        ],        "command": "latexmk",        "env": {},        "name": "xelatexmk"    },    {        "args": [            "-synctex=1",            "-pdf",            "-interaction=nonstopmode",            "-file-line-error",            "-outdir=%OUTDIR%",            "%DOC%"        ],        "command": "latexmk",        "env": {},        "name": "pdflatexmk"    }],</code></pre><h4 id="预览与同步TeX"><a href="#预览与同步TeX" class="headerlink" title="预览与同步TeX"></a>预览与同步TeX</h4><p><a href="https://github.com/James-Yu/LaTeX-Workshop/wiki/View#internal-pdf-viewer%EF%BC%8C">https://github.com/James-Yu/LaTeX-Workshop/wiki/View#internal-pdf-viewer，</a> 官网上有详细配置说明，这里简单介绍通过内置的pdf预览器的前向与反向搜索的快捷键：</p><ul><li><p>Forward/Direct search: 在编辑器中点击某个位置，window通过<code>ctrl+alt+j</code> 跳转，mac通过<code>cmd+option+j</code> 跳转。</p></li><li><p>Backward/Inverse search: 使用内部查看器时，指向 pdf 预览中的元素的默认键绑定是 <code>ctrl+click</code>。可以使用设置 <code>latex-workshop.view.pdf.internal.synctex.keybinding</code> 将其更改为<code>double-click</code> 。</p><pre class="line-numbers language-json" data-language="json"><code class="language-json">"latex-workshop.view.pdf.internal.synctex.keybinding": "double-click"</code></pre></li></ul><p>参考：</p><blockquote><p>在 VSCode 中配置 LaTeX 环境 <a href="https://github.com/shinyypig/latex-vscode-config">https://github.com/shinyypig/latex-vscode-config</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Latex </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Latex </tag>
            
            <tag> VSCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>日本-消失的三十年-启示与借鉴</title>
      <link href="/2024/12/10/ri-ben-xiao-shi-de-san-shi-nian-qi-shi-yu-jie-jian/"/>
      <url>/2024/12/10/ri-ben-xiao-shi-de-san-shi-nian-qi-shi-yu-jie-jian/</url>
      
        <content type="html"><![CDATA[<h2 id="二战后日本经济奇迹"><a href="#二战后日本经济奇迹" class="headerlink" title="二战后日本经济奇迹"></a>二战后日本经济奇迹</h2><p>二战后，世界迎来了冷战时期，日本作为美国抵御苏联影响的桥头堡，因此受到了美国极大的经济援助。</p><p>美国认为，经济发展不仅可以防止日本再次出现军国主义，并且可以防止共产主义。</p><p>1950年，由于朝鲜战争的爆发，日本成为美军军需生产和维修的基地，美国政府支付巨额的特殊采购，这些特殊采购占当年日本出口贸易的27%。1950年代后期，日本的经济完全恢复。</p><p>日本政府的一系列经济政策，加上没有养军队的束缚（《日本国宪法》第九条）也为日本经济的快速发展奠定了基础。</p><p>1956年，日本政府制定“电力五年计划”，建立电力工业和用石油取代煤炭，由此带来的良好影响带动了耐用品消费，出现战后第一次经济发展高潮，时称<strong>神武景气</strong>。</p><p>1958年后，日本政府开始引导企业生产如汽车、电视等家用电器和钢铁，出现了第二次经济发展高潮，称为<strong>岩户景气</strong>。</p><p>1960年，1960年代日本首相池田勇人提出了国民收入倍增计划，10年的时间让日本的国民生产总值增加到26兆日圆，但实际的经济成长远超过预期：仅六年的时间，国民平均收入就达到了倍增的计划。</p><p>1964年，日本成功举办东京奥运会，日本为了东京奥运的直接场馆投资为295亿日元，间接投资（公路、地下铁等交通建设，上下水道铺建等）则达9,600亿日元。房地产市场迅速发展，再一次拉动了经济，时称<strong>奥运会景气</strong>。</p><p>但奥运会结束后日本经济出现了放缓，于是日本政府决定发行战后的第一次建设国债刺激经济。在这期间，有不少大企业合并，同时私家车和彩色电视快速普及，国民的收入水平快速提高，时称<strong>伊奘诺景气</strong>。</p><h2 id="美国贸易逆差与广场协议"><a href="#美国贸易逆差与广场协议" class="headerlink" title="美国贸易逆差与广场协议"></a>美国贸易逆差与广场协议</h2><p>在美国扶持下，日本制造业迎来飞速发展，很多产品出口到美国，抢占其本土产品市场。</p><p>1、1957-1972年：日本 纺织品 大量进入美国，美国密集出台限制日本纺织品的法案；</p><p>2、1968-1978年：日本 钢铁行业 接棒纺织业，成为对美出口的主力，随后遭到美国钢铁行业工会的阻击；</p><p>3、1970-1980年：日本 家电行业 崛起，特别是彩电，巅峰期美国市场三成的彩电都是日本产品；</p><p>4、1979-1987年：日本 汽车行业 崛起，成为日本赚取美元的主力，导致全美范围内的抗议潮。</p><p>为阻止贸易逆差扩大，美国对日出台一系列限制政策，可惜效果一直不大，直到1985年的广场协议。</p><p>美国、日本、英国、法国及德国5个工业发达国家代表人在纽约的广场饭店会晤后，于1985年9月签署该协议，协议要求干预外汇市场，使美元对日元、德国马克等主要货币贬值。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241210195705.png"></p><p>签完协议头一年，日元仅相对美元升值20%，还在可控范围，日本制造企业省省钱，裁裁员，还撑得下去，到了1987年直接失控，日元相对美元升值到100%，从1美元兑240日元左右上升到一年后的1美元兑120日元，由于汇率的剧烈变动，由美国国债组成的资产发生账面亏损，因此大量资金为了躲避汇率风险而进入日本国内市场。但是日本出口企业受到了重创。</p><h2 id="泡沫经济与破灭"><a href="#泡沫经济与破灭" class="headerlink" title="泡沫经济与破灭"></a>泡沫经济与破灭</h2><h3 id="虚假的繁荣"><a href="#虚假的繁荣" class="headerlink" title="虚假的繁荣"></a>虚假的繁荣</h3><p>为了缓解日元升值的速度，日本政府采取的办法是 <strong>降息</strong>，日本政府认为只要存款利息减少，大家的钱存银行不划算，市场上流动的日元会大大增多，刺激国民消费，扩大内需，实现一种出口转内销的效果，同时缓解日元升值的速度。</p><p>于是1986年宣布将银行利率从5%降到4.5%，到1987年直接降到2.5%。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241210200859.png"></p><p>汇率是稳下来，从汇率表可以看到，日元升值到1987年就停下来。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241210201032.png"></p><p>但没想到的是，降低利率对国内基本消费需求没有刺激作用，但对投机、特别是投资房地产行为却能起很大的促进的作用，市场上多出来的日元没有进入制造业，而是<strong>投到股市和房市上</strong>，当时采取这样的政策，本身就是以虚假的经济泡沫去粉饰经济增长的数据，泡沫撑起日本GDP自1987年后的高速增长。日经平均股价从85年的13000，一路飙涨到90年的38000点左右，接近3倍。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241210201556.png"></p><p>房价从83年的37万日元一平米，涨到91年的278万日元一平米，如果按目前1: 0.05 的人民币日元汇率算，相当于 14万人民币一平米。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241210201624.png"></p><p>当时东京23个区的地价总和甚至达到了可以购买美国全部国土的水平，而银行则以不断升值的土地作为担保，向债务人大量放款。此外地价上升也使得土地所有者的账面财产增加，刺激了消费欲望，从而导致了国内消费需求增长。由于日元急速升值导致国外资产变得便宜，当时很多人卖奢侈品和买菜一样平常。当时日本媒体为了给这种经济繁荣状况命名，还希望募集像岩户景气、神武景气类似的名称。</p><h3 id="泡沫经济的破灭"><a href="#泡沫经济的破灭" class="headerlink" title="泡沫经济的破灭"></a>泡沫经济的破灭</h3><p>随着越来越多的资本流向股市和房市，而不是实体产业，加上由于美国发动了伊拉克战争，石油价格飙升，日本国内制造业成本激增，于是大量制造业倒闭。</p><p>1989年，日本央行开始加息，希望通过加息来抑制股市和房市的投机热情，但是这一举措却引起了外资的抛售，股市在1989年12月达到38957.44点后开始下跌，泡沫经济开始正式破裂。</p><p>1991年一月，日本政府发现事情不对，又开始大幅降息，想以此稳住股市，但依旧无法挽救股市，外资持续出逃。</p><p>比股市更惨的是楼市，1991年9月，土地开始暴跌，1992年日本出台“地价税”政策，要求所有持有土地的人交纳地价税，导致大量人开始抛售房产，陷入了供大于求的局面。随着房价暴跌，越来越多的人发现自己持有的房子的价值比自己的贷款还低，于是大量人违约，日本银行出现大量不良资产，银行开始倒闭，许多日本居民的存款一夜归零。</p><p>到这里，大家都没钱了，整体经济断崖式下跌，买了房的人，失业后还要还房贷，没买房的人，也失去了买房拼搏的动力。年轻人逐渐开始走向佛系，选择低欲望生活，他们不上班，在家啃老，沉迷二次元和网络游戏，被称为“平成废物”。</p><h2 id="启示与借鉴"><a href="#启示与借鉴" class="headerlink" title="启示与借鉴"></a>启示与借鉴</h2><h3 id="当前中国经济发展与日本战后经济发展的比较"><a href="#当前中国经济发展与日本战后经济发展的比较" class="headerlink" title="当前中国经济发展与日本战后经济发展的比较"></a>当前中国经济发展与日本战后经济发展的比较</h3><ol><li>   快速工业化</li></ol><ul><li>日本战后通过政府规划和政策支持实现了高速工业化，中国自改革开放以来也经历了类似的工业化进程，逐步从农业经济转向制造业和服务业为主。</li><li>两国都通过基础设施建设和出口导向政策，拉动经济增长。</li></ul><ol start="2"><li>   高储蓄率与高投资率</li></ol><ul><li>日本战后储蓄率高，资金主要流向工业和基础设施投资；中国近年来也保持着高储蓄率和投资率，大量资金用于基建、房地产和科技产业。</li><li>在高投资驱动下，经济增长迅速，但也存在投资过剩的风险。</li></ul><ol start="3"><li>   政府主导经济</li></ol><ul><li>日本战后经济受到政府强烈干预，实施产业政策支持关键领域，中国同样通过五年规划和政策导向调控经济发展。</li><li>政府对市场的干预在某种程度上加剧了市场的失衡，带来潜在风险。</li></ul><ol start="4"><li>   快速城市化和房地产市场繁荣</li></ol><ul><li>日本战后城市化进程迅速，房地产市场经历了高速增长；中国也在近年来快速城市化，房地产行业成为经济增长的重要引擎。</li><li>房地产泡沫在日本曾造成严重的经济问题，中国当前也面临类似的泡沫风险。</li></ul><ol start="5"><li>   债务问题</li></ol><ul><li>日本战后经济高速增长时期，企业债务迅速膨胀，政府债务也不断攀升。中国目前地方政府债务和企业债务同样增长迅速，潜在风险值得关注。</li></ul><h3 id="泡沫经济的潜在危机：中国是否会重蹈日本覆辙？"><a href="#泡沫经济的潜在危机：中国是否会重蹈日本覆辙？" class="headerlink" title="泡沫经济的潜在危机：中国是否会重蹈日本覆辙？"></a>泡沫经济的潜在危机：中国是否会重蹈日本覆辙？</h3><ol><li>   房地产市场风险</li></ol><ul><li>日本经验：1980年代日本房地产价格暴涨，但随着政府收紧货币政策，泡沫破裂，导致金融系统崩溃。</li><li>中国现状：房地产行业在中国经济中占据重要地位，但存在过度依赖的问题。目前房地产市场面临高杠杆和需求疲软的双重压力，部分城市已经显现泡沫破裂迹象。</li></ul><ol start="2"><li>   债务积累</li></ol><ul><li>日本经验：泡沫时期企业过度借贷，最终因资产价格下跌和经济衰退导致大规模债务违约。</li><li>中国现状：地方政府隐性债务和企业债务风险持续上升，部分债务已经难以持续偿还，一旦经济下行可能触发债务危机。</li></ul><ol start="3"><li>   金融系统稳定性</li></ol><ul><li>日本经验：泡沫破裂后，银行因不良贷款增加而陷入困境，金融系统崩溃成为长期经济低迷的主要原因。</li><li>中国现状：中国金融系统中存在不良贷款率上升的问题，特别是在房地产和地方政府融资平台领域，可能对银行体系构成威胁。</li></ul><ol start="4"><li>   人口与消费问题</li></ol><ul><li>日本经验：泡沫经济破裂后，人口老龄化和消费疲软导致经济难以复苏。</li><li>中国现状：人口增速放缓、老龄化问题显现，内需不足可能进一步制约经济增长，形成长期性结构问题。</li></ul><p>历史证明，<strong>当经济以超出潜在经济增长率的速度奔跑时，发生资产泡沫乃至成为泡沫经济几乎是不可避免的事情</strong>。</p><h3 id="中国会不会重蹈复辙？"><a href="#中国会不会重蹈复辙？" class="headerlink" title="中国会不会重蹈复辙？"></a>中国会不会重蹈复辙？</h3><p>中国经济发展与日本战后有诸多相似之处，但中国的经济规模更大、政策弹性更强，并且具备应对危机的经验。然而，中国当前面临的房地产泡沫、债务问题和人口老龄化等挑战，确实可能引发类似于日本的经济泡沫危机。如果不能妥善处理，这些问题可能成为经济发展的重大障碍。因此，继续推进经济结构改革和风险防控是避免陷入泡沫经济困境的关键。</p><h3 id="逆势上涨的行业机会"><a href="#逆势上涨的行业机会" class="headerlink" title="逆势上涨的行业机会"></a>逆势上涨的行业机会</h3><p>经济越不好，什么行业反而越好呢？其实日本“消失的三十年”早就写好了答案。当时的年轻人也是想着去考公考编，去银行工作，结果日本的公务员年年缩编，153家银行直接倒闭了。消费降级，年轻人躺平，跟我们当下如出一辙。那当时的日本什么行业反而好呢？主要有三个方向。一、多巴胺经济，就是那种花点小钱能买到大快乐的。三得利当时就是靠着卖茶和咖啡一波崛起的。二、平价经济。日本连续七年的首富就是优衣库。那咱们的蜜雪冰城、拼多多、名创优品是不是也很类似呢？最后就是造梦经济。通俗点说，就是能让人短暂地忘记痛苦，逃避现实的行业。比如20年前日本的索尼和任天堂，以及国内爆火的《黑神话：悟空》，都是因为消费降级了，大家不爱出门，宁愿花点小钱在家玩游戏，主打的就是忘记痛苦，过得快乐。 </p>]]></content>
      
      
      <categories>
          
          <category> 杂谈 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 经济发展 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型优化-FlashAttention-v2/v3</title>
      <link href="/2024/12/10/da-mo-xing-you-hua-flashattention-v2-v3/"/>
      <url>/2024/12/10/da-mo-xing-you-hua-flashattention-v2-v3/</url>
      
        <content type="html"><![CDATA[<h2 id="FlashAttention-v1回顾"><a href="#FlashAttention-v1回顾" class="headerlink" title="FlashAttention-v1回顾"></a>FlashAttention-v1回顾</h2><p>我们先快速回顾一下V1的运作流程：以K，V为外循环，Q为内循环。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241210021847.png"></p><h2 id="FlashAttention-v2"><a href="#FlashAttention-v2" class="headerlink" title="FlashAttention-v2"></a>FlashAttention-v2</h2><p>FlashAttention V2 出自论文(《FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning》)[<a href="https://arxiv.org/pdf/2307.08691]%EF%BC%8C">https://arxiv.org/pdf/2307.08691]，</a><br>主要改进包括：</p><ul><li>优化计算次序，减少非矩阵计算量。</li><li>增加 seq_len 维度的并行计算，提升 SM 利用率。</li><li>优化 warp 级工作模式，减少内部通信和 shared memory 访问。</li></ul><h2 id="FlashAttention-v3"><a href="#FlashAttention-v3" class="headerlink" title="FlashAttention-v3"></a>FlashAttention-v3</h2><p>Flash Attention V3 出自论文(《FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision》)[<a href="https://arxiv.org/pdf/2407.08608]%EF%BC%8C%E4%B8%BB%E8%A6%81%E6%94%B9%E8%BF%9B%E5%A6%82%E4%B8%8B%EF%BC%9A">https://arxiv.org/pdf/2407.08608]，主要改进如下：</a></p><p>引入生产者-消费者异步机制，提升并行度。<br>优化 GEMM 和 Softmax 操作的重叠计算。<br>支持 FP8 低精度硬件加速，提升吞吐量并减少精度损失。</p><p>参考：</p><blockquote><p>图解大模型计算加速系列：Flash Attention V2，从原理到并行计算: <a href="https://zhuanlan.zhihu.com/p/691067658">https://zhuanlan.zhihu.com/p/691067658</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLMs </tag>
            
            <tag> FlashAttention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo正确渲染md/latex公式</title>
      <link href="/2024/12/07/hexo-zheng-que-xuan-ran-md-latex-gong-shi/"/>
      <url>/2024/12/07/hexo-zheng-que-xuan-ran-md-latex-gong-shi/</url>
      
        <content type="html"><![CDATA[<p>我们平时使用markdown写文档的时候，免不了会碰到数学公式，好在有强大的Mathjax，可以解析网页上的数学公式，大部分情况下都是可以的，但是Markdwon本身的特殊符号与Latex中的符号会出现冲突的时候:</p><ul><li><code>_</code>的转义，在markdown中，<code>_斜体_</code>是 _斜体_，但是在latex中，却有下标的意思，就会出现问题。</li><li><code>\\</code>的换行，在markdown中，<code>\\</code>会被转义为<code>\</code>这样也会影响影响mathjax对公式中的<code>\\</code>进行渲染</li></ul><p>我从网上找到的解决办法大多都是使用pandoc提换markdown的渲染引擎：</p><h2 id="hexo-render-pandoc"><a href="#hexo-render-pandoc" class="headerlink" title="hexo-render-pandoc"></a>hexo-render-pandoc</h2><p>使用 hexo-render-pandoc, 可以避免上述提到的很多歧义问题步骤如下：</p><ol><li>安装配置hexo-render-pandoc</li></ol><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">npm un hexo-renderer-marked --save#or npm un hexo-renderer-kramed --save (如果安装的是kramed的话)# 在安装之前，建议先删除node_modules文件夹下所有文件# 并运行命令 npm installnpm i hexo-renderer-pandoc --save</code></pre><p>前去官网安装pandoc，并保证能在命令行中运行 pandoc -v.</p><ol start="2"><li>主题配置打开mathjax开关</li></ol><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"># MathJax Supportmathjax:  enable: true</code></pre><ol start="3"><li>文章的Front-matter里打开mathjax开关(一定要打开，不然不会渲染)</li></ol><hr><h2 id="title-index-htmldate-2016-12-28-21-01-30tags-mathjax-true"><a href="#title-index-htmldate-2016-12-28-21-01-30tags-mathjax-true" class="headerlink" title="title: index.htmldate: 2016-12-28 21:01:30tags:mathjax: true"></a>title: index.html<br>date: 2016-12-28 21:01:30<br>tags:<br>mathjax: true</h2><p>但是pandoc比较笨重，需要首先安装Pandoc, 不过的确可以完美解决上述的不兼容问题，另外它的语法与markdown有些微的差异，需要注意。</p><h2 id="分离markdown和latex"><a href="#分离markdown和latex" class="headerlink" title="分离markdown和latex"></a>分离markdown和latex</h2><h3 id="使用或者标签"><a href="#使用或者标签" class="headerlink" title="使用<div>或者<span>标签"></a>使用<code>&lt;div&gt;</code>或者<code>&lt;span&gt;</code>标签</h3><p>针对有可能被转义的公式（如带换行符的），用<code>&lt;span&gt;</code>或者<code>&lt;div&gt;</code>标签将LaTeX公式包裹起来，这样公式内容就不会被markdown渲染器识别为转义字符。</p><p>比如：</p><pre class="line-numbers language-latex" data-language="latex"><code class="language-latex">$$\begin{pmatrix}    a_{11} &amp; a_{12} &amp; a_{13} \\    a_{21} &amp; a_{22} &amp; a_{23} \\    a_{31} &amp; a_{32} &amp; a_{33}  \end{pmatrix} $$</code></pre><p><code>\\</code>会无法识别导致不能换行，可以写成：</p><pre class="line-numbers language-latex" data-language="latex"><code class="language-latex">&lt;span&gt;$$\begin{pmatrix}    a_{11} &amp; a_{12} &amp; a_{13} \\    a_{21} &amp; a_{22} &amp; a_{23} \\    a_{31} &amp; a_{32} &amp; a_{33}  \end{pmatrix}$$&lt;/span&gt;</code></pre><p>渲染结果就正常了。</p><h3 id="使用hexo自带的分离标记（推荐，简单）"><a href="#使用hexo自带的分离标记（推荐，简单）" class="headerlink" title="使用hexo自带的分离标记（推荐，简单）"></a>使用hexo自带的分离标记（推荐，简单）</h3><p>在官方文档中提到了可以为hexo提供标记，阻止其按照自己的规则解释我们的字符串，显示其原本的含义</p><pre class="line-numbers language-latex" data-language="latex"><code class="language-latex">{% raw %}...{% endraw %}</code></pre><p>这样就可以避免markdown解释器对我们的字符串进行转义，而是直接输出原本的字符串。</p><p>比如：</p><pre class="line-numbers language-latex" data-language="latex"><code class="language-latex">{% raw %}$$\begin{align*}a &amp;= b + c \\    &amp;= d + e \\    &amp;= f + g\end{align*}$${% endraw %}</code></pre><p>我测试使用<code>&lt;div&gt;</code>或者<code>&lt;span&gt;</code>在多行公式渲染时依旧会出现异常，而使用<code></code>则可以正常渲染。</p>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> hexo公式渲染 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型优化-FlashAttention-v1</title>
      <link href="/2024/12/07/da-mo-xing-you-hua-flashattention-v1/"/>
      <url>/2024/12/07/da-mo-xing-you-hua-flashattention-v1/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在传统的自注意力机制中，注意力矩阵的<strong>计算复杂度为 O(N²)<strong>，其中 N 是序列的长度。对于长序列的输入（如文本或图像中的像素点），这种计算代价极高，特别是在训练大型语言模型或视觉模型时，内存占用和计算开销随着序列长度的增加而急剧上升。此外，</strong>注意力矩阵的大小为 N×N</strong>，这也对 GPU 内存消耗极大。自注意力机制不仅在计算时消耗大量内存，还需要存储所有中间变量（如 Q、K、V 矩阵及注意力权重），以支持后续的反向传播。</p><p>因此，找到有效降低 Transformer 模型 O(N²) 复杂度的方案至关重要。理想情况下，若能将复杂度降至 O(N)，将大大提升模型效率。即使无法完全实现 O(N)，逼近这一复杂度也是十分有价值的。在这一背景下，Flash Attention 应运而生，成为解决该问题的有效方案。</p><p>从 Flash Attention（Fast and Memory Efficient Exact Attention with IO-Awareness）的命名可见其优势：</p><ul><li>Fast：在Flash Attention之前，也出现过一些加速Transformer计算的方法，这些方法的着眼点是“减少计算量FLOPs”，例如用一个稀疏attention做近似计算。而FlashAttention发现：<strong>计算慢的卡点不在运算能力，而是在读写速度上。</strong> 所以它通过降低对显存（HBM）的访问次数来加快整体运算速度，这种方法又被称为<strong>O-Awareness</strong>。</li><li>Memory Efficient：在 Flash Attention 中，内存使用压力从 O(N²) 降至 O(N)，显著节省内存。</li><li>Exact Attention：与稀疏 Attention 不同，Flash Attention 完全等效于标准 Attention。</li></ul><h2 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h2><h3 id="计算限制与内存限制"><a href="#计算限制与内存限制" class="headerlink" title="计算限制与内存限制"></a>计算限制与内存限制</h3><h4 id="受限原因分析"><a href="#受限原因分析" class="headerlink" title="受限原因分析"></a>受限原因分析</h4><p>首先介绍几个关键概念：</p><ul><li>$\pi$：<strong>硬件算力上限</strong>，表示一个计算平台在全负荷情况下每秒能够执行的浮点运算次数，单位为 FLOPS（浮点运算次数每秒）。</li><li>$\beta$：<strong>硬件带宽上限</strong>，表示一个计算平台在全负荷情况下每秒能够完成的数据交换量，单位为 Byte/s。</li><li>$\pi_t$：<strong>某算法所需的总运算量</strong>，单位为 FLOPs。（t 表示 total）</li><li>$\beta_t$：<strong>某算法所需的总数据读取和存储量</strong>，单位为 Byte。</li></ul><blockquote><p>这里强调一下对FLOPS和FLOPs的解释：</p></blockquote><p>FLOPS：等同于FLOP/s，表示Floating Point Operations Per Second，即每秒执行的浮点数操作次数，用于衡量硬件计算性能。</p><p>FLOPs：表示Floating Point Operations，表示某个算法的总计算量（即总浮点运算次数），用于衡量一个算法的复杂度。</p><p>在实际执行过程中，时间不仅消耗在计算上，也消耗在数据读取和存储上。因此，我们定义：</p><ul><li>$T_{cal}$：算法执行所需的计算时间，其公式为$\pi_t / \pi$。</li><li>$T_{load}$：算法执行所需的数据读取与存储时间，公式为 $\beta_t / \beta$。</li></ul><p>由于计算和数据传输可以同时进行，我们定义算法的总执行时间：$T = max(T_{cal}, T_{load})$</p><ul><li>当 $T_{cal}&gt;T_{load}$ 时，算法的瓶颈在计算部分，称为<strong>计算限制（math-bound）。</strong>此时，$\pi_t / \pi &gt; \beta_t / \beta$，即 $\pi_t/\beta_t &gt; \pi/\beta$。</li><li>当 $T_{cal}&lt;T_{load}$ 时，瓶颈在数据读取部分，称为<strong>内存限制</strong>（memory-bound）。此时，$\pi_t / \pi &lt; \beta_t / \beta$，即 $\pi_t/\beta_t &lt; \pi/\beta$。</li></ul><p>算法的<strong>计算强度</strong>（Operational Intensity）定义为 $\pi_t/\beta_t$，表示每个数据读取操作对应的计算量。当算法的计算强度越高，说明计算部分的工作量越大，反之则说明数据读取部分的工作量越大。</p><p>假设我们现在采用的硬件为A100-40GB SXM，同时采用混合精度训练（可理解为训练过程中的计算和存储都是fp16形式的，一个元素占用2byte），则：<br>$$ \pi/\beta = 312<em>10^{12} / 1555</em>10^9 = 201 FLOPs/Bytes $$</p><p>对于一个模型，$Q, K \in \mathbb{R}^{n \times d}$，其中N为序列长度，d为embedding dim。现在计算$S = QK^T$，则有：<br>$$<br>\frac{\pi_t}{\beta_t} = \frac{2N^2d}{2Nd + 2Nd + 2N^2} = \frac{N^2d}{2Nd + N^2}<br>$$</p><p>下表记录了不同的N，d下的受限类型：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241207024735.png"></p><p>根据这个表格，我们可以来做下总结：</p><ul><li>计算限制（math-bound）：大矩阵乘法（N和d都非常大）、通道数很大的卷积运算。相对而言，读得快，算得慢。</li><li>内存限制（memory-bound）：逐点运算操作。例如：激活函数、dropout、mask、softmax、BN和LN。相对而言，算得快，读得慢。</li></ul><p>所以，“Transformer计算受限于数据读取”也不是绝对的，要综合硬件本身和模型大小来综合判断。但从表中的结果我们可知，memory-bound的情况还是普遍存在的，所以Flash attention的改进思想在很多场景下依然适用。</p><p>在Flash attention中，<strong>计算注意力矩阵时的softmax计算就受到了内存限制，这也是flash attention的重点优化对象</strong>，我们会在下文来详细看这一点。</p><h4 id="roof-line-模型"><a href="#roof-line-模型" class="headerlink" title="roof-line 模型"></a>roof-line 模型</h4><p>一个算法运行的效率是离不开硬件本身的。我们往往想知道：对于一个运算量为 $\pi_t$，数据读取存储量为 $\beta_t$ 的算法，它在算力上限为 $\pi$，带宽上限为 $\beta$ 的硬件上，能达到的最大性能 $P$（Attanable Performance）是多少？</p><p>这里最大性能 $P$ 指的是当前算法实际运行在硬件上时，每秒最多能达到的计算次数，单位是FLOP/s。</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241207025413.png"></p><p>从图中可以直观的看出，当计算强度达到了硬件的上限时，算法的性能达到最大值。而在之前的计算强度范围内，都属于内存限制。</p><h3 id="GPU存储与计算"><a href="#GPU存储与计算" class="headerlink" title="GPU存储与计算"></a>GPU存储与计算</h3><h4 id="GPU存储分类"><a href="#GPU存储分类" class="headerlink" title="GPU存储分类"></a>GPU存储分类</h4><p>通常，GPU 存储分为片上内存（on-chip memory）和片下内存（off-chip memory），这主要取决于存储单元是否位于芯片内部。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241207030019.png"></p><ul><li><strong>片上内存</strong>：用于缓存等，容量小但带宽极高。如上图中的 <strong>SRAM</strong>，容量仅 20MB，带宽却达 19TB/s。</li><li><strong>片下内存</strong>：用于全局存储（即显存），容量大但带宽相对较小。如 HBM，容量可达 40GB，带宽为 1.5TB/s。</li></ul><h4 id="GPU的计算"><a href="#GPU的计算" class="headerlink" title="GPU的计算"></a>GPU的计算</h4><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md/20240826055530.png"><br>如图，负责GPU计算的一个核心组件叫SM（Streaming Multiprocessors，流式多处理器），可以将其理解成GPU的计算单元，一个SM又可以由若干个SMP（SM Partition）组成，例如图中就由4个SMP组成。SM就好比CPU中的一个核，但不同的是一个CPU核一般运行一个线程，但是一个SM却可以运行多个轻量级线程（由Warp Scheduler控制，一个Warp Scheduler会抓一束线程（32个）放入cuda core（图中绿色小块）中进行计算）。</p><p>现在，我们将GPU的计算核心SM及不同层级GPU存储结构综合起来，绘制一张简化图：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241207030737.png"></p><ul><li>HBM2：即是我们的显存。</li><li>L1缓存/shared memory：每个SM都有自己的L1缓存，用于存储SM内的数据，被SM内所有的cuda cores共享。SM间不能互相访问彼此的L1。NV Volta架构后，L1和shared memory合并（Volta架构前只有Kepler做过合并），目的是为了进一步降低延迟。合并过后，用户能写代码直接控制的依然是shared memory，同时可控制从L1中分配多少存储给shared memory。<strong>Flash attention中SRAM指的就是L1 cache/shared memory</strong>。</li><li>L2缓存：所有SM共享L2缓存。L2缓存不直接由用户代码控制。L1/L2缓存的带宽都要比显存的带宽要大，也就是读写速度更快，但是它们的存储量更小。</li></ul><p>GPU 的计算流程可以理解为：数据从显存（HBM）加载到片上内存（SRAM），由 SM（Streaming Multiprocessor）读取并进行计算，计算结果再通过 SRAM 返回显存。具体可参考：<a href="https://www.yidoo.xyz/nvidia-gpu-principles">NVIDIA GPU 原理详解</a>。</p><p>显存带宽远低于 SRAM，因此从显存读取数据往往较耗时。为了优化读取效率，我们会<strong>尽量将数据填满 SRAM，从而减少频繁读取</strong>。</p><h4 id="kernel-fusion"><a href="#kernel-fusion" class="headerlink" title="kernel fusion"></a>kernel fusion</h4><p>为减少显存读取次数，若 SRAM 容量允许，多个计算步骤可合并在一次数据加载中完成。这被称为kernel 融合。</p><p>举例来说，我现在要做计算A和计算B。在老方法里，我做完A后得到一个中间结果，写回显存，然后再从显存中把这个结果加载到SRAM，做计算B。但是现在我发现SRAM完全有能力存下我的中间结果，那我就可以把A和B放在一起做了，这样就能节省很多读取时间，我们管这样的操作叫kernel融合。</p><p>对于kernel可以粗犷地理解成是“函数”，它包含对线程结构（grid-block-thread）的定义，以及结构中具体计算逻辑的定义。理解到这一层已不妨碍我们对flash attention的解读了，想要更近一步了解的朋友，推荐阅读这篇<a href="https://zhuanlan.zhihu.com/p/34587739">小小将：CUDA编程入门极简教程</a>。</p><h3 id="标准Attention计算"><a href="#标准Attention计算" class="headerlink" title="标准Attention计算"></a>标准Attention计算</h3><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241208192423.png"><br>其中， $S=QK^T, P=softmax(S)$。在GPT类的模型中，还需要对$P$做mask处理。为了表达方便，诸如mask、dropout之类的操作，我们都忽略掉，下文也是同理。</p><h3 id="标准safe-softmax"><a href="#标准safe-softmax" class="headerlink" title="标准safe softmax"></a>标准safe softmax</h3><p>在$softmax(x_i) = \frac{e^{x_i}}{\sum_{j=1}^d e^{x_j}}$的计算中，如果$x_i$的值很大，那么$e^{x_i}$会变得非常大，这样就会导致数值溢出。为了解决这个问题，我们可以对$x_i$做一个平移，即$x_i - max(x)$，这样就能保证$e^{x_i}$不会溢出。</p><p>下图展示了safe softmax的过程，这里 $\tilde{P}, P$ 分别表示平移前后的softmax结果。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241208194350.png"></p><h2 id="Flash-Attention"><a href="#Flash-Attention" class="headerlink" title="Flash Attention"></a>Flash Attention</h2><h3 id="Flash-Attention的核心思想"><a href="#Flash-Attention的核心思想" class="headerlink" title="Flash Attention的核心思想"></a>Flash Attention的核心思想</h3><ul><li><strong>分块计算</strong>：将输入矩阵划分为小块，并逐块在 SRAM 上计算注意力，避免将整个 N×N 矩阵存储于显存。</li><li><strong>重计算</strong>：通过前向传播时保存归一化因子，避免在反向传播中存储中间结果，而是通过重计算得出注意力矩阵。这虽然增加了浮点运算次数，但通过减少 HBM 访问，提升了整体效率。</li></ul><h3 id="前向计算"><a href="#前向计算" class="headerlink" title="前向计算"></a>前向计算</h3><h4 id="分块计算tiling"><a href="#分块计算tiling" class="headerlink" title="分块计算tiling"></a>分块计算tiling</h4><p>我们先来了解分块计算的整体流程（帮助大家理解数据块是怎么流转的），然后我们再针对其中的细节做一一讲解。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241208195502.png"></p><p>在计算这些分块时，GPU是可以做并行计算的，这也提升了计算效率。</p><p>好！现在你已经知道了单块的计算方式，现在让我们把整个流程流转起来把。在上图中，我们注明了 j 是外循环， i 是内循环，在论文里，又称为K，V是外循环，Q是内循环。写成代码就是:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"># ---------------------# Tc: K和V的分块数# Tr: Q的分块数量# ---------------------for 1 &lt;= j &lt;= Tc:    for 1 &lt;= i &lt;= Tr:        do....</code></pre><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241208201454.png"><br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241208201509.png"></p><p>这里的 $O$ 还需要经过一定的处理，才能和不分块场景下的 $O$完全等价。这里我们将每一块的 $O$ 单独画出，是为了帮助大家更好理解分块计算的整体流程，不代表它是最终的输出结果。</p><h4 id="tiliing中的safe-softmax"><a href="#tiliing中的safe-softmax" class="headerlink" title="tiliing中的safe softmax"></a>tiliing中的safe softmax</h4><p>回顾之前绘制的标准safe softmax流程图，我们知道 m、l都是针对完整的一行做rowmax、rowsum后的结果，那么在分块场景下，会变成什么样呢？<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241208201852.png"><br>以上图红圈内的数据为例，在标准场景下，我们是对红圈内的每一行做rowmax、rowsum后得到 $\tilde{P}$的。而分块后这部分数据会被分到不同的块中。</p><p>所以Flash Attention中采用如下方式实现safe softmax：</p><p>(1) 我们假设标准场景下，$S$ 矩阵某一行的向量为 $x = [x_1, x_2, \dots, x_d]$，因为分块的原因，<br>它被我们切成了两部分 $x = \begin{bmatrix} x^{(1)}, x^{(2)} \end{bmatrix}$。</p><p>(2) 我们定义：</p><ul><li>$m(x)$：标准场景下，该行的全局最大值</li><li>$m(x^{(1)})$：分块1的全局最大值</li><li>$m(x^{(2)})$：分块2的全局最大值</li></ul><p>那么易知：<br>$m(x) = m\left( \begin{bmatrix} x^{(1)}, x^{(2)} \end{bmatrix} \right) = \max \left( m(x^{(1)}), m(x^{(2)}) \right)$</p><p>(3) 我们定义：</p><ul><li>$f(x)$：标准场景下，$\exp(x - m(x))$ 的结果</li><li>$f(x^{(1)})$：分块场景下，$\exp(x^{(1)} - m(x^{(1)}))$ 的结果</li><li>$f(x^{(2)})$：分块场景下，$\exp(x^{(2)} - m(x^{(2)}))$ 的结果</li></ul><p>那么易知：$f(x) = \left[ e^{m(x^{(1)}) - m(x)} f(x^{(1)}), e^{m(x^{(2)}) - m(x)} f(x^{(2)}) \right]$</p><p>这个很好理解，详细的证明过程就不写了。</p><p>(4) 我们定义：</p><ul><li>$l(x)$：标准场景下，$\text{rowsum}[f(x)]$ 的结果</li><li>$l(x^{(1)})$：分块场景下，$\text{rowsum}[f(x^{(1)})]$ 的结果</li><li>$l(x^{(2)})$：分块场景下，$\text{rowsum}[f(x^{(2)})]$ 的结果</li></ul><p>那么由（3）易知：$l(x) = e^{m(x^{(1)}) - m(x)} l(x^{(1)}) + e^{m(x^{(2)}) - m(x)} l(x^{(2)})$</p><p>(5) 现在，我们就可以用分块计算的结果，来表示标准场景下 safe softmax 的结果了：<br>$$softmax(x) = \frac{f(x)}{l(x)} = \frac{\left[e^{m(x^{(1)}) - m(x)} f(x^{(1)}), e^{m(x^{(2)}) - m(x)} f(x^{(2)})\right]}{e^{m(x^{(1)}) - m(x)} l(x^{(1)}) + e^{m(x^{(2)}) - m(x)} l(x^{(2)})}$$</p><p>我们配合上面的图例和flash attention论文中的伪代码，再来进一步理解一下分块计算safe softmax的（1）～（5）步骤。</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241209021146.png"></p><p>我们用 $S_{00}$（图中浅绿色方块）替换掉（1）$\sim$（5）步骤中的 $x^{(1)}$，用 $S_{01}$（图中深绿色方块）替换掉 $x^{(2)}$。我们关注点在伪代码部分的 6 $\sim$ 11 行。</p><p>由于伪代码中的表达符号较多，容易阻碍大家的理解，因此我们先明确各个数学符号表达的含义：</p><ul><li>$S_{ij}$：对应在我们的例子里，就是 $S_{00}$ 和 $S_{01}$，即 $Q_i K_j^\top$ 的结果。</li><li>$m_{ij}$：对于当前分块 $S_{ij}$ 来说，每行的局部最大值。相当于前面步骤（2）中对 $m(x^{(1)})$, $m(x^{(2)})$ 的定义。</li><li>$\tilde{P}_{ij}$：分块场景下，各块的 $p$ 矩阵（归一化前）结果。相当于步骤（3）中对 $f(x^{(1)})$, $f(x^{(2)})$ 的定义。</li><li>$l_{ij}$：分块场景下，$\text{rowsum}$ 的结果。相当于步骤（4）中对 $l(x^{(1)})$, $l(x^{(2)})$ 的定义。</li><li>$m$：标准场景下，对 $S$ 矩阵而言，每行的最大值，这里是全局最大值（$m$ 首次定义在伪代码第 2 行），相当于前面步骤（2）中对 $m(x)$ 的定义。</li><li>$l$：标准场景下，全局 $\text{rowsum}$ 的结果（$l$ 首次定义在伪代码第 2 行），相当于前面步骤（4）中对 $l(x)$ 的定义。</li><li>$m_i$：表示 $\max(m_{i0}, m_{i1}, \dots, m_{i(j-1)})$。如果当前分块是 $S_{ij}$，则 $m_i$ 表示固定 $i$ 时，前 $j - 1$ 个分块中的局部最大值。容易推出，当固定 $i$，遍历完成 $S_{00}, S_{01}$ 后，$m_i$ 的结果就是全局最大值 $m_0$。</li><li>$m_i^{\text{new}}$：表示 $\max(m_{i0}, m_{i1}, \dots, m_{i(j-1)}, m_{ij})$。如果当前分块为 $S_{ij}$，则 $m_i^{\text{new}}$ 表示固定 $i$ 时，截止到当前分块为止的局部最大值。</li><li>$l_i$：和 $m_i^{\text{new}}$ 对应，相当于步骤（4）中用分块更新 $l(x)$ 的步骤。</li><li>$l_i$：和 $m_i$ 同理，即当我们将 $j$ 遍历完后，我们就能得到针对 $i$ 的全局 rowmax 和全局 rowsum。</li></ul><p>而根据前面的定义，$m_i^{\text{new}}$ 和 $l_i^{\text{new}}$ 是遍历完成最新的 $S_{ij}$ 后得到的 rowmax 和 rowsum 结果，<br>所以每遍历完一块 $S_{ij}$，我们就执行伪代码的第 13 行，做一次更新。</p><p>从伪代码 5-13 行中，你会发现，在整个计算过程中，只有 $m_i, l_i, O_i$ 被从 on-chip 的 SRAM 中写回到显存（HBM）中。<br>把 $i$ 都遍历完成后，读写量也不过是 $m, l, O$。相比于标准场景下，我们要读写的是 $S, P, O$，读写量是不是一下就少很多，这不就能解决 memory-bound 的问题了吗。</p><p>所以，<strong>分块计算 safe softmax 的意义，就是抹去对 $S, P$ 的读写</strong>。</p><h4 id="分块计算中的-O"><a href="#分块计算中的-O" class="headerlink" title="分块计算中的$O$"></a>分块计算中的$O$</h4><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241209024821.png"><br>上图中画的6个$O$并不是我们最终想要的结果。我们期望维护并更新 $O_i$，当该 $i$下的所有 $j$ 遍历完毕后，我们的 $O_i$就应该和标准场景下的 $O_i$完全相等。</p><p>在图中，$O_i$应该是红圈部分的乘积，但是我们只存了$m_i, l_i, O$，但是没有存$S, P$，所以到了最后一块我们也无法计算出$O_i$。</p><p>所以这里我们换个思路： $O_i$不是每遍历一块就更新一次吗？那有没有一种办法，<strong>不断用当前最新的rowmax和rowsum去更新 $O_i$，直到遍历完最后一块，这时的 $O_i$不就和标准场景下的结果完全一致了吗？也就是我们想构造形如下面这样的更新等式：</strong><br>$$<br>O_i = O_i + 当前最新结果<br>$$</p><p>因此我们有了伪代码中12行的推导：</p>$$\begin{aligned}O_i^{(j+1)} &amp;= P_{i, j+1} V_{:j+1} \\            &amp;= \text{softmax}(S_{i, :j+1}) V_{:j+1} \\            &amp;= \text{diag}(l^{(j+1)})^{-1} \left[ \exp\left([S_{i, :j}, S_{i(j+1)}] - m^{(j+1)}\right) \right]             \begin{bmatrix}                 V_{:j} \\                 V_{j+1}            \end{bmatrix} \\            &amp;= \text{diag}(l^{(j+1)})^{-1} \left[\exp(S_{i, :j} - m^{(j+1)})V_{:j} + \exp(S_{i(j+1)} - m^{(j+1)})V_{j+1}\right] \\            &amp;= \text{diag}(l^{(j+1)})^{-1} \left[\exp(S_{i, :j} - m^{(j)})V_{:j} + e^{-m^{(j+1)}}\exp(S_{i(j+1)})V_{j+1}\right] \\             &amp;= \text{diag}(l^{(j+1)})^{-1} \left[\text{diag}(l^{(j)})e^{m^{(j)} - m^{(j+1)}}O_i^{(j)} + e^{-m^{(j+1)}}\exp(S_{i(j+1)})V_{j+1}\right] \\             &amp;= \text{diag}(l^{(j+1)})^{-1} \left[\text{diag}(l^{(j)})e^{m^{(j)} - m^{(j+1)}}O_i^{(j)} + e^{m^{(j)} - m^{(j+1)}} \tilde{P}_{i(j+1)} V_{j+1}\right]\end{aligned}$$<p>推导过程中的符号上下标的含义：</p><ul><li>$i$：这个大家应该很熟悉了。例如图例中，$i=0, 1, 2$ 分别对应着深浅绿、深浅蓝、深浅黄块。</li><li>$(j+1)$：表示当前分块的相关结果。</li><li>$i, :j+1$：表示截止到当前分块（包含当前分块）的相关结果。$i, :j$ 表示截止到前一分块（包含前一分块）的相关结果。</li></ul><h3 id="后向计算"><a href="#后向计算" class="headerlink" title="后向计算"></a>后向计算</h3><h4 id="softmax的求导"><a href="#softmax的求导" class="headerlink" title="softmax的求导"></a>softmax的求导</h4><p>设<br>$$<br>\begin{cases}<br>    y = \text{softmax}(z) \<br>    L = f(y)<br>\end{cases}<br>$$</p><p>其中，$L$ 表示 Loss，$f(\cdot)$ 表示 Loss 函数，$y = [y_1 ; y_2 ; y_3]$，$z = [z_1 ; z_2 ; z_3]$，若现在我们想求 $\frac{\partial L}{\partial z_j}$，要怎么计算呢？</p><p>根据链式法则，我们有：<br>$$<br>\frac{\partial L}{\partial z_j} = \frac{\partial L}{\partial y} \frac{\partial y}{\partial z_j}<br>$$<br>所以我们分别来看这两项。</p><p>(1) $\frac{\partial L}{\partial y}$<br>我们现在不考虑具体的 Loss 函数，直接假设这一项的结果为 $[m_1 ; m_2 ; m_3]$。</p><p>(2) $\frac{\partial y}{\partial z_j}$<br>我们知道，对于某个 $z_j$ 来说，在 $\text{softmax}$ 的操作下，它参与了 $y_1, y_2, y_3$ 三者的计算，<br>因此它的偏导和这三者密切切相关，这里我们分成两种情况：<br>$$<br>\begin{cases}<br>\frac{\partial y_i}{\partial z_j} = y_i (1 - y_i), &amp; \text{当 } i = j \<br>\frac{\partial y_i}{\partial z_j} = -y_i y_j, &amp; \text{当 } i \neq j<br>\end{cases}<br>$$</p><p>具体的推倒过程可以看这篇文章：<a href="https://www.cnblogs.com/wuliytTaotao/p/10787510.html">对 softmax 和 cross-entropy 求导 </a></p><p>有了这个理解，我们再来谈谈基于 $y = softmax(z)$ 的 Jacobian 矩阵 $diag(y) - y^T y$：</p>$$\begin{aligned}diag(y) - y^T y &amp;=\begin{bmatrix}y_1 &amp; 0 &amp; 0 \\0 &amp; y_2 &amp; 0 \\0 &amp; 0 &amp; y_3\end{bmatrix}-\begin{bmatrix}y_1 \\ y_2 \\ y_3\end{bmatrix}*\begin{bmatrix}y_1 &amp; y_2 &amp; y_3\end{bmatrix} \\&amp;=\begin{bmatrix}y_1 - y_1^2 &amp; -y_1 y_2 &amp; -y_1 y_3 \\-y_2 y_1 &amp; y_2 - y_2^2 &amp; -y_2 y_3 \\-y_3 y_1 &amp; -y_3 y_2 &amp; y_3 - y_3^2\end{bmatrix}\end{aligned}$$<p>很容易发现只要把每行/每列相加，就能得到对应 $z$ 的偏导。别着急求和，我们继续往下看。</p><p>(3) $ \frac{\partial L}{\partial z_j} = \frac{\partial L}{\partial y} \frac{\partial y}{\partial z_j}$</p><p>有了 (1) (2) 的结果，现在就可以来推导 $\frac{\partial L}{\partial z_j}$，我们有：<br>$$<br>\frac{\partial L}{\partial z_j} = \frac{\partial L}{\partial y} \frac{\partial y}{\partial z_j}<br>= \sum_{i=1}^l \frac{\partial L}{\partial y_i} \frac{\partial y_i}{\partial z_j} = y_j (d y_j - \sum_{j=1}^l y_j d y_j)<br>$$</p><p>举个例子，若我们现在想求 $\frac{\partial L}{\partial z_1}$，我们将 $\frac{\partial L}{\partial y} = [m_1 ; m_2 ; m_3]$ 代入上面公式，则有：</p><p>$$<br>\frac{\partial L}{\partial z_1} = m_1 (y_1 - y_1^2) - m_2 y_1 y_2 - m_3 y_1 y_3<br>$$</p><p>现在，针对所有的 $z$，我们将 $\frac{\partial L}{\partial z}$ 写成矩阵表达式有：</p>$$\begin{aligned}\frac{\partial L}{\partial z} &amp;= \frac{\partial L}{\partial y} \frac{\partial y}{\partial z} = dy(diag(y) - y^T y) \\&amp;= [m_1 \; m_2 \; m_3]\begin{bmatrix}y_1 &amp; 0 &amp; 0 \\0 &amp; y_2 &amp; 0 \\0 &amp; 0 &amp; y_3\end{bmatrix}-\begin{bmatrix}y_1 \\ y_2 \\ y_3\end{bmatrix}\begin{bmatrix}y_1 &amp; y_2 &amp; y_3\end{bmatrix} \\ &amp;= [m_1 \; m_2 \; m_3]\begin{bmatrix}y_1 - y_1^2 &amp; -y_1 y_2 &amp; -y_1 y_3 \\-y_2 y_1 &amp; y_2 - y_2^2 &amp; -y_2 y_3 \\-y_3 y_1 &amp; -y_3 y_2 &amp; y_3 - y_3^2\end{bmatrix}\end{aligned}$$<p><strong>至此，大家记住这两个重要的结论：</strong></p><p>$$<br>\frac{\partial L}{\partial z} = \frac{\partial L}{\partial y} \frac{\partial y}{\partial z} = dy(diag(y) - y^T y)<br>$$</p><p>$$<br>\frac{\partial L}{\partial z_j} = y_j \left( dy_j - \sum_{j=1}^l y_j dy_j \right)<br>$$</p><h4 id="标准后向计算"><a href="#标准后向计算" class="headerlink" title="标准后向计算"></a>标准后向计算</h4><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241209041228.png"><br>首先我们先回顾一下标准前向过程：<br>$$<br>S = QK^T<br>$$</p><p>$$<br>P = \text{softmax}(S)<br>$$</p><p>$$<br>O = PV<br>$$</p><p>$$<br>L = f(O)<br>$$</p><p>对于标准backward来说，在计算开始时，显存（HBM）上已经存放有$Q, K, V, O, S, P$这些数据。</p><h4 id="分块后向计算"><a href="#分块后向计算" class="headerlink" title="分块后向计算"></a>分块后向计算</h4><p>首先回顾一下经过分块 Forward 计算后，显存（HBM）上都存了哪些数据：</p><ul><li>**$m$**：全局 rowmax</li><li>**$l$**：全局 rowsum</li><li>**$Q, K, V$**：等同于标准 attention 场景下的结果</li><li>**$O$**：等同于标准 attention 场景下的输出结果 $O$</li><li>**$dO$**：有了完整的 $O$，我们就可以按正常的 backward 步骤先求出它的梯度，也存放在显存上。<br>然后我们就能按照链式法则，分块地去求列的矩阵的梯度了。</li></ul><p>既然有了全局的 $m, l$，那么现在对任意一块 $S_{ij}$，我们就能基于 $m, l$ 算出和标准场景下完全一致的 $P_{ij}$ 了。<br><strong>因此，在 backward 的过程中，flash attention 将采用重计算的方式，重新算出 $S_{ij}, P_{ij}$，<br>并将它们运用到 backward 的计算中去</strong>，所以在接下来的讲解中，大家就可以把 $S, P$ 理解成完全等同于标准场景下的结果，<br>而不是像分块计算 forward 中那样的 $S, P$。</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241209042346.png"></p><p>(1) 求 $V_j$ 梯度</p><p>由 Forward 过程我们知：$O = PV$，因此有了 $dO$ 后，我们就可以先来求 $dP$ 和 $dV$ 了。<br>观察下方的图，我们会发现此时所有的 $P$ 都是不带波浪号的，再强调一下，这是因为经过了重计算，<br>此处 $S, P$ 的结果都等同于标准场景下的结果，而不是 forward 中所代表的含义。</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241209042743.png"></p><p>假设现在 $j = 0$，那我们要怎么求 $dV_0$ 呢？</p><p>我们先来看 $V_0$ 都参与了 $O$ 哪些部分的计算，以及是怎么参与的：由图可知，$P_{00}$ 和 $V_0$ 参与了 $O_0$ 的计算，<br>$P_{10}$ 和 $V_0$ 参与了 $O_1$ 的计算，$P_{20}$ 和 $V_0$ 参与了 $O_2$ 的计算。所以我们有：</p><p>$$<br>dV_0 = (P_{00})^T dO_0 + (P_{10})^T dO_1 + (P_{20})^T dO_2<br>$$</p><p>进而推知：<br>$$<br>dV_j = \sum_i (P_{ij})^T dO_i<br>$$</p><p>在伪代码 11～15 行中，做的都是 $S, P$ 重计算的过程，伪代码的第 16 行，<br>就是在按这个方法分块计算并累积 $dV_j$。</p><p>(2) 求 $P_{ij}$ 梯度</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241209043105.png"></p><p>观察上图，可以发现 $P_{ij}$ 只与 $V_j, O_i$ 相关，例如 $P_{10}$ 只与 $V_0, O_1$ 相关。因此我们有：</p><p>$$<br>dP_{ij} = dO_i V_j^T<br>$$</p><p>这就是伪代码第 17 行做的事情。</p><p>(3) 求 $S_{ij}$ 梯度</p><p>这一块是令许多人感到迷惑的，我们先来回顾下 “softmax 求导” 部分让大家记住的一个重要结论：</p><p>$$<br>\frac{\partial L}{\partial z} = \frac{\partial L}{\partial y} \frac{\partial y}{\partial z} = dy(diag(y) - y^T y)<br>$$</p><p>我们假设 $s_i, p_i, o_i$ 分别为矩阵 $S, P, O$ 的某一行（注意这里 $i$ 不是表示第 $i$ 块的意思，是表示第 $i$ 行，所以我们用小写的 $s, p, o$ 表示），那么根据这个结论，我们有：</p><p>$$<br>\begin{aligned}<br>ds_i &amp;= dp_i \left( diag(p_i) - p_i^T p_i \right) \<br>&amp;= dp_i diag(p_i) - dp_i p_i^T p_i \<br>&amp;= dp_i diag(p_i) - dO_i V_j^T p_i \<br>&amp;= dp_i diag(p_i) - dO_i o_i^T p_i \<br>&amp;= p_i \circ \left[ dp_i - \text{rowsum}(dO_i \cdot o_i) \right]<br>\end{aligned}<br>$$<br>你可能对这个推导的最后一步有疑惑：为什么要大费周章，将 $ds_i$ 改写成这么复杂的形式呢？因为在最后一步之前，我们都是针对“某一行”来求导，而引入最后一步的目的，是为了延展至对“某一块（多行）”的求导，也就是说针对某一块 $dS_i$（注意这里是大写的 $S$，$i$ 的含义也回归至“第几块”），我们有：</p><p>$$<br>dS_i = P_i \circ \left[dP_i - \text{rowsum}(dO_i \circ O_i)\right]<br>$$</p><p>如果实在难以理解推导过程，建议大家可以带一些具体的值进去，就能理解我们为什么要写成这种形式了。进而，我们可以推知：</p><p>$$<br>dS_{ij} = P_{ij} \circ \left[dP_{ij} - \text{rowsum}(dO_i \circ O_i)\right]<br>$$</p><p>这就是伪代码第 19～20 行做的事情。</p><p>(4) 求 $Q_i$ 梯度</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241209043616.png"></p><p>到目前为止，我们已经知道 $dS_{ij}$，那么现在就可以根据链式法则继续求 $dQ_i$ 了。</p><p>对照上图，我们把目光聚焦在 $Q_0$ 身上，由 forward 过程可知：</p><p>$$<br>S_{00} = Q_0 K_0^T<br>$$</p><p>$$<br>S_{01} = Q_0 K_1^T<br>$$</p><p>因此，针对 $Q_0$，我们有：</p><p>$$<br>dQ_0 = dS_{00} K_0 + dS_{01} K_1<br>$$</p><p>推广到任意 $Q_i$，我们有：</p><p>$$<br>dQ_i = \sum_j dS_{ij} K_j<br>$$</p><p>这就是伪代码第 21 行做的事情。</p><p>(5) 求 $K_j$ 梯度<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241209043830.png"></p><p>这一步就很简单啦，如果你被复杂的分块推导弄得晕了脑袋，那不妨再复习一下我们前面提过的 trick：<br>对照上图，取出某一块 $K_j$。由于我们是从 $dS_{ij}$ 链式推向 $K_j$，所以这里只要搞明白这块 $K_j$ 和哪些 $Q_i$ 一起计算出了哪些 $S_{ij}$ 再把相关结果相加即可。</p><p>只要看了流程图，就不难得知：某块 $K_j$ 和对应的 $Q_i$ 共同计算出了对应的 $S_{ij}$，因此有：</p><p>$$<br>dK_j = \sum_i dS_{ij}^T Q_i<br>$$</p><p>这就是伪代码第 22 行做的事情。</p><h2 id="计算量与显存占用"><a href="#计算量与显存占用" class="headerlink" title="计算量与显存占用"></a>计算量与显存占用</h2><h3 id="矩阵相乘的计算量"><a href="#矩阵相乘的计算量" class="headerlink" title="矩阵相乘的计算量"></a>矩阵相乘的计算量</h3><p>我们先来看一个前置知识：两个矩阵相乘，要怎么统计它们的计算量？</p><p>我们一般用<strong>FLOPs（floating point operations，浮点运算次数）</strong>来表示运算量的大小。对于“两矩阵相乘”这个操作而言，其 **运算量 = <strong>乘法运算的次数 + 加法运算的次数</strong>。</p><p>来看一个具体例子：</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241209224248.png"></p><p>两矩阵相乘，为了获取图中深橘色部分的元素，我们一共需要进行<strong>n次乘法运算和n-1次加法运算</strong>。</p><p>对于示例矩阵，我们需要进行：$mp \cdot (n + n - 1)$ 次浮点计算。</p><p>再进一步，假设此时在蓝色和绿色的矩阵外，我们还有一个bias矩阵，意味着计算单个橘色方块时我们需要进行<em>n次乘法和n-1+1次加法运算</em>，那么此时总计算量为：$mp \cdot (n+n) = 2mnp$。当然，即使不加这个bias，我们也可以把-1项给忽略，得到相同的结果。</p><p>总结一下：</p><ul><li>假设有两个矩阵A和B，它们的维度分别为(m, n)和(n, p)，则这两矩阵相乘的运算量为<strong>2mnp</strong>。</li><li>由于乘法运算的时间要高于加法运算的时间，因此有时在统计运算量时，我们只考虑乘法运算的次数，则此时两矩阵相乘的运算量可近似为<strong>mnp</strong>。</li></ul><h3 id="Flash-Attention的计算量"><a href="#Flash-Attention的计算量" class="headerlink" title="Flash Attention的计算量"></a>Flash Attention的计算量</h3><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241209021146.png"></p><p>我们知道矩阵相乘运算占据了运算量的大头，因此我们把分析目光集中到所有的矩阵运算上来。</p><p>(1) 在代码第 9 行，我们有 $S_{ij} = Q_i K_j^T$，其中 $Q_i \in \mathbb{R}^{B_r \times d}$，$K_j^T \in \mathbb{R}^{d \times B_c}$。根据前置知识，求 $S_{ij}$ 的计算量为 $O(B_r B_c d)$。</p><p>(2) 在代码第 12 行，我们有 $\tilde{P}<em>{ij} V_j$，其中 $\tilde{P}</em>{ij} \in \mathbb{R}^{B_r \times B_c}$，$V_j \in \mathbb{R}^{B_c \times d}$。则这里的计算量同样为 $O(B_r B_c d)$。</p><p>(3) 接下来我们看一共计算了多少次 (1) 和 (2)，也就是执行了多少次内循环：</p><p>$$<br>T_c T_r = \frac{N}{B_c} \cdot \frac{N}{B_r}<br>$$</p><p>(4) 综合以上三点，<strong>flash attention 的 forward 计算量为</strong>：</p><p>$$<br>O\left(\frac{N^2}{B_r B_c} B_r B_c d \right) = O(N^2 d)<br>$$</p><p>注意，因为计算量是用大 O 阶表示的，所以这里我们把常数项都省略了。</p><p>同理大家可以自行推一下 backward 中的计算量，在论文里给出的结论是 $O(N^2)$，d 远小于 N，因此 d 也可以略去不表述。</p><h3 id="Flash-Attention的显存占用"><a href="#Flash-Attention的显存占用" class="headerlink" title="Flash Attention的显存占用"></a>Flash Attention的显存占用</h3><p>和标准 attention 相比，如果不考虑 $O$ 的话，Flash Attention 只需要存储 $m, l$，其显存需求为 $O(N)$。</p><p>而标准 attention 需要存储 $S, P$，其显存需求为 $O(N^2)$。</p><p>FlashAttention 将显存需求降低到 O(N)，通过分块处理和重计算，<strong>显著减少了显存使用</strong>。实验显示，其显存消耗可减少至标准 Attention 的 1/20。</p><h2 id="IO复杂度分析"><a href="#IO复杂度分析" class="headerlink" title="IO复杂度分析"></a>IO复杂度分析</h2><p>flash attention相比于标准attention的最大优势，就是其<strong>减少了对显存（HBM）的访问次数</strong>，一定程度上解决了memory bound的问题。所以这一节我们就来具体分析这两者对显存的访问次数（同样都是以forward为例，backward部分论文中也有给出相关推导过程，大家可以类比forward自行阅读）。</p><h3 id="标准Attention的IO复杂度"><a href="#标准Attention的IO复杂度" class="headerlink" title="标准Attention的IO复杂度"></a>标准Attention的IO复杂度</h3><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241209233647.png"></p><ol><li><p><strong>从 HBM 中读取 $Q, K \in \mathbb{R}^{N \times d}$，计算 $S = Q K^T$，$S \in \mathbb{R}^{N \times N}$ 并将 $S$ 写回 HBM</strong>。</p><ul><li>一读一写的 IO 复杂度为：$O(2Nd + N^2)$。</li></ul></li><li><p><strong>从 HBM 中读取 $S \in \mathbb{R}^{N \times N}$，同时计算 $P \in \mathbb{R}^{N \times N}$ 并将其写回 HBM</strong>。</p><ul><li>一读一写的 IO 复杂度为：$O(2N^2)$。</li></ul></li><li><p><strong>从 HBM 中读取 $P \in \mathbb{R}^{N \times N}, V \in \mathbb{R}^{N \times d}$，计算 $O = P V, O \in \mathbb{R}^{N \times d}$ 并将 $O$ 写回 HBM</strong>。</p><ul><li>两读一写的 IO 复杂度为：$O((N^2 + Nd) + Nd)$。</li></ul></li></ol><p><strong>因此，总体来说标准 attention 的 IO 复杂度为：$O(Nd + N^2)$。</strong></p><h3 id="Flash-Attention的IO复杂度"><a href="#Flash-Attention的IO复杂度" class="headerlink" title="Flash Attention的IO复杂度"></a>Flash Attention的IO复杂度</h3><ol><li><p>我们来看伪代码的第 6 行，在每个外循环中，我们都会加载 $K, V$ 的 block。所有外循环结束后，相当于我们加载了完整的 $K, V \in \mathbb{R}^{N \times d}$，因此这里的 IO 复杂度为：$O(2Nd)$。</p></li><li><p>再看伪代码第 8 行，在每个内循环中，我们都加载部分 $Q, O, m, l$ block。由于 $m, l$ 本身比较小（IO 复杂度是 $O(N)$），因此我们暂时忽略它们，只考虑 $Q, O$（原论文也是这么分析的）。<br>固定某个小循环，对于所有内循环结束后，我们相当于完整遍历了 $Q, O \in \mathbb{R}^{N \times d}$。同时我们经历了 $T_c$ 次外循环。因此这里最终的 IO 复杂度为：$O(T_c Nd)$。</p></li><li><p><strong>将 $O, m, l$ 写回 HBM</strong>，这里近似后 IO 复杂度为：$O(Nd)$。</p></li></ol><p>不过在原论文的分析中并没有考虑写回的复杂度，不过省略一些常数项不会影响我们最终的分析。</p><p>总体来说，<strong>flash attention 的 IO 复杂度为</strong>：<br>$$<br>O(T_c Nd + Nd) = O\left(\frac{N}{B_c} Nd \right) = O\left(\frac{4Nd}{M} Nd \right) = O\left(N^2 d \frac{d}{M} \right)<br>$$<br>在文章中提到，一般的 $d$ 取值在 64～128，$M$ 的取值在 100KB 左右，因此有 $\frac{d^2}{M} \ll 1$。因此可以看出，<strong>Flash Attention 的 IO 复杂度是显著小于标准 attention 的 IO 复杂度的</strong>。</p><h3 id="复杂度总结"><a href="#复杂度总结" class="headerlink" title="复杂度总结"></a>复杂度总结</h3><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241210000354.png"></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241210000654.png"></p><p>Flash attention 的作者将 $N = 1024$, $d = 64$, $B = 64$ 的 GPT2-medium 部署在 A100 GPU 上，来观察采用 flash attention 前后的模型的计算性能。</p><p>我们先看最左侧图表，标准 attention 下，**计算强度 $I = \frac{66.6}{40.3} \approx 1.6 &lt; 201$**，说明 GPT2 在 A100 上的训练是受到内存限制的。而在采用 flash attention 后得到明显改善，runtime 也呈现了显著下降。</p><p>我们再来看中间的图表，它表示在使用 flash attention 的前提下，以 forward 过程为例，每个数据块的大小对 HBM 读写次数（绿色）和耗时（蓝色）的影响。可以发现，数据块越大，读写次数越少，而随着读写次数的减少，runtime 也整体下降了（复习一下，读写复杂度为 $O(T_c Nd)$，数据块越大意味着 $T_c$ 越小）。<strong>但有趣的是，当数据块大小 $&gt; 256$ 后，runtime 的下降不明显了，这是因为随着矩阵的变大，计算耗时也更大了，会抵平读写节省下来的时间</strong>。</p><p>参考资料</p><blockquote><p>图解大模型计算加速系列：FlashAttention V1，从硬件到计算逻辑:<a href="https://zhuanlan.zhihu.com/p/669926191">https://zhuanlan.zhihu.com/p/669926191</a><br>图解大模型计算加速系列：Flash Attention V2，从原理到并行计算:<a href="https://zhuanlan.zhihu.com/p/691067658">https://zhuanlan.zhihu.com/p/691067658</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLMs </tag>
            
            <tag> FlashAttention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>个人博客加入谷歌元标记与站点地图</title>
      <link href="/2024/12/04/ge-ren-bo-ke-jia-ru-gu-ge-yuan-biao-ji-yu-zhan-dian-di-tu/"/>
      <url>/2024/12/04/ge-ren-bo-ke-jia-ru-gu-ge-yuan-biao-ji-yu-zhan-dian-di-tu/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在建立好我们的博客或者个人网站后，需要再让自己的链接被搜索引擎所收录。这里有两种方法能够被搜索引擎添加自己网站的索引。一个是自己努力提高自己的网站知名度，让搜索引擎主动去添加索引。另外一种就是自己把自己的链接添加到搜索引擎的索引当中。</p><h2 id="查看是否收录"><a href="#查看是否收录" class="headerlink" title="查看是否收录"></a>查看是否收录</h2><p>在google或者百度等搜索引擎中搜索<code>site:website addres</code>，查看是否已经被收录。</p><p>如果搜索不出来自己等网站，那么就需要自己添加到搜索引擎的索引当中。下面以google为例，介绍如何添加自己的网站到google的索引当中。</p><h2 id="添加谷歌元标记"><a href="#添加谷歌元标记" class="headerlink" title="添加谷歌元标记"></a>添加谷歌元标记</h2><p>首先你需要一个谷歌账号，然后使用<a href="https://search.google.com/search-console/about">Google Search Console</a>服务，点击立即使用！</p><p>接下来你可以看到如下的验证界面:<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205035222.png"></p><p>这里选择前缀，然后输入自己网站地址。之后按照提示添加meta标签到自己的网站中。对于hexo博客，可以在<code>themes/next/layout/_partials/head.ejs</code>文件中添加。</p><p>在添加完元标记后，进行网址检查，如果显示网址未收录，点击右下角的请求编入索引，别着急，一般来说一到两天后Google就会收录。</p><h2 id="添加站点地图"><a href="#添加站点地图" class="headerlink" title="添加站点地图"></a>添加站点地图</h2><h3 id="生成站点地图"><a href="#生成站点地图" class="headerlink" title="生成站点地图"></a>生成站点地图</h3><ul><li>安装sitemap插件：<code>npm install hexo-generator-sitemap --save</code></li><li>生成站点地图：<code>hexo generate</code>，然后在<code>public</code>目录下会生成<code>sitemap.xml</code>文件</li><li>修改配置：<ul><li>在<code>_config.yml</code>文件中添加如下配置  <pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml">Plugins:    - hexo-generator-sitemapsitemap:    path: /sitemap.xml</code></pre></li><li>在主题配置文件中添加如下配置  <pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml">menu:...sitemap: /sitemap.xml || fa fa-sitemap...</code></pre></li></ul></li></ul><h4 id="提交站点地图"><a href="#提交站点地图" class="headerlink" title="提交站点地图"></a>提交站点地图</h4><p>还是Google Search Console，点击左侧边栏中的站点地图，添加新的站点地图，在主站地址后面填入sitemap.xml，即与前面生成的站点地图文件名称相同！</p><p>等待Googlec处理，一般来说一到两天后Google就会收录。</p><blockquote><p><a href="https://blog.csdn.net/tzhuwb/article/details/125477001">https://blog.csdn.net/tzhuwb/article/details/125477001</a><br><a href="https://mizeri.github.io/2021/04/18/hexo-sitemap-google/">https://mizeri.github.io/2021/04/18/hexo-sitemap-google/</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> google search console </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> sitemap </tag>
            
            <tag> google search console </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型高效微调方法PEFT--LoRA/QLoRA</title>
      <link href="/2024/12/04/da-mo-xing-gao-xiao-wei-diao-fang-fa-peft-lora-qlora/"/>
      <url>/2024/12/04/da-mo-xing-gao-xiao-wei-diao-fang-fa-peft-lora-qlora/</url>
      
        <content type="html"><![CDATA[<h2 id="参数高效微调PEFT"><a href="#参数高效微调PEFT" class="headerlink" title="参数高效微调PEFT"></a>参数高效微调PEFT</h2><h4 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h4><p>微调（Fine-tuning）是一种迁移学习的技术，用于在一个已经预训练好的模型基础上，通过进一步训练来适应特定的任务或数据集。微调可以在具有相似特征的任务之间共享知识，从而加快训练速度并提高模型性能。</p><p>以下是一般的微调步骤：</p><ul><li><strong>选择择预训练模型</strong>：选择一个在大规模数据集上预训练好的模型，如ImageNet上的预训练的卷积神经网络（如ResNet、VGG等）。这些模型通常具有良好的特征提取能力。</li><li><strong>冻结底层权重</strong>：将预训练模型的底层权重（通常是卷积层）固定住，不进行训练。这是因为底层权重通常学习到了通用的特征，可以被用于许多不同的任务。</li><li><strong>替换顶层分类器</strong>：将预训练模型的顶层分类器（通常是全连接层）替换为适合特定任务的新的分类器。新的分类器的输出节点数量应该与任务的类别数相匹配。</li><li><strong>解冻部分权重（可选）</strong>：根据任务的复杂性和可用的训练数据量，可以选择解冻一些底层权重，以便更好地适应新的任务。这样可以允许底层权重进行微小的调整，以更好地适应新任务的特征。</li><li><strong>进行训练</strong>：使用特定任务的训练数据集对新的分类器进行训练。可以使用<strong>较小的学习率</strong>进行训练，以避免对预训练模型的权重进行过大的更新。</li><li><strong>评估和调整</strong>：在训练完成后，使用验证集或测试集评估模型的性能。根据评估结果，可以进行调整，如调整学习率、调整模型结构等。</li></ul><p>微调的关键是在预训练模型的基础上进行训练，从而将模型的知识迁移到特定任务上。通过这种方式，可以在较少的数据和计算资源下，快速构建和训练高性能的模型。</p><h4 id="PEFT"><a href="#PEFT" class="headerlink" title="PEFT"></a>PEFT</h4><p>PEFT（Parameter-Efficient Fine-Tuning，参数高效微调）是一种在大语言模型（Large Language Model，LLM）上进行微调的新技术，旨在降低微调大模型的计算和存储成本，使得微调过程更为高效。与传统的微调技术相比，PEFT 可以显著减少训练所需的参数量，并保持与标准微调相似的性能。PEFT 技术在优化资源效率、降低硬件需求、并提高开发灵活性方面有显著优势，特别是在需要频繁微调以适应不同任务或领域的场景中尤为有用。</p><p>目前主流的方法包括2019年 Houlsby N 等人提出的 Adapter Tuning，2021年微软提出的 LORA，斯坦福提出的 Prefix-Tuning，谷歌提出的 Prompt Tuning，2022年清华提出的 P-tuning v2。</p><ul><li>Adapter Tuning 在模型的某些层之间插入小的适配器模块，而不对原有的大量参数进行修改。适配器模块通常包含一小部分可训练的参数，并通过这些参数来调整模型的输出；但是增加了模型层数，引入了额外的推理延迟</li><li>Prefix-Tuning 每个 Transformer 层前面添加一个可学习的前缀（prefix）来实现模型的调整。这个前缀是固定长度的，可视作特定任务的提示（prompt）；但是预留给 Prompt 的序列挤占了下游任务的输入序列空间，影响模型性能</li><li>P-tuning v2 使用可学习的嵌入来替代硬编码的提示文本；但是很容易导致旧知识遗忘，微调之后的模型，在之前的问题上表现明显变差</li><li>基于上述背景，LORA 得益于前人的一些关于内在维度（intrinsic dimension）的发现：</li></ul><blockquote><p>模型是过参数化的，它们有更小的内在维度，模型主要依赖于这个低的内在维度（low intrinsic dimension）去做任务适配。</p></blockquote><p>假设模型在任务适配过程中权重的改变量是低秩（low rank）的，由此提出<strong>低秩自适应</strong>（LoRA）方法。</p><p>LoRA 允许我们通过优化适应过程中密集层变化的秩分解矩阵，来间接训练神经网络中的一些密集层，同时保持预先训练的权重不变。</p><h2 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h2><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241204190635.png"><br>LoRA 的思想很简单:</p><ul><li><p>在原始 PLM (Pre-trained Language Model) 旁边增加一个旁路，做一个降维再升维的操作，来模拟所谓的intrinsic rank。</p></li><li><p>训练的时候固定 PLM 的参数，只训练降维矩阵 与升维矩阵。而模型的输入输出维度不变，输出时将与 PLM 的参数叠加。</p></li><li><p>用随机高斯分布初始化 A，用 0 矩阵初始化 B，保证训练的开始此旁路矩阵依然是 0 矩阵。</p><blockquote><p>为什么A用随机高斯分布初始化，B用0矩阵初始化呢？</p></blockquote></li><li><p>首先增量矩阵$\Delta W = BA$ 在训练刚开始肯定是要用0来初始化。</p></li><li><p>如果A用0矩阵初始化，在计算梯度时由于B的梯度有一个因子A，导致B的梯度始终为0，无法被更新，导致梯度传播受阻。</p></li></ul><p>我们也可以理解为因为A是降维矩阵，所以它能够学到一些有用的信息，所以用随机高斯分布初始化；而B是升维矩阵，我们希望它能够保持原始信息，所以用0矩阵初始化。</p><p><strong>秩的选择</strong><br>在 Transformer 中，LoRA 主要作用在 attention 过程的四个权重矩阵W_Q, W_K, W_V, W_O（也可以选择其中部分）。A，B矩阵的秩r也时远远小于原始的权重大小。<br>对于一般的任务， r=1，2，4，8就足够了。而一些领域差距比较大的任务可能需要更大的r。<br>同时，增加r值变大并不能提升微调的效果，这可能是因为参数量增加需要更多的语料.<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241204204545.png"></p><p><strong>实现</strong><br>huggingface:<br><a href="https://huggingface.co/docs/peft/main/conceptual_guides/lora">https://huggingface.co/docs/peft/main/conceptual_guides/lora</a><br>llama_factory:<br><a href="https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/sft.html">https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/sft.html</a><br>关键参数：</p><ul><li>lora_r：秩，控制低秩矩阵的表示能力。</li><li>lora_alpha：缩放因子，控制 LoRA 的影响力。</li><li>lora_dropout：dropout 概率，防止过拟合。</li><li>lora_target_modules：指定微调的目标层。</li><li>lora_trainable_only：是否只训练 LoRA 增加的部分。</li><li>lora_init_scale：控制低秩矩阵初始化的尺度。</li><li>lora_layers_to_freeze：指定冻结哪些层。</li><li>lora_lr：设置 LoRA 的学习率。</li><li>lora_warmup_steps：指定热身阶段的步数。</li></ul><h2 id="QLoRA"><a href="#QLoRA" class="headerlink" title="QLoRA"></a>QLoRA</h2><p>QLoRA（Quantized Low-Rank Adapter）是一种高效的微调方法，是 LoRA 的量化版本（什么是 LoRA？）。该调优方法由华盛顿大学发表于论文《QLORA: Efficient Finetuning of Quantized LLMs》。通过降低内存使用，实现在单个 GPU 上对大型语言模型进行微调。它可以在单个 48GB GPU 上微调 650 亿个参数的模型，并且能够保持完整的 1 6 位微调任务性能。</p><p>QLoRA 核心是使用量化进行微调。其成果有三个。</p><ul><li>4 位标准浮点数量化（4-bit NormalFloat Quantization）</li><li>双重量化（Double Quantization）</li><li>分页优化（Paged Optimizer）就是显存不足时将部分梯度检查点转移到内存上。</li></ul><p>为了降低极端的量化对微调造成的不利影响，<strong>QLoRA 借助 LoRA 对原模型权重进行更新</strong>，且 LoRA 侧的权重保持高精度。这就是 QLoRA 与 LoRA 的关系。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241204231609.png"></p><h3 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h3><h4 id="量化"><a href="#量化" class="headerlink" title="量化"></a>量化</h4><p><strong>量化</strong>指的是将连续或高精度的数值转换为较低精度（比如较少的位数）的表示形式的过程。这通常用于减少模型的存储需求和加快其运算速度。例如，将一个 FP32 的 tensor 转成 Int8:<br>$$X^{\text{Int8}} = \text{round}\left(\frac{127}{\text{absmax}(X^{\text{FP32}})} \cdot X^{\text{FP32}}\right)=round(c^{\text{FP32}} \cdot X^{\text{FP32}})$$ </p><p>其中，c 为量化常数，通常是这个张量的特征的绝对值的最大值。逆量化公式则为：<br>$$\text{dequant}(c^{\text{FP32}}, X^{\text{Int8}}) = \frac{X^{\text{Int8}}}{c^{\text{FP32}}}$$</p><p>按照量化过程是否以0点为对称点量化又可以分为对称量化和非对称量化。其中对称量化将原浮点数的最小或最大值的绝对值作为映射值的范围，而非对称量化是将原浮点数的最小和最大值映射为量化数据的最小和最大值。在非对称量化中，0的映射也可能会有偏移，因此不一定会被映射到0。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205004225.png"></p><h4 id="分位数量化"><a href="#分位数量化" class="headerlink" title="分位数量化"></a>分位数量化</h4><p>分位数量化（Quantile Quantization）是隶属于非线性量化。分位数（Quantile）在数学上的定义指的是把顺序排列的一组数据分割为若干个相等块的分割点的数值。在标准正态分布中，对于分布X给定的概率$\alpha$，如果存在$\mu_\alpha$使得它的累积分布函数（CDF） $P(X &lt; \mu_\alpha)= \alpha$，则称 $\mu_\alpha$是标准正态分布的 $\alpha$分位数，因为CDF表示的是概率值小于$\mu_\alpha$的阴影部分的面积，因此具有严格递增的特性，所以它一定存在反函数。CDF的反函数的一个重要作用是用来生成服从该随机分布的随机变量。假设a是[0,1)区间上均匀分布的一个随机变量，那么 $F_{X}^{-1}(a)$服从分布$X$。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205004852.png"></p><p><strong>分位数量化是通过分位数将张量分成了大小相同的若干个块，这样我们得到更加均匀的量化特征</strong>，对于4比特量化，我们希望需要找到15个分位数来将这个曲线下面的面积（积分）等分成16份。两个分位数的中点便是模型量化到这个区间映射的值 $q_i$。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205005255.png"></p><p>由于大模型参数通常服从正态分布，因此有：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205015121.png"><br>其中$Q$是CSF的反函数。</p><h4 id="分块k位量化"><a href="#分块k位量化" class="headerlink" title="分块k位量化"></a>分块k位量化</h4><p>常规的量化方法的局限性是当 tensor 中有一个非常大的数字（一般称为 outlier）时，导致大多数值被压缩到较小的范围，精度损失大，影响最终的量化结果。</p><p>因此，Block-wise k-bit Quantization 方法就是把 tensor 分割成 B 块，每块有自己的量化常数 c，独自量化。从而解决模型参数的极大极小的异常值的问题。分块量化的另外一个好处是减少了核之间的通信，可以实现更好的并行性，并充分利用硬件的多核的能力。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205010158.png"></p><h3 id="4位标准浮点量化—NF4"><a href="#4位标准浮点量化—NF4" class="headerlink" title="4位标准浮点量化—NF4"></a>4位标准浮点量化—NF4</h3><p>4-bit NormlFLoat 量化是结合了分位数量化和分块量化，<br>使用上面介绍的分位数量化方法我们可以将FP2精度量化到4bit的精度，但是直接这么用的一个问题是不能保证高精度的0一定被映射到低精度的0，但是0点又是深度学习中一个重要的值，例如在模型稀疏化，数据padding的时候一般都是使用0来完成。</p><p>假设offset的值是0.99，我们可以通过下面的代码片段计算出它的16个 $q_i$。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">offset = 0.99num_bins = 16quantile = norm.ppf(torch.linspace(1 - offset, offset, num_bins + 1)).tolist() # 将[1-offset,offset]区间等分为16份tmp = [(quantile[1:][idx] + val) / 2 for idx, val in enumerate(quantile[:-1])] # 计算分位数r_max, r_min = tmp[-1], tmp[0]S = (r_max - r_min)/(1 - (-1))Z = 1 - r_max / SQ = [x/S + Z for x in tmp] # 分位数量化到[-1,1]print (Q)&gt;&gt;&gt; Q = [-1.0, -0.680534899946304, -0.5217156169574965, -0.4015399993912077, -0.299784167882981, -0.20835410767681603, -0.12291223249970012, -0.040639059218818274, 0.04063881956774142, 0.12291199284862328, 0.20835391124150712, 0.2997839714476721, 0.40153976366883704, 0.5217154126647753, 0.6805348056573558, 1.0]</code></pre><p>这种方式的一个问题是0的映射值不是0，如果我们考虑奇数个bin，0是可以有个映射值但是却无法充分利用4比特的16位的信息。为了确保零点映射到0并且使用4位数据类型的全部16位，我们通过估计正负两个范围的分位数来创建一个非对称的数据类型：负数部分映射其中7位，正数部分映射8位，0占据1位，总共用满了4位数的16位。另外我们也可以使用对称的量化，其中正数和负数均使用7位，0占用2个位。我这里和论文介绍的略有不同，论文说的是正数部分取9个值，负数部分取8个值，不过它们都会取到0，所以合并时再去掉一个重复的0，这两个说法其实是一样的，只是实现方式略有差异。</p><p>接下来根据作者的源码来看下量化分位数如何计算的。其中核心代码片段摘抄如下。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">from scipy.stats import normimport torchdef create_normal_map(offset=0.9677083, use_extra_value=True, num_bins=16):    # INT8 : num_bins = 256    # INT4 : num_bins = 16    if use_extra_value:        # one more positive value, this is an asymmetric type        v1 = norm.ppf(torch.linspace(offset, 0.5, 9)[:-1]).tolist() # 正数部分        v2 = [0]*(num_bins-15) ## we have 15 non-zero values in this data type        v3 = (-norm.ppf(torch.linspace(offset, 0.5, 8)[:-1])).tolist() #负数部分        v = v1 + v2 + v3    else:        v1 = norm.ppf(torch.linspace(offset, 0.5, 8)[:-1]).tolist()        v2 = [0]*(num_bins-14) ## we have 14 non-zero values in this data type        v3 = (-norm.ppf(torch.linspace(offset, 0.5, 8)[:-1])).tolist()        v = v1 + v2 + v3    values = torch.Tensor(v)    values = values.sort().values    values /= values.max()    assert values.numel() == num_bins    return values    Q = create_normal_map()&gt;&gt;&gt; Q = [-1.0, -0.6961928009986877, -0.5250730514526367, -0.39491748809814453, -0.28444138169288635, -0.18477343022823334, -0.09105003625154495, 0.0, 0.07958029955625534, 0.16093020141124725,0.24611230194568634, 0.33791524171829224, 0.44070982933044434, 0.5626170039176941, 0.7229568362236023, 1.0]函数create_normal_map有两个入参：offset和use_extra_value。其中offset的作用是确定分位数的始末值。use_extra_value用来控制是使用对称量化还是非对称量化。v1计算正数部分，v3计算负数部分。v2直接将0映射到0，并且根据要量化的单位计算0的个数。源码是使用NF4来表示8比特的量化，如果是使用4比特的量化，我们将里面的256改成16就行。接下来最后几行用来将量化值归一化到[-1,1]。接下来我们举一个具体的实例。假设一个张量有16个值，它的被分成了4块：```pythoninput_blocked_tensor = [[-1.28645003578589, -1.817660483275528, 9.889441349505042, 0.010208034676132627], [ -15.009014631551885, 1.4136255086268115, -7.815595761491153, 10.766760590950263],  [-0.731406153917959, 3.468224595908726, 2.445252541840315, -8.970824523299282],  [-9.641638854625175, 7.696158363188889, -5.323939281255154, 5.97160401402024]]</code></pre><p>根据每个块的特征的绝对值的最大值，我们为每个块保存一个量化常数，它的计算方式是每个块中特征的绝对值中最大的那个：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">c1 = max(|-1.28645003578589|, |-1.817660483275528|, |9.889441349505042|, |0.010208034676132627|) = 9.889441349505042c2 = max(|-15.009014631551885|, |1.4136255086268115|, |-7.815595761491153|, |10.766760590950263|) = 15.009014631551885c3 = max(|-0.731406153917959|, |3.468224595908726|, |2.445252541840315|, |-8.970824523299282|) = 8.970824523299282c4 = max(|-9.641638854625175|, |7.696158363188889|, |-5.323939281255154|, |5.97160401402024|) = 9.641638854625175</code></pre><p>最后我们便可以计算这个张量的量化值了。例如第一个值-1.28645003578589，它除以这个块的量化常数c1后得到-0.13008318572517502，接下来我们要按照分位数的值来量化这个值。-0.13008318572517502在Q中最接近的值是-0.12291223249970012，这个值在Q中对应的索引是6，因此这个值被量化后的值是6。同理我们可以得到这个输入张量所有的值量化后的结果。<strong>在模型保存时，除了要保存量化后的值，我们还要保存每个块对应的量化常数c_i</strong>，因为这个值在我们进行反量化时需要用到。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">[[6, 5, 15, 7],[0, 8, 2, 14],[6, 11, 10, 0],[0, 14, 2, 13]]</code></pre><p>在反量化时，我们以量化结果作为索引，从Q中查找到它对应的分位数，再乘以为每个块保存的量化常数c_i，便可以得到最终结果。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">[[-0.9004339933799617, -1.8273060011889755, 9.889441349505042, 0.0], [-15.009014631551885, 1.1944218804231184,  -7.880829111886221,  10.850869732860506], [-0.816793898052648, 3.0313783372030603, 2.2078302737800004, -8.970824523299282], [-9.641638854625175, 6.970488722350373, -5.062564734402345, 5.424549965245643]]</code></pre><h3 id="双量化"><a href="#双量化" class="headerlink" title="双量化"></a>双量化</h3><p>在上面我们介绍到，当我们保存模型时我们不仅要保存量化后的结果，还要保存每个块的量化常数。虽然量化后的参数只有4bit的精度，但是这个量化常量的精度是float32。</p><p><strong>在QLoRA中，每个块的大小是64，块中的每个值占4比特。这相当于为了存储量化常数，模型要额外占用 32/（64*4）=12.5% 的显存。</strong></p><p>QLoRA的双重量化就是对这个量化常数再做一次8bit的量化，在进行量化常数的量化时，QLoRA以每256个量化常数为一组再做一次量化。因此它额外增加的内存消耗有两部分组成，一部分是量化后的8bit的第一层的量化常数，它额外增加的显存占比是 8/（64x4）= 3.125%， 第二部分是为量化常数做量化的第二层的32bit的量化常数，它额外增加的显存占比是32/（256x64x4）= 0.049%，额外显存增加只有3.17%。</p><h3 id="分页优化器"><a href="#分页优化器" class="headerlink" title="分页优化器"></a>分页优化器</h3><p>分页优化是针对梯度检查点做的进一步优化，以防止在显存使用峰值时发生显存OOM的问题。QLoRA分页优化其实就是当显存不足是，将保存的部分梯度检查点转移到CPU内存上，和计算机的内存数据转移到硬盘上的常规内存分页一个道理。</p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>要使用 HuggingFace 进行 QLoRA 微调，您需要安装BitsandBytes 库和 PEFT 库。BitsandBytes 库负责 4 位量化以及整个低精度存储和高精度计算部分。PEFT 库将用于 LoRA 微调部分。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_modelfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfigmodel_id = "EleutherAI/gpt-neox-20b"bnb_config = BitsAndBytesConfig(    load_in_4bit=True,    bnb_4bit_use_double_quant=True,    bnb_4bit_quant_type="nf4",    bnb_4bit_compute_dtype=torch.bfloat16) # setup bits and bytes configmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={"":0})model.gradient_checkpointing_enable()model = prepare_model_for_kbit_training(model) # prepares the whole model for kbit trainingconfig = LoraConfig(    r=8,     lora_alpha=32,     target_modules=["query_key_value"],     lora_dropout=0.05,     bias="none",     task_type="CAUSAL_LM")model = get_peft_model(model, config) # Now you get a model ready for QLoRA training</code></pre><p>在模型的微调过程中，虽然主干参数以 4-bit 精度存储，但计算（例如前向传播、反向传播和梯度计算）可以使用更高精度的数据类型<code>bnb_4bit_compute_dtype</code>，以减少由于低精度存储引起的累积误差。</p><p>参考资料：</p><blockquote><p><a href="https://arxiv.org/pdf/2106.09685">https://arxiv.org/pdf/2106.09685</a></p></blockquote><blockquote><p><a href="https://arxiv.org/abs/2305.14314">https://arxiv.org/abs/2305.14314</a></p></blockquote><blockquote><p><a href="https://zhuanlan.zhihu.com/p/623543497">https://zhuanlan.zhihu.com/p/623543497</a></p></blockquote><blockquote><p><a href="https://mingchao.wang/ShYWOOwr/">https://mingchao.wang/ShYWOOwr/</a></p></blockquote><blockquote><p><a href="https://zhuanlan.zhihu.com/p/690739797">https://zhuanlan.zhihu.com/p/690739797</a></p></blockquote><blockquote><p><a href="https://zhuanlan.zhihu.com/p/666234324">https://zhuanlan.zhihu.com/p/666234324</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> PEFT </tag>
            
            <tag> LoRA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型优化--PagedAttention</title>
      <link href="/2024/12/03/da-mo-xing-you-hua-pagedattention/"/>
      <url>/2024/12/03/da-mo-xing-you-hua-pagedattention/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>基于KV Cache的大模型推理过程通常分为两个阶段：Prefill 和 Decoding。</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205200821.png"></p><p>在 Prefill 阶段，推理引擎将整段 prompt 输入模型进行前向计算。如果引入了 KV Cache 技术，prompt 经 Wk 和 Wv 计算得到的结果（即 K 和 V）会被存储到 K Cache 和 V Cache 中。这样，在后续 token 的 Attention 计算中，无需重复计算 K 和 V，从而显著节约计算时间。</p><p>进入 Decoding 阶段后，推理引擎会基于 Prefill 阶段的结果，逐步生成响应，每次输出一个 token。如果同样使用了 KV Cache，每生成一个 token，都会将其对应的 K 和 V 值存入缓存，加速后续推理。</p><p>下图展示了一个13B的模型在A100 40GB的gpu上做推理时的显存占用分配（others表示forward过程中产生的activation的大小，这些activation你可以认为是转瞬即逝的，即用完则废，因此它们占据的显存不大），从这张图中我们可以直观感受到推理中KV cache对显存的占用。因此，<strong>如何优化KV cache，节省显存，提高推理吞吐量，就成了LLM推理框架需要解决的重点问题。</strong><br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205201020.png"></p><h2 id="KV-Cache的常规存储分配"><a href="#KV-Cache的常规存储分配" class="headerlink" title="KV Cache的常规存储分配"></a>KV Cache的常规存储分配</h2><p>由于推理所生成的序列长度大小无法事先预知，所以大部分推理框架都会按 batch_size x max_seq_len 这样的固定尺寸来分配。在请求到来时，预先在显存中申请一块连续的区域。然而，这种“静态”显存分配策略，显存利用率是很低的。</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205201213.png"></p><p>我们假设 max_seq_len = 8，所以当第1条请求(prompt1)过来时，我们的推理框架为它安排了(1, 8)大小的连续存储空间。</p><p>当第2条请求（prompt2）过来时，同样也需要1块(1, 8)大小的存储空间。但此时prompt1所在的位置上，只剩3个空格子了，所以它只能另起一行做存储。对prompt3也是同理。</p><p>问题：</p><ul><li>浅色块：prefill阶段prompt的KV cache，是无论如何都会被使用的空间，它不存在浪费。</li><li>中色块：decode阶段的KV cache，其中<eos>表示序列生成的截止符。虽然这些中色块最终都会被我们用上，但是在decode阶段一个个token生成时，我们并不能预知哪些块会被最终用上，导致一种“潜在的浪费”， 称为<strong>预留碎片（reservation fragment）</strong>。</eos></li><li>深色块：decode阶段预留空间但是最终没有用上，称为<strong>内部碎片（internal fragment）</strong>。</li><li>灰色块：不是预留的KV cache的一部分，且最终也没有被用上，称这些灰色块为<strong>外部碎片（external fragment）</strong>。想象一下，此时新来了一条prompt4，它也要求显存中的8个格子作为KV cache。此时你的显存上明明有9个空格子，但因为它们是不连续的碎片，所以无法被prompt4所使用。这时prompt4的这条请求只好在队列中等待，直到gpu上有足够显存资源时再进行推理。</li></ul><p>观察整个KV cache排布，你会发现它们的毛病在于太过 <strong>“静态化”</strong>。当你无法预知序列大小时，你为什么一定要死板地为每个序列预留KV cache空间呢？为什么不能做得更动态化一些，即“用多少占多少”呢？这样我们就能减少上述这些存储碎片，使得每一时刻推理服务能处理的请求更多，提高吞吐量，这就是vLLM在做的核心事情，我们先通过一张实验图来感受下vLLM在显存利用上的改进效果（VS 其它推理框架）：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205202611.png"></p><p>不难发现，相比于别的推理框架，vLLM几乎能做到将显存完全打满。</p><h2 id="PagedAttention"><a href="#PagedAttention" class="headerlink" title="PagedAttention"></a>PagedAttention</h2><p>vLLM通过一种名为PagedAttention的技术，动态地为请求分配KV cache显存，提升显存利用率。</p><p>整体上来说，PagedAttention的设计灵感来自操作系统中虚拟内存的分页管理技术。</p><h3 id="操作系统的虚拟内存"><a href="#操作系统的虚拟内存" class="headerlink" title="操作系统的虚拟内存"></a>操作系统的虚拟内存</h3><p>虚拟内存分页是一种操作系统管理内存的技术，它将物理内存抽象为一个连续的虚拟地址空间，使每个进程都像拥有独立的内存。<br>分页通过将内存划分为<strong>固定大小的页（虚拟内存）和页框（物理内存）进行管理</strong>。页表记录虚拟页到物理页框的映射关系。<br>若虚拟页不在物理内存中，会发生页缺失（Page Fault），操作系统会从磁盘调入对应页。分页提高了内存利用率，并支持进程隔离与动态内存扩展。</p><h4 id="分段式内存管理"><a href="#分段式内存管理" class="headerlink" title="分段式内存管理"></a>分段式内存管理</h4><p>在分段式内存管理中，虚拟内存会尽量为每个进程在物理内存上找到一块连续的存储空间，让进程加载自己的全部代码、数据等内容。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205213618.png"></p><p>在这个例子中，3个进程的虚拟内存各自为它们在物理内存上映射了一块连续的存储空间。在某一时刻，我释放了进程2，同时想运行进程4。这时我尴尬地发现，虽然物理内存上有640M的空间剩余，但因为是碎片化的，我的进程4无法加载进去，因此它只能等待。</p><p>在这个情况下，如果我硬要运行进程4，也是有办法的：我可以先把进程3从物理内存上交换（swap）到磁盘上，然后把进程4装进来，然后再把进程3从磁盘上加载回来。通过这种方法我重新整合了碎片，让进程4能够运行。</p><p>但这种办法的显著缺点是：如果进程3过大，同时内存到磁盘的带宽又不够，整个交换的过程就会非常卡顿。这就是分段式内存管理的缺陷。</p><h4 id="分页式内存管理"><a href="#分页式内存管理" class="headerlink" title="分页式内存管理"></a>分页式内存管理</h4><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205213826.png"></p><p>虚拟内存的分页管理技术总结起来就是：</p><ul><li>将物理内存划分为固定大小的块，我们称每一块为页（page）。从物理内存中模拟出来的虚拟内存也按相同的方式做划分</li><li>对于1个进程，我们不需要静态加载它的全部代码、数据等内容。我们想用哪部分，或者它当前跑到哪部分，我们就动态加载这部分到虚拟内存上，然后由虚拟内存帮我们做物理内存的映射。</li><li>对于1个进程，虽然它在物理内存上的存储不连续（可能分布在不同的page中），但它在自己的虚拟内存上是连续的。通过模拟连续内存的方式，既解决了物理内存上的碎片问题，也方便了进程的开发和运行。</li></ul><h3 id="PagedAttention-工作流程"><a href="#PagedAttention-工作流程" class="headerlink" title="PagedAttention 工作流程"></a>PagedAttention 工作流程</h3><p>假设现在你向推理引擎发送请求，prompt 为 Four score and seven years ago our，你希望模型进行续写。PagedAttention 的运作流程如下<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241206002752.png"></p><h4 id="Prefill-阶段："><a href="#Prefill-阶段：" class="headerlink" title="Prefill 阶段："></a>Prefill 阶段：</h4><p><strong>划分逻辑块</strong>：vLLM 在接收到这条 prompt 后，会根据设定的块大小 B（本例中 B=4），将 prompt 划分为若干逻辑块（Logical KV Blocks）。由于该 prompt 包含 7 个 token，因此 vLLM 使用两个逻辑块（block 0 和 block 1）来存储它们的 KV 值。在逻辑块 1 中，目前仅存储了 “years”、”ago” 和 “hour” 这 3 个 token 的 KV 值，还有 1 个位置为空，称为保留位（Reservation）。</p><p><strong>划分物理块</strong>：完成逻辑块的划分后，这些逻辑块会映射到物理块，即实际存储 KV 值的空间。映射关系通过一张 block table（块表）记录，其主要内容包括：</p><ul><li><strong>逻辑块与物理块的映射关系</strong>（Physical Block Number）：例如，逻辑块 0 映射到物理块 7。</li><li><strong>物理块已填满的槽位数量</strong>（# Filled）：例如，在 Prefill 阶段，物理块 7 的 4 个槽位已填满，而物理块 1 的 4 个槽位中填满了 3 个。</li></ul><p>系统正常计算 prompt 的 KV 值后，根据上述划分关系，将这些值填入对应的物理块中。</p><h4 id="Decode-阶段："><a href="#Decode-阶段：" class="headerlink" title="Decode 阶段："></a>Decode 阶段：</h4><p>在使用 KV cache 计算 attention 并生成第一个词 “fathers” 时，不难发现，计算过程中使用的是<strong>逻辑块</strong>，即从形式上看，这些 token 是连续的。与此同时，vLLM 通过后台的 block table 映射关系，从对应的物理块中获取数据以完成实际计算。通过这种方式，每个请求都能认为自己是在一个连续且充足的存储空间上操作，尽管这些数据在物理存储上并不是连续的。</p><p>基于新生成的词，系统会对逻辑块、物理块和 block table 进行更新。例如，对于 block table，vLLM 将其 filled 字段从 3 更新为 4。</p><p>当 “fathers” 被写入后，当前逻辑块已装满，因此 vLLM 会开辟一个新的逻辑块（逻辑块 2），并同步更新 block table 和对应的物理块，以确保后续生成过程能够顺利进行。</p><h4 id="PagedAttention在不同解码策略下的运作"><a href="#PagedAttention在不同解码策略下的运作" class="headerlink" title="PagedAttention在不同解码策略下的运作"></a>PagedAttention在不同解码策略下的运作</h4><p>我们知道，根据实际需求，大模型的解码方式也比较复杂，例如：</p><ul><li><strong>Parallel Sampling</strong>：我给模型发送一个请求，希望它对prompt做续写，并给出三种不同的回答。我们管这个场景叫parallel sampling。在这个场景中，我们可以将prompt复制3次后拼接成1个batch喂给模型，让它做推理。但我们也需注意到，这种方式会产生prompt部分KV cache的重复存储。</li><li><strong>Beam Search 束搜索</strong>：这是LLM常用的deocde策略之一，即在每个decode阶段，我不是只产生1个token，而是产生top k个token（这里k也被称为束宽）。top k个token必然对应着此刻的top k个序列。我把这top k个序列喂给模型，假设词表的大小为|V|，那么在下一时刻，我就要在k*|V|个候选者中再选出top k，以此类推。不难想象每一时刻我把top k序列喂给模型时，它们的前置token中有大量的KV cache是重复的。</li><li><strong>Shared prefix</strong>：在某些大模型中，所有请求可能都会共享一个前置信息（比如system message: “假设你是一个有帮助的AI助手….”），这些前置信息没有必要重复存储KV cache</li><li><strong>其余一般场景</strong>：在一些更通用的场景中，虽然两个prompt可能完全没有关系，但它们中某些KV cache却是可以共用的。例如两个prompt的相同位置（position）恰好出现了完全一样的序列，比如它们的结尾都是好想下班。假设这个相同序列已经存在于KV cache中，那也没有必要重复计算和存储了。<br><strong>纠正： 对于部分序列部分相同的情况，应该是只能作用在layer1的KV cache上，而不能作用在后序的KV cache上。因为计算完layer1的attention后，后面的序列已经不重复了。</strong></li></ul><h5 id="Parallel-Sampling"><a href="#Parallel-Sampling" class="headerlink" title="Parallel Sampling"></a>Parallel Sampling</h5><p><strong>传统KV cache怎么做</strong>：<br>假设模型的max_seq_len = 2048。传统KV cache可能在显存中分配两块长度是2048的空间。由于prompt一致，这两块2048的空间中存在大量重复的KV cache。</p><p><strong>vLLM怎么做</strong>：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241206004803.png"><br>假定我们发给模型1个request，这个request中包含2个prompt/sample，记为Sample A1和Sample A2，这两个prompt完全一致，都为Four score and seven years ago our，我们希望模型对这两个prompt分别做续写任务。</p><ul><li>Prefill阶段：<ul><li>分配逻辑块：对于A1，vLLM为其分配逻辑块block0和block1；对于A2，vLLM为其分配逻辑块block0和block1。需要注意的是，A1的逻辑块和A2的逻辑块是独立的（尽管它们都叫block0和block1），你可以将A1和A2视作操作系统中两个独立运行的进程。</li><li>分配物理块：对于A1和A2，虽然逻辑块独立，但因为它们的文字完全相同，所以可以在物理内存上共享相同的空间。所以A1的逻辑块block0/1分别指向物理块block7/1；A2的逻辑块block0/1分别指向物理块block7/1。我们设每个物理块下映射的逻辑块数量为<strong>ref count</strong>，所以对物理块block7/1来说，它们的ref count都为2。</li></ul></li><li>decode阶段：A1和A2各自做推理，得到第一个token，分别为fathers和mothers。<ul><li>将生成的token装入逻辑块：对于A1和A2来说，将其生成的token装入各自的逻辑块block1。</li><li>触发物理块<strong>copy-on-write</strong>机制：由于fathers/mothers是两个完全不同的token，因此对物理块block1触发复制机制，即在物理内存上新开辟一块空间。此时物理块block1只和A2的逻辑块block1映射，将其ref count减去1；物理块block3只和A1的逻辑块block1映射，将其ref count设为1。</li></ul></li></ul><p>总结起来，vLLM节省KV cache显存的核心思想是，对于相同数据对应的KV cache，能复用则尽量复用；无法复用时，再考虑开辟新的物理空间。</p><h5 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h5><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241206005411.png"></p><p>我们从右往左来看这张图。虚线位置表示“当前decoding时刻”，beam width = 4。图中所有的block皆为逻辑块。</p><p>因为beam width = 4，这意味着根据beam search算法，在当前阶段我们生成了top 4个概率最大的token（我们记这4个token为beam candidate 0/1/2/3），它们分别装在block5，block6，block7和block8中。</p><p>现在我们继续使用beam search算法做decoding，继续找出top 4个最可能的next token。经过我们的计算，这top 4 next token，有2个来自beam candidate 1，有2个来自beam candidate 2。因此我们在block6中引出block9和block10，用于装其中两个top 2 next token；对block7也是同理。</p><p>现在，block9/10/11/12中装的top 4 next token，就成为新的beam candidates，可以按照和上述一样的方式继续做beam search算法。而对于block5和block8，它们已经在beam search的搜索算法中被淘汰了，后续生成的token也不会和它们产生关系，所以可以清除掉这两个逻辑块，并释放它们对应的物理块的内存空间。</p><p>好，我们继续往左边来看这幅图。block3引出block5/6/7，block4引出block8，这意味着当前这4个top4 token，是上一个timestep下candidate1和candidate3相关序列生成的（candidate0和2的block没有画出，是因为它们所在的序列被beam search算法淘汰了，因此没有画出的必要）。由于block8已经被淘汰，所以block4也相继被淘汰，并释放对应的物理内存空间。</p><p>由此往左一路推，直到block0为止（block0代表着prompt，因此被beam seach中所有的序列共享）。这一路上，我们都<strong>根据最新时刻的beam search decoding结果，释放掉不再被需要的逻辑块和对应的物理内存空间</strong>，达到节省显存的目的。</p><h3 id="调度和抢占"><a href="#调度和抢占" class="headerlink" title="调度和抢占"></a>调度和抢占</h3><p>到目前为止，我们已经解答了“vLLM 如何优化 KV cache 的显存分配”这一问题。接下来，我们将探讨另一个关键问题：当采用动态显存分配策略时，虽然表面上可以同时处理更多的 prompt，但由于没有为每个 prompt 预留充足的显存空间，<strong>如果某一时刻显存被完全占满，而所有正在运行的 prompt 都尚未完成推理，系统又该如何应对呢？</strong></p><p>vllm 的调度和抢占原则：</p><ul><li>采用“<strong>先到先服务</strong>”（FCFS）的调度策略来处理所有请求，确保公平性并防止请求饿死。</li><li>当 vLLM 需要抢占请求时，它会优先服务最早到达的请求，并优先抢占最新到达的请求。暂停它们的执行，同时将与之相关的 KV cache 从 gpu 上释放掉。等 gpu 资源充足时，重新恢复它们的执行。</li></ul><h4 id="Swapping交换策略"><a href="#Swapping交换策略" class="headerlink" title="Swapping交换策略"></a>Swapping交换策略</h4><p>对于被抢占的请求，vLLM要将其KV cache从gpu上释放掉，那么：</p><p><strong>问题1：该释放哪些KV cache？</strong><br>由前文PagedAttention原理可知，一个请求可能对应多个block。我们既可以选择释放掉部分block，也可以选择释放掉全部block，或者更科学地，我们可以预测一下哪些block被使用的频率最低，然后释放掉这些低频block（但这种方式实现起来难度较大，性价比不是很高）。</p><p>在vLLM中，采取的是all-or-nothing策略，即释放被抢占请求的所有block。</p><p><strong>问题2：要把这些KV cache释放到哪里去？</strong><br>对于这些被选中要释放的KV block，如果将它们直接丢掉，那未免过于浪费。vLLM采用的做法是将其从gpu上交换（Swap）到cpu上。这样等到gpu显存充份时，再把这些block从cpu上重载回来。</p><h4 id="Recomputing重计算策略"><a href="#Recomputing重计算策略" class="headerlink" title="Recomputing重计算策略"></a>Recomputing重计算策略</h4><p>知道了Swapping机制，重计算的过程也很好理解了：对于有些任务，当它们因为资源不足而被抢占时，可以不做swap，而是直接释放它们的物理块，把它们重新放入等待处理的队列中，等后续资源充足时再重新从prefill阶段开始做推理。</p><p>比如parallel sampling中并行采样数n=1的任务，对于这种小粒度任务，直接重计算（从 prefill 阶段重新开始）所需的成本可能比执行 swapping（迁移内存到 CPU）并在未来重新加载更低。Swapping 的过程会将内存页（或张量）从 GPU 转移到 CPU，这涉及到 PCIe 或 NVLink 通信，速度远低于 GPU 内部计算和内存操作。</p><h3 id="分布式管理"><a href="#分布式管理" class="headerlink" title="分布式管理"></a>分布式管理</h3><p>最后，我们一起来看一下vLLM的整体架构。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241206012807.png"></p><p>在分布式场景下，vLLM 的整体运作流程如下：</p><ul><li>中央调度器（Scheduler）：vLLM 配备了一个中央调度器，负责计算并管理每张显卡上 KV cache 从逻辑块到物理块的映射表（block tables）。</li><li>映射表广播：在进行分布式计算时，Scheduler 会将生成的映射表广播到各显卡。</li><li>缓存引擎管理：每张显卡上的 Cache Engine 在接收到对应的映射信息后，负责具体管理该显卡上的 KV block。</li></ul><p>上图中给出的例子，是用张量模型并行（megatron-lm）做分布式推理时的情况，所以图中每个worker上写的是model shard。<strong>在张量并行中，各卡上的输入数据相同，只是各卡负责计算不同head的KV cache</strong>。所以这种情况下，各卡上的逻辑块-物理块的映射关系其实是相同的（用的同一张block table），只是各卡上物理块中实际存储的数据不同而已。</p><p>参考资料：</p><blockquote><p>极智AI | 大模型优化技术PagedAttention：<a href="https://juejin.cn/post/7290163879287881765">https://juejin.cn/post/7290163879287881765</a></p></blockquote><blockquote><p>图解大模型计算加速系列之：vLLM核心技术PagedAttention原理：<a href="https://zhuanlan.zhihu.com/p/691038809">https://zhuanlan.zhihu.com/p/691038809</a></p></blockquote><blockquote><p>LLM推理优化 - PagedAttention：<a href="https://www.yidoo.xyz/paged-attention">https://www.yidoo.xyz/paged-attention</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> LLM </tag>
            
            <tag> PagedAttention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型优化--KV Cache</title>
      <link href="/2024/12/02/da-mo-xing-you-hua-kv-cache/"/>
      <url>/2024/12/02/da-mo-xing-you-hua-kv-cache/</url>
      
        <content type="html"><![CDATA[<h2 id="KV-Cache-介绍"><a href="#KV-Cache-介绍" class="headerlink" title="KV Cache 介绍"></a>KV Cache 介绍</h2><p>KV Cache是Transformer标配的推理加速功能，transformer官方use_cache这个参数默认是True，但是它只能用于Decoder架构的模型，这是因为Decoder有Causal Mask，在推理的时候前面已经生成的字符不需要与后面的字符产生attention，从而使得前面已经计算的K和V可以缓存起来。</p><p>我们先看一下不使用KV Cache的推理过程。假设模型最终生成了“遥遥领先”4个字。</p><p>当模型生成第一个“遥”字时，<code>input="&lt;s&gt;"</code>, <code>"&lt;s&gt;"</code>是起始字符。Attention的计算如下：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202155542.png"></p><p>为了看上去方便，我们暂时忽略scale项$1/\sqrt(d)$， 但是要注意这个scale面试时经常考。</p><p>如上图所示，最终Attention的计算公式如下，（softmaxed 表示已经按行进行了softmax）:<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202155748.png"></p><p>当模型生成第二个“遥”字时，<code>input="&lt;s&gt;遥"</code>, Attention的计算如下：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202155837.png"></p><p>当QK变为矩阵时，softmax 会针对 行 进行计算。写详细一点如下，softmaxed 表示已经按行进行了softmax。</p><p><strong>（关键）由于decoder架构的模型有Causal Mask，所以$Q_1$与$K_2$的计算结果为$-\infty$。</strong><br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202204856.png"></p><p>假设$Att_1$表示 Attention 的第一行， $Att_2$表示 Attention 的第二行，则根据上面推导，其计算公式为：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202205002.png"></p><p>我们发现：</p><ul><li>$Q_1$在第二步参与的计算与第一步是一样的，而且第二步生成的$Att_1$仅仅依赖于$Q_1$，与$Q_2$毫无关系。</li><li>$Att_2$仅仅依赖于$Q_2$，与$Q_1$毫无关系。</li></ul><p>当模型生成第三个“领”字时，<code>input="&lt;s&gt;遥遥"</code>, Attention的计算如下：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202210952.png"><br>详细的推导参考第二步，其计算公式为：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202211018.png"><br>同样的，第三步生成的$Att_3$仅仅依赖于$Q_3$，与$Q_1$和$Q_2$毫无关系。</p><p>当模型生成第四个“先”字时，<code>input="&lt;s&gt;遥遥领"</code>, Attention的计算如下：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202211156.png"><br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202211222.png"><br>和之前类似，不再赘述。</p><p>看上面图和公式，我们可以得出结论：</p><ul><li>当前计算方式存在大量冗余计算</li><li>$Attn_k$只与$Q_k$有关</li><li>推理第$x_k$个字符时，只需要输入字符$x_{k-1}$即可。<br>第三个结论的前提是，我们需要把每一步的K和V缓存起来，这样在推理第$x_k$个字符时，只需要输入字符$x_{k-1}$计算其$Q_k,K_k,V_k$, 结合之前保存的KV Cache即可得到对应的$Attn_k$。</li></ul><p>下图展示了使用KV Cache和不使用KV Cache的过程对比：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202211901.png"></p><p><a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L318C1-L331C97">huggingface 的实现</a></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">if layer_past is not None:        past_key, past_value = layer_past        key = torch.cat((past_key, key), dim=-2)        value = torch.cat((past_value, value), dim=-2)        if use_cache is True:        present = (key, value)    else:        present = None        if self.reorder_and_upcast_attn:        attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)    else:        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)</code></pre><h2 id="KV-Cache-步骤"><a href="#KV-Cache-步骤" class="headerlink" title="KV Cache 步骤"></a>KV Cache 步骤</h2><p>正是因为 Self Attention 中带 Maske ，因此，在推理的时候，前面已经生成的 token 不需要与后面的 token 计算 Attention ，从而使得前面已经计算的 K 和 V 可以缓存起来。<br>一个典型的带有 KV Cache 的推理过程包含以下两个阶段：</p><ol><li>预填充阶段：输入一个 prompt 序列，为每个 transformer 层生成 Key Cache 和 Value Cache（KV cache）。</li><li>解码阶段：使用并更新 KV Cache，一个接一个地生成 token，当前生成的 token 依赖于之前已经生成的token。</li></ol><h3 id="预填充阶段"><a href="#预填充阶段" class="headerlink" title="预填充阶段"></a>预填充阶段</h3><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241204023238.png"></p><h3 id="解码阶段"><a href="#解码阶段" class="headerlink" title="解码阶段"></a>解码阶段</h3><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241204023814.png"></p><p>KV Cache采用动态分配缓冲区大小，当超过当前容量时，内存大小会翻倍。这种方法虽然可行，但在 GPU 上频繁申请和释放内存的开销较大，导致效率较低。目前流行的解决办法是数据拆分与元数据管理：将数据按最小单元存储，并使用元数据记录每一块数据的位置，称为 PageAttention。程序在初始化时申请一块较大的显存区域（例如 4GB），然后按照 KVCache 的大小将显存划分成多个小块，并记录每个 token 在推理过程中需要访问的小块。显存的分配、释放和管理类似于操作系统对物理内存的虚拟化过程。这一思路被 vLLM（具体参见论文 Efficient Memory Management for Large Language Model Serving with PagedAttention）所采用，并广泛应用于大规模语言模型的推理中。</p><h2 id="MQA与GQA"><a href="#MQA与GQA" class="headerlink" title="MQA与GQA"></a>MQA与GQA</h2><p>在GPU上部署模型时，我们遵循的原则是：能在一张卡上部署的，就不要跨多张卡；能在一台机器上部署的，就不要跨多台机器。这是因为“卡内通信带宽 &gt; 卡间通信带宽 &gt; 机间通信带宽”。由于“木桶效应”，模型部署时跨的设备越多，受到设备间通信带宽的制约就越大。</p><p>因此，减少 KV Cache 的目的是为了在更少的设备上推理更长的 Context，或者在相同的 Context 长度下实现更大的推理 batch size，从而提升推理速度或增加吞吐总量。最终目的都是为了降低推理成本。</p><h3 id="MHA"><a href="#MHA" class="headerlink" title="MHA"></a>MHA</h3><p>MHA（Multi-Head Attention），也就是多头注意力，是 Transformer 中的标准 Attention 形式。在数学上，多头注意力 MHA 等价于多个独立的单头注意力的拼接。其遵循前面所讲的KV Cache 的原理。而后面的 MQA、GQA、都是围绕“如何减少 KV Cache 同时尽可能地保证效果”这个主题发展而来的产物。</p><h3 id="MQA"><a href="#MQA" class="headerlink" title="MQA"></a>MQA</h3><p>MQA，即“Multi-Query Attention”，2019年由 Google 在论文 <a href="chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/1911.02150">Fast Transformer Decoding: One Write-Head is All You Need</a> 中提出。</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241204024825.png"><br>使用 MQA 的模型包括 PaLM、StarCoder、Gemini 等。很明显，MQA 直接将 KV Cache 减少到了原来的 1/head_num。</p><p>效果方面，目前看来大部分任务的损失都比较有限。</p><h3 id="GQA"><a href="#GQA" class="headerlink" title="GQA"></a>GQA</h3><p>也有人担心 MQA 对 KV Cache 的压缩太严重，以至于会影响模型的学习效率以及最终效果。为此，一个 MHA 与 MQA 之间的过渡版本 GQA（Grouped-Query Attention）应运而生，出自 2023 年 Google 的论文 <a href="https://arxiv.org/abs/2305.13245">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints</a>。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241203015549.png"></p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241204025114.png"></p><table><thead><tr><th>模型</th><th>参数量</th><th>非Embedding参数量</th><th>GQA</th><th>上下文长度</th></tr></thead><tbody><tr><td>Qwen2-0.5B</td><td>0.49B</td><td>0.35B</td><td>√</td><td>32K</td></tr><tr><td>Qwen2-1.5B</td><td>1.54B</td><td>1.31B</td><td>√</td><td>32K</td></tr><tr><td>Qwen2-7B</td><td>7.07B</td><td>5.98B</td><td>√</td><td>128K</td></tr><tr><td>Qwen2-57B-A14B</td><td>57.41B</td><td>56.32B</td><td>√</td><td>64K</td></tr><tr><td>Qwen2-72B</td><td>72.71B</td><td>70.21B</td><td>√</td><td>128K</td></tr></tbody></table><p>在 Llama 2/3-70B 中，GQA 的 g=8 ，其他用了 GQA 的同体量模型基本上也保持了这个设置，这并非偶然，而是同样出于推理效率的考虑。我们知道，70B 这个体量的模型，如果不进行极端的量化，那么不可能部署到单卡（A100/H100 80G）上。单卡不行，那么就能单机了，一般情况下一台机可以装 8 张卡，Attention 的每个Head 实际上是独立运算然后拼接起来的，当 g=8 时，正好可以每张卡负责计算一组 K、V 对应的 Attention Head，这样可以在尽可能保证 K、V 多样性的同时最大程度上减少卡间通信。</p><p>下面看一下 GQA 的实验效果。</p><table><thead><tr><th>模型</th><th>推理时间</th><th>效果</th></tr></thead><tbody><tr><td>MHA-Large</td><td>0.37</td><td>46.0</td></tr><tr><td>MHA-XXL</td><td>1.51</td><td>47.2</td></tr><tr><td>MQA-XXL</td><td>0.24</td><td>46.6</td></tr><tr><td>GQA-8-XXL</td><td>0.28</td><td>47.1</td></tr></tbody></table><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241204025823.png"></p><p>参考：</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/708120479">https://zhuanlan.zhihu.com/p/708120479</a></p></blockquote><blockquote><p>大模型推理加速：看图学KV Cache <a href="https://zhuanlan.zhihu.com/p/662498827">https://zhuanlan.zhihu.com/p/662498827</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> KV Cache </tag>
            
            <tag> LLM </tag>
            
            <tag> MQA </tag>
            
            <tag> GQA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型显存占用分析</title>
      <link href="/2024/11/30/da-mo-xing-xian-cun-zhan-yong-fen-xi/"/>
      <url>/2024/11/30/da-mo-xing-xian-cun-zhan-yong-fen-xi/</url>
      
        <content type="html"><![CDATA[<h1 id="大模型消耗的显存"><a href="#大模型消耗的显存" class="headerlink" title="大模型消耗的显存"></a>大模型消耗的显存</h1><p>在详细说明大模型需要消耗的显存大小之前我们需要先明确几个概念。<br>一个就是大模型在不同阶段对显存的消耗是不同的。但是大致可以分为三个阶段或者说三个场景。即大模型<strong>预训练阶段</strong>、大模型<strong>微调阶段</strong>和大模型<strong>推理阶段</strong>。</p><ul><li>在<strong>预训练阶段</strong>，大模型通常选择较大规模的数据集获取泛化能力，因此需要较大的批次等来保证模型的训练强大。而模型的权重也是从头开始计算，因此通常也会选择高精度（如32位浮点数）进行训练。需要消耗大量的GPU显存资源。</li><li>在<strong>微调阶段</strong>，通常会冻结大部分参数，只训练小部分参数。同时，也会选择非常多的优化技术和较少的高质量数据集来提高微调效果，此时，由于模型已经在预训练阶段进行了大量的训练，微调时的数值误差对模型的影响通常较小。也常常选择16位精度或者混合精度训练。因此通常比预训练阶段消耗更低的显存资源。</li><li>在<strong>推理阶段</strong>，通常只是将一个输入数据经过模型的前向计算得到结果即可，因此需要最少的显存即可运行。</li></ul><h3 id="模型权重"><a href="#模型权重" class="headerlink" title="模型权重"></a>模型权重</h3><p>这部分显存用于存储神经网络模型的参数，包括权重（weights）和偏置（biases）。模型内存是模型在训练和推理过程中都需要的，因为它包含了模型的结构和学习到的知识。在训练过程中，模型内存的大小通常与模型的复杂度和参数数量成正比。</p><h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><p>在模型训练反向传播（Backward）过程中，计算的梯度所占的显存大小。梯度内存的大小与模型的参数数量有关，因为每个参数都需要计算对应的梯度。</p><h3 id="优化器状态"><a href="#优化器状态" class="headerlink" title="优化器状态"></a>优化器状态</h3><p>优化器内存用于存储优化器状态，这通常包括梯度的一阶和二阶矩（如在Adam优化器中使用的均值和方差估计）优化器内存的大小取决于所使用的优化器类型。例如，Adam优化器需要额外的内存来存储梯度的一阶和二阶矩，而SGD只需要存储梯度信息，无其他优化器内存占用。</p><h3 id="激活值"><a href="#激活值" class="headerlink" title="激活值"></a>激活值</h3><p>激活内存用于存储神经网络在前向传播过程中计算的中间激活值。这些激活值在反向传播过程中需要被重用，以计算关于模型参数的梯度。激活内存的大小与网络的深度和输入数据大小（batch size）有关。更深的网络和更大的 batch size 会导致更大的激活内存需求。</p><h1 id="数据精度"><a href="#数据精度" class="headerlink" title="数据精度"></a>数据精度</h1><p>想要计算显存，从“原子”层面来看，就需要知道我们的使用数据的精度，因为精度代表了数据存储的方式，决定了一个数据占多少bit。对于一个1B参数的模型，如果使用FP32精度存储，那么模型权重占用的显存就是1B * 2 = 2GB。</p><h3 id="常见精度类型"><a href="#常见精度类型" class="headerlink" title="常见精度类型"></a>常见精度类型</h3><p>浮点数主要是由符号位（sign）、指数位（exponent）和小数位（mantissa）三部分组成。 符号位都是1位（0表示正，1表示负），指数位影响浮点数范围，小数位影响精度。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202035151.png"></p><ul><li>FP32：32位浮点数，每个数据占4字节</li><li>TF32：<strong>19位浮点数</strong>，每个数据占2字节</li><li>FP16：16位浮点数，每个数据占2字节</li><li>BF16：16位浮点数，每个数据占2字节</li><li>Int8：8位整数，每个数据占1字节</li><li>Int4：4位整数，每个数据占0.5字节</li></ul><h3 id="混合精度训练AMP"><a href="#混合精度训练AMP" class="headerlink" title="混合精度训练AMP"></a>混合精度训练AMP</h3><p>较低模型精度对于运算效率和显存占用都更友好，但是如果直接使用FP16精度在训练过程中会出现很多问题：</p><ul><li>underflow：梯度再乘以学习率会很小，无法用fp16表示</li><li>rounding error：fp16各个区间之间存在gap，即使梯度可以用fp16表示，但是也没有把法加在fp16的权重上（被舍去）</li><li>模型预测准确度降低</li></ul><h4 id="FP32权重备份：解决舍入误差问题"><a href="#FP32权重备份：解决舍入误差问题" class="headerlink" title="FP32权重备份：解决舍入误差问题"></a>FP32权重备份：解决舍入误差问题</h4><p>保留一份FP32的主权重（Master-Weights），同时在训练中使用FP16存储权重、激活、梯度等数据。在参数更新的过程汇总，用FP16更新FP32的主权重。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202040432.png"></p><p>Step1:优化器会先备份一份FP32精度的模型权重，初始化好FP32精度的一阶和二阶动量（用于更新权重）。</p><p>Step2:开辟一块新的存储空间，将FP32精度的模型权重转换为FP16精度的模型权重。</p><p>Step3:运行forward和backward，产生的梯度和激活值都用FP16精度存储。</p><p>Step4:优化器利用FP16的梯度和FP32精度的一阶和二阶动量去更新备份的FP32的模型权重。</p><p>Step5:重复Step2到Step4训练，直到模型收敛。</p><p>我们可以看到训练过程中显存主要被用在四个模块上：</p><ul><li>模型权重本身（FP32+FP16）</li><li>梯度（FP16）</li><li>优化器（FP32）</li><li>激活值（FP16）</li></ul><p>写到这里，我们应该对于分析大模型训练时候的显存问题应该不在话下了（除了动态部分），那么我们就来实测一下，正在阅读的小伙伴也可以先自己尝试计算一下，看看是不是真的懂了。 对于llama3.1 8B模型，FP32和BF16混合精度训练，用的是AdamW优化器，请问模型训练时占用显存大概为多少？</p><p>解：</p><p>模型参数：16（BF16） + 32（PF32）= 48G</p><p>梯度参数：16（BF16）= 16G</p><p>优化器参数：32（PF32） + 32（PF32）= 64G</p><p>不考虑激活值的情况下，总显存大约占用 （48 + 16 + 64） = 128G</p><h4 id="损失缩放：解决数据下溢问题"><a href="#损失缩放：解决数据下溢问题" class="headerlink" title="损失缩放：解决数据下溢问题"></a>损失缩放：解决数据下溢问题</h4><p>当采用FP16而不是FP32更新梯度时，由于值太小，会造成FP16精度下数据下溢的问题，一些梯度会变为0，导致模型不收敛。故采用在前向过程结束后对损失进行放大，在反向过程结束后对梯度进行缩小。<br>损失缩放可以有两种主要方式：静态损失缩放和动态损失缩放。</p><ul><li>静态损失缩放：在训练开始前，设置一个固定的缩放因子，在训练过程中保持不变。</li><li>动态损失缩放：在训练过程中，根据损失值的大小动态调整缩放因子。<ul><li>如果在某轮训练中检测到梯度正常且没有溢出，缩放因子会逐渐增大。</li><li>如果检测到梯度出现 NaN 或 Inf，则缩放因子减小以防止数值不稳定。</li></ul></li></ul><h4 id="精度累加"><a href="#精度累加" class="headerlink" title="精度累加"></a>精度累加</h4><p>此外，研究者还发现，可以在模型训练的过程中，使用FP16进行乘法预算，使用FP32进行累加运算，并将FP32转换为FP16存储。FP32可以弥补损失的精度，减少舍入误差。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202042004.png"><br>如英伟达Volta架构中的Tensor Core可以使用FP16混合精度进行加速，采用的是FP16的矩阵乘法，得出全精度乘积，然后使用FP32累加，将该乘积与其他中间乘积累加，减少因FP16带来的精度损失。</p><h4 id="更为动态的精度缩放方法"><a href="#更为动态的精度缩放方法" class="headerlink" title="更为动态的精度缩放方法"></a>更为动态的精度缩放方法</h4><p>在英伟达最新的Hopper架构GPU中，英伟达的Tensor Core能够自动根据所需的精度进行动态的数据缩放调整，特别是针对Transformer网络架构，能够在数据存入内存前，根据需求改变各种参数精度。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202042340.png"><br>Hopper白皮书内容如下：</p><blockquote><p>在 Transformer 模型的每一层，Transformer Engine 都会分析 Tensor Core 产生的输出值的统计数据。了解了接下来会出现哪种类型的神经网络层以及它需要什么精度后，Transformer Engine 还会决定将张量转换为哪种目标格式，然后再将其存储到内存中。 FP8 的范围比其他数字格式更有限。为了优化使用可用范围，Transformer Engine 还使用从张量统计数据计算的缩放因子动态地将张量数据缩放到可表示的范围内。因此，每一层都在其所需的范围内运行，并以最佳方式加速。</p></blockquote><h1 id="其他显存占用"><a href="#其他显存占用" class="headerlink" title="其他显存占用"></a>其他显存占用</h1><ul><li>KV Cache：在推理过程中，大模型需要缓存一些中间结果，以便在处理下一个输入时重用。这些缓存的结果通常称为KV Cache。KV Cache占用的显存大小与模型的层数、序列长度和每个序列的token数量有关。</li><li>显存碎片：显存碎片是指显存中未被使用的空闲空间，这些空闲空间可能无法被有效利用，导致显存利用率降低。paged attention机制可以有效减少显存碎片。</li></ul><h3 id="推理与KV-cache-显存"><a href="#推理与KV-cache-显存" class="headerlink" title="推理与KV cache 显存"></a>推理与KV cache 显存</h3><p>推理的时候，显存几乎只考虑模型参数本身，除此之外就是现在广泛使用的KV cache也会占用显存。KV cache与之前讲的如何减少显存不一样，KV cache的目的是减少延迟，也就是<strong>为了推理的速度牺牲显存</strong>。</p><h4 id="kv-cache介绍"><a href="#kv-cache介绍" class="headerlink" title="kv cache介绍"></a>kv cache介绍</h4><p>具体可以参考另一篇博客：<a href="https://baoblei.github.io/2024/12/02/da-mo-xing-you-hua-kv-cache/">大模型优化–KV Cache</a><br>KV Cache是Transformer标配的推理加速功能，transformer官方use_cache这个参数默认是True，但是它只能用于Decoder架构的模型，这是因为Decoder有Causal Mask，在推理的时候前面已经生成的字符不需要与后面的字符产生attention，从而使得前面已经计算的K和V可以缓存起来。</p><p>下图展示了使用KV Cache和不使用KV Cache的过程对比：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202211901.png"><br>从图中，我们可以得出结论：</p><ul><li>当前计算方式存在大量冗余计算</li><li>$Attn_k$只与$Q_k$有关</li><li>推理第$x_k$个字符时，只需要输入字符$x_{k-1}$即可。<br>第三个结论的前提是，我们需要把每一步的K和V缓存起来，这样在推理第$x_k$个字符时，只需要输入字符$x_{k-1}$计算其$Q_k,K_k,V_k$, 结合之前保存的KV Cache即可得到对应的$Attn_k$。</li></ul><p><a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L318C1-L331C97">huggingface 的实现</a></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">if layer_past is not None:        past_key, past_value = layer_past        key = torch.cat((past_key, key), dim=-2)        value = torch.cat((past_value, value), dim=-2)        if use_cache is True:        present = (key, value)    else:        present = None        if self.reorder_and_upcast_attn:        attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)    else:        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)</code></pre><h4 id="KV-Cache显存占用"><a href="#KV-Cache显存占用" class="headerlink" title="KV Cache显存占用"></a>KV Cache显存占用</h4><p>当<strong>sequence特别长</strong>的时候，KV Cache其实还是个Memory刺客。</p><p>对于fp16精度保存的KV Cache，其占用的显存大小为：<br>$$<br>memory = batch_size * hidden_size * seq_length * layer * 2 * 2<br>$$<br>其中两个2分别表示K和V，fp16精度字节数。</p><p>比如llama 7B模型，batch_size=32, layer=32, dim_size=4096, seq_length=2048, float32类型，则需要占用的显存为 2 * 32 * 4096 * 2048 * 32 * 4 / 1024/1024/1024 = 64G。</p><p>为了解决KV Cache显存占用问题，研究者提出了<strong>MQA和GQA</strong>。其核心思想是：<strong>共享多头KV Cache</strong>。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241203015549.png"><br>以GQA为例，我们将hidden_size维度切分为head*head_dim，然后将多个head分成group组，每个group共享一个KV。则总的KV Cache显存占用为：<br>$$<br>memory = batch_size * group * head_dim * seq_length * layer *  2 * 2<br>$$<br>而MQA则是group=1，即每个head单独保存一个KV。</p><blockquote><p>大模型推理加速：看图学KV Cache <a href="https://zhuanlan.zhihu.com/p/662498827">https://zhuanlan.zhihu.com/p/662498827</a></p></blockquote><h3 id="LoRA-与-QLoRA-训练显存"><a href="#LoRA-与-QLoRA-训练显存" class="headerlink" title="LoRA 与 QLoRA 训练显存"></a>LoRA 与 QLoRA 训练显存</h3><h4 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h4><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241204190635.png"><br>LoRA是在原来的权重矩阵的旁路新建一对低秩的可训练权重，训练的时候只训练旁路，大大降低了训练的权重数量，参数量从 dxd 降为 2xdxr。</p><p>有了前面的全参情况下训练的显存分析，现在分析起来就比较通顺了，我们一步一步来，还是以BF16半精度模型Adamw优化器训练为例子，lora部分的参数精度也是BF16，并且设1字节模型参数对应的显存大小 $\Phi$。</p><ul><li>首先是模型权重本身的权重，这个肯定是要加载原始模型和lora旁路模型的，因为lora部分占比小于2个数量级，所以显存分析的时候忽略不计，显存占用 $2\Phi$。</li><li>然后就是优化器部分，优化器也不需要对原模型进行备份了，因为优化器是针对于需要更新参数的模型权重部分进行处理，也就是说优化器只包含Lora模型权重相关的内容，考虑到数量级太小，也忽略不计，故优化器部分占用显存 0。</li><li>原始模型都不更新梯度，肯定只需要Lora部分的梯度显存，而这部分占用显存也可以近似为 0。<br>想深入探究的可以去看了一篇<a href="https://baoblei.github.io/2024/12/04/da-mo-xing-gao-xiao-wei-diao-fang-fa-peft-lora-qlora/">博文</a>和<a href="https://zhuanlan.zhihu.com/p/702629428">大模型高效微调-LoRA原理详解和训练过程深入分析</a>。</li></ul><p>总的来说，不考虑激活值的情况下，Lora微调训练的显存占用只有$2\Phi$，一个7B的模型Lora训练只需要占用显存大约14G左右。验证一下，我们来看Llama Factory里给出训练任务的显存预估表格：<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205031352.png"></p><h4 id="QLoRA"><a href="#QLoRA" class="headerlink" title="QLoRA"></a>QLoRA</h4><p>QLoRA本质上还是对模型的主体进行了量化，以4Bit量化为例，Qlora占用的显存主要就是4Bit量化后的模型本身也就是$0.5\Phi$，由于A、B矩阵的参数量很小，故忽略不计。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><table><thead><tr><th>部分显存对应精度（训练）</th><th>全参微调（全FP16）</th><th>全参微调（BF16混合精度）</th><th>LoRA</th><th>QLoRA</th></tr></thead><tbody><tr><td>主干模型（模型存储/计算参数）</td><td>FP16/FP16</td><td>BF16/BF16</td><td>BF16/BF16</td><td>NF4/BF16</td></tr><tr><td>主干模型（梯度）</td><td>FP16</td><td>BF16</td><td>Null</td><td>Null</td></tr><tr><td>主干模型（adamw优化器）</td><td>2 x FP16</td><td>3 x FP32</td><td>Null</td><td>Null</td></tr><tr><td>LoRA部分（可忽略不计）</td><td>Null</td><td>Null</td><td>BF16</td><td>BF16</td></tr><tr><td>总和（大约）</td><td>8Byte</td><td>16Byte</td><td>2Byte</td><td>0.5Byte</td></tr></tbody></table><h1 id="huggingface-显存分析工具"><a href="#huggingface-显存分析工具" class="headerlink" title="huggingface 显存分析工具"></a>huggingface 显存分析工具</h1><p>huggingface 提供了一个工具可以方便的查看大模型在不同阶段消耗的显存大小。<br><a href="https://huggingface.co/docs/accelerate/usage_guides/model_size_estimator">model size estimator</a></p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><blockquote><p><a href="https://blog.zhexuan.org/archives/llm-gpu-memory.html">https://blog.zhexuan.org/archives/llm-gpu-memory.html</a></p></blockquote><blockquote><p><a href="https://juejin.cn/post/7352387675837480995">https://juejin.cn/post/7352387675837480995</a></p></blockquote><blockquote><p><a href="https://gitcode.csdn.net/662a062ca2b051225566cf63.html">https://gitcode.csdn.net/662a062ca2b051225566cf63.html</a></p></blockquote><blockquote><p><a href="https://hub.baai.ac.cn/view/16045">https://hub.baai.ac.cn/view/16045</a></p></blockquote><blockquote><p><a href="https://zhuanlan.zhihu.com/p/624740065">https://zhuanlan.zhihu.com/p/624740065</a></p></blockquote><blockquote><p>NVIDIA H100 Tensor Core GPU Architecture：<a href="https://nvdam.widen.net/s/9bz6dw7dqr/gtc22-whitepaper-hopper">https://nvdam.widen.net/s/9bz6dw7dqr/gtc22-whitepaper-hopper</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLMs </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> LoRA </tag>
            
            <tag> AI </tag>
            
            <tag> KV Cache </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo博客搭建</title>
      <link href="/2024/11/28/hexo-zhu-ti-next-pei-zhi/"/>
      <url>/2024/11/28/hexo-zhu-ti-next-pei-zhi/</url>
      
        <content type="html"><![CDATA[<h1 id="基础搭建"><a href="#基础搭建" class="headerlink" title="基础搭建"></a>基础搭建</h1><h2 id="Hexo"><a href="#Hexo" class="headerlink" title="Hexo"></a>Hexo</h2><p>选一个博客框架，hexo是静态网站框架，基于nodejs，可以生成静态网页，部署到github上。<br>需要提前安装<strong>git</strong>，<strong>nodejs</strong>。</p><ul><li>check git version: <code>git --version</code></li><li>check nodejs version: <code>node -v</code></li><li>check npm version: <code>npm -v</code><br>安装 cnmp (optional) </li><li><code>npm install -g cnpm --registry=https://registry.npm.taobao.org</code></li><li>check cnmp version: <code>cnpm -v</code></li></ul><p>安装 <strong>hexo-cli</strong></p><ul><li><code>cnpm install -g hexo-cli</code></li><li>check hexo version: <code>hexo -v</code></li></ul><h2 id="初始化blog目录"><a href="#初始化blog目录" class="headerlink" title="初始化blog目录"></a>初始化blog目录</h2><ul><li><code>mkdir blog | cd blog</code></li><li><code>sudo hexo init</code>    # clone hexo-starter repo</li><li><code>hexo s</code>            # start blog, default port 4000</li></ul><h2 id="添加博客"><a href="#添加博客" class="headerlink" title="添加博客"></a>添加博客</h2><ul><li><code>hexo n "newblog"</code>  # add new blog, save in /source/_posts<ul><li>edit blog in /source/_posts/newblog.md</li></ul></li><li><code>hexo clean</code>        # clean cache</li><li><code>hexo g</code>            # generate static files</li><li><code>hexo s</code>            # start blog, default port 4000</li></ul><h2 id="github部署"><a href="#github部署" class="headerlink" title="github部署"></a>github部署</h2><ul><li>build blog address:<ul><li>add new token in github</li><li>github new repo: https://<username>.github.io/</username></li></ul></li><li><code>cnpm install hexo-deployer-git --save</code>  <pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml">deploy:    type: git    repo: https://&lt;token&gt;@github.com/&lt;username&gt;/&lt;username&gt;.github.io.git    branch: master</code></pre></li><li><code>hexo d</code>            # deploy blog</li></ul><h1 id="主题"><a href="#主题" class="headerlink" title="主题"></a>主题</h1><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><ul><li>theme 推荐：<ul><li><a href="https://github.com/litten/hexo-theme-yilia.git">https://github.com/litten/hexo-theme-yilia.git</a></li><li><a href="https://github.com/theme-next/hexo-theme-next.git">https://github.com/theme-next/hexo-theme-next.git</a></li></ul></li><li>clone theme into themes folder, e.g. <code>git clone https://github.com/theme-next/hexo-theme-next.git themes/next</code></li><li>edit _config.yml, set theme to next  <pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml">theme: next</code></pre></li><li><code>hexo c</code>&amp;&amp;<code>hexo g</code>&amp;&amp;<code>hexo d</code>            # clean cache, generate static files, deploy blog</li></ul><h2 id="配置主题"><a href="#配置主题" class="headerlink" title="配置主题"></a>配置主题</h2><p>大部分配置在<code>_config.yml</code> 文件中，可以参考<a href="https://theme-next.js.org/docs/getting-started/">官方文档</a></p><h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><h4 id="scheme"><a href="#scheme" class="headerlink" title="scheme"></a>scheme</h4><ul><li>Scheme 是 NexT 提供的一种特性，借助于 Scheme，NexT 为你提供多种不同的外观。同时，几乎所有的配置都可以 在 Scheme 之间共用。</li><li>muse - 默认 Scheme，这是 NexT 最初的版本，黑白主调，大量留白</li><li>Mist - Muse 的紧凑版本，整洁有序的单栏外观</li><li>Pisces - 双栏 Scheme，小家碧玉似的清新<br>Scheme 的切换通过更改 <code>_config.yml</code> 文件，搜索 scheme 关键字。 你会看到有三行 scheme 的配置，将你需用启用的 scheme 前面注释 # 去除即可。</li></ul><h4 id="language"><a href="#language" class="headerlink" title="language"></a>language</h4><h4 id="menu"><a href="#menu" class="headerlink" title="menu"></a>menu</h4><p>菜单配置包括三个部分，第一是菜单项（名称和链接），第二是菜单项的显示文本，第三是菜单项对应的图标。 NexT 使用的是 Font Awesome 提供的图标，可以在 <a href="https://fontawesome.com/icons">Font Awesome</a> 查看。</p><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml">menu:  home: / || home  archives: /archives || archive  tags: /tags || tags  categories: /categories || th</code></pre><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>主题美化是个逐渐积累的过程, 后期可以在相册、挂件、评论、搜索、sitemap、rss等方面进行配置。<br>一些工具插件：<br><strong>gallery page</strong>–justified gallery</p>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> next </tag>
            
            <tag> 个人静态博客搭建 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
