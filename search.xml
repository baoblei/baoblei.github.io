<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>大模型动力引擎-单卡性能优化专题</title>
      <link href="/2025/03/12/da-mo-xing-dong-li-yin-qing-dan-qia-xing-neng-you-hua-zhuan-ti/"/>
      <url>/2025/03/12/da-mo-xing-dong-li-yin-qing-dan-qia-xing-neng-you-hua-zhuan-ti/</url>
      
        <content type="html"><![CDATA[<p>在单卡GPU训练环境中，性能问题主要分为四类： -GPU被阻塞：这是由于数据预处理或传输任务等前置依赖未完成，导致GPU计算资源空闲等待的情况。-GPU运行效率不高：这通常是因为GPU上的计算任务设计得不够好，未能充分发挥硬件的计算能力。-不必要的GPU与CPU间同步：GPU与CPU之间的同步是一个极其费时的操作，用户有时会在无意中频繁使用同步操作，进而严重降低性能。-数据传输带宽不足：在GPU与CPU之间传输数据时，如果带宽不足，就会导致数据传输速度变慢，进而影响训练性能。</p><h2 id="提高数据任务的并行度">提高数据任务的并行度</h2><ul><li>CPU的预处理要足够快，能够及时给GPU提交计算任务，确保GPU上有大量计算任务在排队，始终有活可以干。</li><li>GPU任务需要的数据总能够在它开始执行前就传输到显存。这样可以尽量减少GPU的空闲时间，让其始终保持在计算状态。</li></ul><h3 id="增加数据预处理的并行度">增加数据预处理的并行度</h3><p>通过增加DataLoader的num_workers参数，可以增加数据预处理的并行度。</p><p>案例： </p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchfrom torch import nnfrom torch.profiler import profile, ProfilerActivityimport torchvision.transforms as transformsfrom torchvision.datasets import CIFAR10from torch.utils.data import DataLoaderclass SimpleNet(nn.Module):    def __init__(self):        super(SimpleNet, self).__init__()        self.fc1 = nn.Linear(512, 10000)        self.fc2 = nn.Linear(10000, 1000)        self.fc3 = nn.Linear(1000, 10)    def forward(self, x):        out = self.fc1(x)        out = self.fc2(out)        out = self.fc3(out)        return outassert torch.cuda.is_available()device = torch.device("cuda")model = SimpleNet().to(device)optimizer = torch.optim.SGD(model.parameters(), lr=0.01)def train(model, optimizer, trainloader, num_iters):    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:        for i, batch in enumerate(trainloader, 0):            if i &gt;= num_iters:                break            #------同步模式-------------------------#            data = batch[0].cuda()            #------非阻塞模式------------------------#            # data = batch[0].to("cuda", non_blocking=True)            # 前向            optimizer.zero_grad()            output = model(data)            loss = output.sum()            # 反向            loss.backward()            optimizer.step()    prof.export_chrome_trace(f"traces/PROF_workers_{trainloader.num_workers}.json")#-----------提高数据预处理并行度---------------#num_workers = 0#-------------------------------------------#transform = transforms.Compose(    [transforms.ToTensor(), transforms.Resize([512, 512])])trainset = CIFAR10(root="./data", train=True, download=True, transform=transform)#----------默认在页内存中加载数据---------------#trainloader = DataLoader(trainset, batch_size=32, num_workers=num_workers)#------------在锁页内存中加载数据---------------##trainloader = DataLoader(trainset, batch_size=32, num_workers=num_workers, pin_memory=True)train(model, optimizer, trainloader, num_iters=20)</code></pre><p></p><p>如果num_workers=0，性能图谱如下： <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250312184417.png"></p><p>可以看出GPU在MemcpyHtoD的数据拷贝任务之前在等待CPU的数据加载，导致GPU空闲。</p><p>如果num_workers=4，性能图谱如下： <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250312184640.png">明显观察到数据加载的耗时大幅下降，与此同时GPU等待数据加载的时间也缩短到了只有40μs，远低于之前的10ms，这正是多进程并行以及数据预加载共同作用的结果。</p><h3 id="使用异步接口提交数据传输任务">使用异步接口提交数据传输任务</h3><p>GPU在MemcpyHtoD的数据拷贝任务之后到执行具体的算子计算之前，还存在一个空闲时间。</p><p>CPU数据传输任务aten::to中，包含了一个同步操作，即cudaStreamSynchronize，这意味数据从主存到显存的拷贝是会阻塞CPU的。</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250312185304.png"></p><p><strong>这时因为代码中使用了tensor.to(device)的方法将张量从CPU复制到GPU，默认采取的是同步模式。</strong></p><p>为了实现启用非阻塞模式的数据传输（允许在向GPU传输数据的同时，CPU能够继续执行其他任务。），必须同时满足下面两个条件：- 需要传输的数据必须存储在<strong>锁页内存(pinnedmemory)</strong>中。锁页内存的物理地址是固定的，不会被操作系统换出到磁盘，从而允许GPU直接访问这部分内存。在PyTorch中创建的张量默认是常规的页内存(pageablememory)，但可以通过<strong>DataLoader的设置直接将数据加载到锁页内存</strong>，或使用<strong>tensor.pin_memory()方法手动将张量移动到锁页内存</strong>。-在调用数据传输时需要设置为非阻塞模式，如<strong>tensor.to("cuda",non_blocking=True)</strong>。这样数据传输的任务会被提交到GPU的任务队列中，CPU则不需要等待数据传输完成即可继续执行后续代码。</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250312190638.png"></p><h3 id="数据传输与gpu计算任务并行">数据传输与GPU计算任务并行</h3><p>现在GPU在进行数据拷贝与计算时可以无缝衔接了，但是有没有可能让这个两个任务并行起来呢？</p><p>我们可以采取类似CPU预加载数据的策略：<strong>在当前训练轮次进行的同时，预先把下一轮训练所需的数据从CPU复制到GPU上。</strong>要做到这一点，我们需要通过配置不同的GPU计算流(CUDAStream)来创建一个并行的数据拷贝任务。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">def train(model, optimizer, trainloader, num_iters):    # Create two CUDA streams    stream1 = torch.cuda.Stream()    stream2 = torch.cuda.Stream()    submit_stream = stream1    running_stream = stream2    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:        for i, batch in enumerate(trainloader, 0):            if i &gt;= num_iters:                break            with torch.cuda.stream(submit_stream):                data = batch[0].cuda(non_blocking=True)                submit_stream.wait_stream(running_stream)                # Forward pass                optimizer.zero_grad()                output = model(data)                loss = output.sum()                # Backward pass and optimize                loss.backward()                optimizer.step()            # Alternate between the two streams            submit_stream = stream2 if submit_stream == stream1 else stream1            running_stream = stream2 if running_stream == stream1 else stream1    prof.export_chrome_trace(f"PROF_double_buffering_wait_after_data.json")</code></pre><p>引入了submit_stream.wait_stream(running_stream)来进行GPU队列间的同步和等待，保证两个GPU队列的重叠部分仅限于数据传输，而计算部分不发生重叠。</p><p>这一技巧与推理中常用的双重缓冲(double buffering)优化有些相似。</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250312192217.png"></p><p>需要注意的是，双重缓冲主要是加快数据的拷贝速度，因此在数据量较小、数据传输用时较短的场景中，效果可能不太明显。此外，它也可能带来GPU队列间同步的额外开销，有时可能会导致性能略有下降。</p><h2 id="提高gpu计算任务的效率">提高GPU计算任务的效率</h2><p>我们可以用模型浮点运算利用率(model FLOPSutilization,MFU)来衡量GPU的使用效率：</p><p><span class="math display">\[MFU = \frac{实际使用FLOPS}{GPU理论最高FLOPS}\]</span></p><p>关于GPU计算优化，主要包括： -(1)GPU算子执行时间太短，或算子中的计算过于简单，导致为该算子调度的额外开销甚至超过了计算本身，使得性价比较低。- (2)GPU算子的并行度不足，未能充分利用GPU中大量的线程块资源导致浪费。 -(3)GPU算子使用了不合适的内存布局，增加了额外的访存开销。 -(4)GPU算子的具体实现还有改进的空间。</p><p>本节只讨论针对(1)和(2)进行优化。</p><h3 id="最大batch-size">最大Batch Size</h3><p>一个算子的CUDA核函数实现通常只针对输入张量的[C,H,W]维度，而不会直接涉及Batch维度的计算。</p><p>也就是说<strong>BatchSize与算子CUDA代码的实现是独立的</strong>，提升BatchSize并不能直接提升CUDA核函数的性能。然而它可以<strong>增加GPU使用的线程块(block)数量</strong>，一次性完成多个样本的计算。本质上来说，BatchSize是通过增加计算并行度的方式来提高算子计算效率的。</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250312195951.png">从图中看出BatchSize通过增加GPU的计算并行度来提高性能，但这种并行度受到GPU线程块总数的限制。</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250312200042.png"></p><p>但是较大的BatchSize可能会影响模型的泛化能力，尽管这也受到训练集大小、组成、网络结构和训练方法等因素的影响。</p><p>实际上，许多大型模型使用较大的BatchSize，这也表明在大数据集上，较大的BatchSize的负面影响可能会有所减轻。选择BatchSize时，需要在模型质量和训练速度之间找到平衡。</p><h3 id="使用融合算子">使用融合算子</h3><p>PyTorch的灵活性也意味着PyTorch中的GPU算子比较轻量，每个算子调用都需要经过一系列层级的调用流程：从Python到C++，再到CUDA执行，然后将结果返回C++，最后回到Python。</p><p>因此，本节将专注于如何通过改变代码写法，合并相邻的算子调用来减少这些调度开销。</p><p>一种有效的策略是<strong>手动合并算子</strong>，这通常需要一定的数学技巧来将多个相邻的算子融合成一个单一的算子。一个典型的例子是合并多个连续的逐元素操作(elementwise operations)，例如：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchx = torch.rand(3, 3)y = torch.rand(3, 3)z = x * yz1 = z + x# 可以将上面的计算合并为一个算子，结果是等价的z2 = torch.addcmul(x, x, y) </code></pre> 一些常见的融合还包括<strong>点积与加法合并</strong>：<pre class="line-numbers language-python" data-language="python"><code class="language-python">import torcha = torch.rand(4, 4)b = torch.rand(4, 4)c = torch.rand(4, 4)x = torch.matmul(a, b)x1 = x + c# 可以将上面的计算合并为一个算子，结果是等价的x2 = torch.addmm(c, a, b)</code></pre><p></p><p>除了依靠数学知识手动融合多个算子之外，PyTorch还在<strong>torch.nn.utils.fusion</strong>模块下提供了一系列常用的算子融合的接口。</p><p>例如，fuse_linear_bn_eval接口能够将相邻的Linear算子和BatchNorm算子合并为一个新的Linear算子。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchimport torch.nn as nnfrom torch.profiler import profile, ProfilerActivityclass SimpleModel(nn.Module):    def __init__(self):        super(SimpleModel, self).__init__()        self.linear = nn.Linear(100, 50)        self.bn = nn.BatchNorm1d(50)    def forward(self, x):        return self.bn(self.linear(x))@torch.no_grad()def run(data, model, num_iters, name):    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:        for _ in range(num_iters):            original_output = model(input_tensor)    prof.export_chrome_trace(f"traces/PROF_cuda_{name}.json")model = SimpleModel().to(torch.device("cuda:0"))model.eval()input_tensor = torch.randn(4, 100, device="cuda:0")# 融合前run(input_tensor, model, num_iters=20, name="no_fusion")# 融合后fused_model = torch.nn.utils.fusion.fuse_linear_bn_eval(model.linear, model.bn)run(input_tensor, fused_model, num_iters=20, name="fusion")</code></pre><p></p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250312201356.png"></p><p>算子融合是优化深度学习网络的一种非常常见的方法，它有助于提升网络运行效率和减少资源消耗。尽管如此，PyTorch的原生接口中并没有提供太多关于算子融合的直接支持，通常需要依赖手动融合算子，而这一过程可能耗时较长。</p><p>在实际训练中，常见的算子融合方法包括使用<strong>torch.compile和引入高性能自定义算子</strong>，但这些方法的原理相对复杂，更详尽的讨论和应用将在后面高级优化方法中进行。</p><h2 id="减少cpu和gpu间的同步">减少CPU和GPU间的同步</h2><p>PyTorch中的一些写法会隐式地进行同步，这常常是PyTorch程序性能的“隐藏杀手”​。<strong>绝大多数隐式同步其实有一个共性，那便是想要在Python中使用GPU张量的值。</strong><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250312202357.png"></p><p>torch.nonzero()造成同步的例子： </p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchimport torch.nn as nnfrom torch.profiler import profile, ProfilerActivityclass Model(torch.nn.Module):    def __init__(self):        super(Model, self).__init__()        self.linear1 = nn.Linear(1000, 5000)        self.linear2 = nn.Linear(5000, 10000)        self.linear3 = nn.Linear(10000, 10000)        self.relu = nn.ReLU()    def forward(self, x):        output = self.relu(self.linear1(x))        output = self.relu(self.linear2(output))        output = self.relu(self.linear3(output))        #----------------隐式同步----------------#        nonzero = torch.nonzero(output)        return nonzerodef run(data, model):    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:        for _ in range(10):            model(data)    prof.export_chrome_trace("traces/PROF_nonzero.json")data = torch.randn(1, 1000, device="cuda")model = Model().to(torch.device("cuda"))run(data, model)</code></pre> <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250312203114.png"><p></p><p>cudaMemcpyAsync，这是一个GPU到CPU的数据拷贝(memcpy device to host)。torch.nonzero()算子会返回一个新的张量，其中包含输入张量中所有非零元素的索引，也就是它们在输入张量中的位置。需要确保nonzero的输入张量的值运算完毕，计算出非零元素的个数后再将这个数字从GPU传回CPU上。</p><h2 id="降低程序中的额外开销">降低程序中的额外开销</h2><h3 id="python接口调用开销">Python接口调用开销</h3><p>当我们使用PyTorch的任何操作时，都会触及两层逻辑：一是上层的Python接口，提供灵活性和易用性；二是底层的C++实现，用于保证计算效率。由于第3章提到的PyTorch的CPU和GPU异步执行机制，开发者有时可能不会意识到调用PyTorchAPI所隐含的性能开销。</p><p>在对性能有极高要求的推理场景中，我们可能会考虑放弃使用Python，转而直接采用C++等静态语言，以最小化调度开销。然而，在更加注重灵活性和易用性的训练场景中，我们可以将Python层的额外开销视为一种“易用税”​，这是在开发效率和程序性能之间达成平衡的必要成本。</p><h3 id="避免张量的创建开销">避免张量的创建开销</h3><p>张量的创建和销毁，特别是涉及显存管理的操作，这些都是开销较大的操作。PyTorch内部的缓存池机制可以在一定程度上改善由动态张量分配引起的性能问题，但它并不能完全解决这一问题。#### 直接在GPU上创建张量而不是CPU 案例： </p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchimport torch.nn as nnfrom torch.profiler import profile, ProfilerActivitydef tensor_creation(num_iters, create_on_gpu):    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:        shape = (10, 6400)        for i in range(num_iters):            #------------------------------------------------------------#            if create_on_gpu:                data = torch.randn(shape, device="cuda") # 直接在GPU上创建张量            else:                data = torch.randn(shape).to("cuda") # 先在CPU上创建张量，然后拷贝到GPU            #------------------------------------------------------------#    prof.export_chrome_trace(        f"traces/PROF_tensor_creation_on_gpu_{create_on_gpu}.json"    )# 情况1. 先在CPU上创建Tensor然后拷贝到GPUtensor_creation(20, create_on_gpu=False)# 情况2. 直接在GPU上创建Tensortensor_creation(20, create_on_gpu=True)</code></pre> <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250312204843.png"><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250312204903.png">在本例中使用torch.randn().to("cuda")的写法耗时265μs，而使用torch.randn(device="cuda")的写法则只需要12μs。<p></p><h4 id="使用原位操作">使用原位操作</h4><p>大部分PyTorch操作默认会为返回张量创建新的内存，但大部分张量在创造出来之后只使用一次即被销毁，这样多少会造成资源的浪费。</p><p>原位操作作不会生成新的输出张量，而是直接在输入张量上进行修改。由于省去了内存的创建过程，原位操作通常在性能上更为高效。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchfrom torch.profiler import profile, ProfilerActivitydef run(data, use_inplace):    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:        for i in range(2):            #------------------------------------------------------------#            if use_inplace:                data.mul_(2) # 原位操作，一般以_结尾            else:                output = data.mul(2) # 非原位操作            #------------------------------------------------------------#    prof.export_chrome_trace(f"traces/PROF_use_inplace_{use_inplace}.json")shape = (32, 32, 256, 256)# Non-Inplacedata1 = torch.randn(shape, device="cuda:0")run(data1, use_inplace=False)# Inplacedata2 = torch.randn(shape, device="cuda:0")run(data2, use_inplace=True)</code></pre><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250312205852.png"></p><h3 id="关闭不必要的梯度计算">关闭不必要的梯度计算</h3><p>在某些模型微调的场景中，我们通常在一个预训练的模型上添加一个自定义的小型模块。在这种情况下，预训练模型的参数会被冻结，只对新增的自定义模块进行训练。对于这部分仅需要前向传播的代码，通常使用<strong>torch.no_grad()</strong>这个上下文管理器。在这个上下文管理器的范围内，所有创建的张量的requires_grad属性都被标志为False，这些张量参与的计算操作不会被跟踪历史，也就是不会在反向传播中计算梯度。</p><p>另一种不需要反向梯度更新的场景是纯推理代码，这样的使用场景下往往所有的代码段都只承担前向传播的任务，整个运行过程中完全不涉及反向传播。一般此时会将模型设置为评估模式，即调用<strong>model.eval()</strong>。此操作主要改变某些层的行为：例如，在推理模式下，BatchNormalization层将不会更新统计数据，且Dropout层不会执行随机丢弃功能。这是确保在推理时获得准确结果的必要步骤，尽管它对性能的提升并不显著。</p><p>为了加速推理过程，PyTorch提供了<strong>torch.inference_mode()</strong>接口，这是比torch.no_grad()更为激进的面向推理预测代码的优化，除了不生成反向算子以外，还会关闭一系列只在反向过程起作用的检查或设置，比如原位算子的版本检测机制等。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchimport torch.nn as nnimport timeclass SimpleCNN(nn.Module):    def __init__(self):        super(SimpleCNN, self).__init__()        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)        self.relu = nn.ReLU()        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)    def forward(self, x):        x = self.conv1(x)        x = self.relu(x)        x = self.conv2(x)        return xdef infer(input_data, num_iters, use_inference_mode):    start = time.perf_counter()    with torch.inference_mode(mode=use_inference_mode):        for _ in range(num_iters):            output = model(input_data)    torch.cuda.synchronize()    end = time.perf_counter()    return (end - start) * 1000model = SimpleCNN().to(torch.device("cuda:0"))input_data = torch.randn(1, 3, 224, 224, device="cuda:0")# ---------------------------------开启Inference Mode---------------------------------#infer(input_data, num_iters=10, use_inference_mode=True)  # warm upruntime = infer(input_data, num_iters=100, use_inference_mode=True)print(f"开启Inference Mode用时: {runtime}s")# ---------------------------------关闭Inference Mode---------------------------------#infer(input_data, num_iters=10, use_inference_mode=False)  # warm upruntime = infer(input_data, num_iters=100, use_inference_mode=False)print(f"关闭Inference Mode用时: {runtime}s")</code></pre><h2 id="有代价的性能优化">有代价的性能优化</h2><h3 id="使用低精度数据进行拷贝">使用低精度数据进行拷贝</h3><p>采用异步方式可以减少GPU等待CPU数据拷贝的时间，但是没有加速拷贝本身，对于大尺寸的输入张量，或者使用很大BatchSize的场景，数据拷贝本身会消耗相当长的时间。</p><p>因此可以采用<strong>低精度数据进行拷贝</strong>，可以参考常用的量化压缩方法来将高精度数据转化为低精度数据，读入到GPU后再转化回高精度数据参与训练；或者使用对GPU友好的编解码算法——读取编码压缩后的数据，然后在GPU上进行解码等。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchimport torch.nn as nnfrom torch.profiler import profile, ProfilerActivitydef data_copy(data, dtype_name=""):    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:        for _ in range(10):            output = data.to("cuda:0", non_blocking=False)    prof.export_chrome_trace(f"traces/PROF_data_copy_{dtype_name}.json")# Float precisiondata1 = torch.randn(4, 32, 32, 1024, dtype=torch.float32)data_copy(data1, "float32")# Uint8 precisiondata2 = torch.randint(0, 255, (4, 32, 32, 1024), dtype=torch.uint8)data_copy(data2, "uint8")</code></pre><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250312211943.png"></p><p>在条件允许的时候应该尽量使用无损的压缩技术，然而实际应用中大部分量化压缩算法以及部分编解码算法都是有损压缩。因此在决定使用低精度数据前还需要仔细验证。<strong>对于一些没有明确数值界限的数据来说，量化压缩到低精度数据可能对于训练结果和收敛性是有损伤的。</strong></p><h3 id="使用性能特化的优化器">使用性能特化的优化器</h3><p>在训练任务中，一般不会将反向传播计算出来的梯度直接累加到模型参数上，而是通过优化器(optimizer)来控制参数更新的数值。一般来说<strong>优化器会对梯度进行一系列加工，随后计算出参数更新的具体数值</strong>。</p><p>PyTorch针对每种优化器，提供了三种不同的梯度更新实现：for-loop、for-each、fused。</p><h4 id="for-loop">for-loop</h4><p>or-loop是最为偏重于节省显存的实现，但是性能比较差。举例说明其实现原理：假如使用SGD方法更新10个参数——从w1到w10，则for-loop会使用Python中的串行循环，每次更新其中一个权重，SGD的梯度更新方式通常包括一次乘法和一次加法：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 伪代码for w in [w1, w2, ..., w10]:    w = w - lr * w.grad</code></pre><p></p><p>在PyTorch中，由于动态图的局限性，所有算子计算都不能自动融合，因此更新所有参数需要调用10次乘法算子和10次加法算子。这总计20次算子的调用成本可能远大于底层CUDA乘法、CUDA加法计算。特别是在参数数量非常多的时候，反复的算子调用会极大地损耗性能。为了解决这个问题，PyTorch进一步引入了for-each方法。</p><h4 id="for-each和fused">for-each和fused</h4><p>for-each是相对偏重于性能的方法，但其占用的显存会更多。由于所有参数更新都是相互独立的，我们可以先把10个参数合并到一个张量中，这样就只需要对这个合并张量调用1次乘法算子和1次加法算子即可。虽然实际计算量没有变化，但是极大地降低了算子反复调用的次数，提高了性能。</p><p>但是对一些更复杂的优化器来说，比如包含了若干乘法、除法、加减法、平方开方等运算的Adam优化器而言，即使使用了for-each方法之后依然还有很多算子调用。fused方法在for-each的基础上，进一步将优化器的所有计算都合并成一个算子，所以能够达到最佳的计算性能，但是其显存占用也比较高。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchfrom torch.profiler import profile, ProfilerActivityclass SimpleNet(torch.nn.Module):    def __init__(self):        super(SimpleNet, self).__init__()        self.fcs = torch.nn.ModuleList(torch.nn.Linear(200, 200) for i in range(20))    def forward(self, x):        for i in range(len(self.fcs)):            x = torch.relu(self.fcs[i](x))        return xdef train(net, optimizer, opt_name=""):    data = torch.randn(64, 200, device="cuda:0")    target = torch.randint(0, 1, (64,), device="cuda:0")    criterion = torch.nn.CrossEntropyLoss()    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:        for _ in range(5):            optimizer.zero_grad()            output = net(data)            loss = criterion(output, target)            loss.backward()            optimizer.step()    prof.export_chrome_trace(f"traces/PROF_perf_{opt_name}.json")# ----------------------For-loop----------------------#net = SimpleNet().to(torch.device("cuda:0"))adam_for_loop = torch.optim.Adam(    net.parameters(), lr=0.01, foreach=False, fused=False)train(net, adam_for_loop, opt_name="for_loop")# ----------------------For-each----------------------#net = SimpleNet().to(torch.device("cuda:0"))adam_for_each = torch.optim.Adam(    net.parameters(), lr=0.01, foreach=True, fused=False)train(net, adam_for_each, opt_name="for_each")# ----------------------Fused----------------------#net = SimpleNet().to(torch.device("cuda:0"))adam_fused = torch.optim.Adam(net.parameters(), lr=0.01, foreach=False, fused=True)train(net, adam_fused, opt_name="fused")</code></pre><p>for-loop每次只更新一个参数，如图所示for-loop实现会循环调用add、lerp、mul、addcmul等算子:<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250312213219.png"></p><p>for-each实现则会将add、lerp、mul、addcmul……这些独立的调用按类型合并，所以GPU队列中只会看到7个融合算子对应的计算任务:<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250312213328.png"></p><p>fused则采用了更为激进的融合策略，可以看到算子调用次数减少到了两次，分别为两个巨大的融合算子，这导致优化器的CPU延迟被压缩到非常短，在性能上更具优势。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250312213357.png"></p><h2 id="小结">小结</h2><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250312213503.png">本章讲述的性能优化方法，其优化上限是将GPU队列中的“气泡”完全消除——也就是让GPU达到满载状态，但这并不是性能优化的终点。</p><p>在GPU达到满载之后，我们还可以借助高级优化方法中的技巧来进一步优化GPU计算效率。</p><p>最后当我们将训练性能优化到极限之后，还可以采用分布式系统中数据并行的策略再次对模型训练进行加速。</p>]]></content>
      
      
      <categories>
          
          <category> 《大模型动力引擎》阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《大模型动力引擎》 </tag>
            
            <tag> 单卡性能优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型动力引擎-数据加载和预处理</title>
      <link href="/2025/03/11/da-mo-xing-dong-li-yin-qing-shu-ju-jia-zai-he-yu-chu-li/"/>
      <url>/2025/03/11/da-mo-xing-dong-li-yin-qing-shu-ju-jia-zai-he-yu-chu-li/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250311171922.png"></p><h2 id="数据接入准备">数据接入准备</h2><p>对于一些经典任务可以从公共数据集中抽取小规模子集来快速进行收敛性验证，排除代码错误。### 常用公开数据集 <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250311172108.png"></p><h2 id="数据集的获取和预处理">数据集的获取和预处理</h2><h3 id="获取原始数据">获取原始数据</h3><p>主要包含两个来源： - 公开数据集 - 自定义数据集</p><h3 id="原始数据的清洗">原始数据的清洗</h3><p><strong>一个算法工程师必须具备的关键能力之一是对数据的敏感度</strong>：对于特定任务，能够判断数据集可能存在的问题，知道理想数据应呈现的形态，能组织高质量的数据集，并能验证数据的质量。</p><p>对于深度学习数据来说，我们可以将数据按照有无标签进行分类。</p><p><strong>无标签的数据一般用于无监督学习</strong>，这类数据的问题往往出在部分数据自身质量较差。这里只列举一些常见情况：- 图片/视频/图形类数据：低分辨率、高噪声、高曝光。 -文本类数据：不符合自然语言，含有奇怪的符号、标点、特殊数值等。</p><p><strong>有标签的数据则往往用于监督学习</strong>，在数据本身质量差的基础上，还会出现标签和数据对不上的问题，也就是所谓的“货不对版”​，比如说图片的分类不正确，或者图片的描述不合适。除此以外，还有可能出现一对多的错误，比如同样的图片被贴上了相互冲突的描述等。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250311172950.png"></p><p>大部分数据清洗工作都依赖于脚本进行。为了写作这样的脚本，我们首先要做的是为脏数据划定一条明确的界限。在对数据进行检查时，有些标准可以简单读取，比如图片分辨率，但是某些标准可能需要借助其他算法或者预训练模型来对数据进行判断。</p><p>另外，直接通过预训练的大模型来对数据进行综合打分也是一种数据清洗的方法，本质上是数据蒸馏，需要保证预训练模型的优质性。</p><h3 id="数据的离线预处理">数据的离线预处理</h3><p>数据预处理主要是为了改善数据的质量和结构，以及针对模型对数据进行适配。一些常见的离线数据预处理步骤包括数值范围的标准化、数据编码、数据增强等。</p><h4 id="数值范围的标准化">数值范围的标准化</h4><p>如最小-最大归一化(min-maxscaling)或者缩放到正态分布的标准化(standardization)。</p><p>需要注意最小-最大归一化需要考虑数据中是否存在极端值，否则会严重影响数据分布。</p><h4 id="数据编码">数据编码</h4><p>将非数值数据转换为数值格式的过程，比如数据集中的标签数据一般都需要经过数据编码才能输入到模型中。</p><h4 id="均衡数据分布">均衡数据分布</h4><p>当数据集中某些类别的样本数量远少于其他类别，这种不平衡有可能导致模型在训练过程中对占多数的类别过度拟合，而无法有效学习到少数类别的特征。</p><p>数据增强可以通过对原始数据进行一些变换，生成新的数据，从而增加数据集的多样性，提高模型的泛化能力。在文本处理中，数据增强包括同义词替换、句子重排等；在图像处理中，数据增强包括旋转、裁剪、翻转、改变亮度、对比度等，还有一些特殊方法，比如mixup、cutout、cutmix、mosaic等。</p><h4 id="python数据处理库">Python数据处理库</h4><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250311175426.png"></p><h3 id="数据的存储">数据的存储</h3><p>一般来说，一个数据集由三种类型的数据组成： -核心数据：比如图片、视频、音频、文本等，根据数据集面向的训练任务而定。 -数据标签或其他补充信息（可选）​：比如图片分类数据中，每张图片对应的类别标签；比如图片-文字数据集中每个图片对应的文字描述。- 数据集信息：比如数据集的版本、原始数据来源、预处理方法、参数等。</p><p>数据的存储格式多种多样： <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250311180157.png"></p><h3 id="pytorch与第三方库的交互">PyTorch与第三方库的交互</h3><p>通过Python库我们可以将数据从硬盘或服务器上读取到内存中，此时读入内存的数据通常以第三方库数据类型如NumPyndarray或PIL Image形式存在，PyTorch无法直接解析这些类型。</p><p>而无论是第三方库的还是Python原生的，都必须转化为张量后才能用于训练。PyTorch为了便于使用NumPy数据，特别提供了将NumPyndarray转换为张量的接口。对于其他无直接接口的库如Pandas等，建议先转换为NumPyndarray再导入到PyTorch。 </p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as npimport torchx = np.zeros((3, 3))y = torch.from_numpy(x)print(y, type(y))# tensor([[0., 0., 0.],#        [0., 0., 0.],#        [0., 0., 0.]], dtype=torch.float64) &lt;class 'torch.Tensor'&gt;</code></pre>对于示例中的from_numpy调用，其<strong>返回的PyTorch张量内存地址与原先的numpy.ndarray完全相同</strong>，也就是说没有做额外的数据复制操作。<p></p><p>但是当PyTorch无法复用原始numpy.ndarray的内存时，from_numpy会报错：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as npimport torchx = np.random.random(size=(4, 4, 2))y = np.flip(x, axis=0) # 翻转x的第一个维度，导致stride为负# 报错# ValueError: At least one stride in the given numpy array is negative,# and tensors with negative strides are not currently supported.# (You can probably work around this by making a copy of your array  with array.copy().)torch.from_numpy(y)# 创建副本后能够正常运行torch.from_numpy(y.copy())</code></pre><p></p><h2 id="数据集的加载与使用">数据集的加载与使用</h2><p>一般的串行加载方法：使用json、csv等库函数读取标签信息，使用PIL等库来加载图片数据，最后再使用如tensor.from_numpy()等接口将数据转化为张量数据送入模型。</p><p>然而在进行大规模训练的时候，串行的数据加载和预处理就会显著阻塞模型运算，严重影响训练效率，高效的方法是：在模型进行GPU运算的同时，CPU能异步准备好一下轮的训练数据。</p><p>PyTorch中使用Dataset类和Dataloader类来支持上述数据加载过程的优化。</p><p>Dataset描述了读取单个数据的方法以及必要的预处理，输出的是单个张量。DataLoader则定义了批量读取数据的方法，包括BatchSize、预读取、多进程读取等，输出的结果是一批张量。</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250311202356.png"></p><h3 id="dataset封装">Dataset封装</h3><p>PyTorch框架提供了两种类型的数据集抽象：映射式数据集(map styledataset)和迭代式数据集(iterable styledataset)，它们在数据的访问和<strong>适用场景</strong>上有所不同。</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250311202525.png"></p><p>Pytorch已经将许多常见数据集预封装成Dataset类，比如torchvision.datasets.CIFAR10，可以直接调用。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchvision.datasets as datasetsimport torchvision.transforms as transformstransform = transforms.Compose([transforms.ToTensor()])train_dataset = datasets.CIFAR10(    root="./data", train=True, download=True, transform=transform)test_dataset = datasets.CIFAR10(    root="./data", train=False, download=True, transform=transform)</code></pre><p></p><p>对于自定义数据集则需要自己对数据进行Dataset封装，我们以本地下载的Cifar10数据集为例，进行Dataset封装。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250311203721.png"></p><p>大概方式是：定义一个继承自Dataset类的CifarDataset。然后为CifarDataset实现下面三个方法：-__init__方法：构造指向每个数据路径的列表，比如在上例中我们构造了“图片路径-标签”的列表。我们没有直接读取图片，这是防止内存被过多数据挤爆。- __len__方法：返回数据集中的样本数。 -__getitem__方法：根据索引读取图片数据，并将数据转换为PyTorch张量。</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250311204632.png"></p><h3 id="dataloader封装">DataLoader封装</h3><p>与模型训练相关的数据加载操作，如对数据集进行采样并批量加载数据、使用多个子进程来并行加载数据等都在Dataloader类中有良好的实现和封装。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250311205133.png">- batch_size：指定每个批次中的样本数量。 -shuffle：是sampler参数的“快捷键”​。shuffle=False相当于顺序采样，即sampler=SequentialSampler；而shuffle=True相当于随机采样即sampler=RandomSampler。如果需要自定义更为复杂的采样策略，用户也可以实现一个Sampler类，并指定Dataloader的sampler参数。- num_workers：加载数据时使用的子进程数量。 -drop_last：当数据集中的样本数量不能被batch_size整除时，是否忽略最后一个不完整的批次。</p><h2 id="数据加载性能分析">数据加载性能分析</h2><p>首先需要明确数据部分的性能优化目标是保持GPU持续工作，避免因数据等待导致GPU空闲。GPU空闲可能由多种因素引起，如从硬盘读取数据到内存的延时、CPU预处理时间过长，或是数据从CPU传输到GPU的速度慢等。</p><p>注意<strong>只需优化到确保GPU运行不被阻塞</strong>，在GPU任务已经排队的情况下，过度提交任务不仅不会提升GPU的运行速度，还可能因CPU资源争夺而引起性能下降。</p><h3 id="充分利用cpu的多核资源">充分利用CPU的多核资源</h3><p>使用<code>htop</code>命令查看CPU活动状态，如果有“一核工作，多核围观”的情况，则说明CPU的核数没有充分利用，可以通过增加<code>num_workers</code>来充分利用CPU的多核资源。</p><p>但是要注意子进程过多可能导致内存占用过多、I/O阻塞等副作用。</p><h3 id="优化cpu上的计算负载">优化CPU上的计算负载</h3><p>如果开启多进程优化后，GPU仍在等待数据，且CPU上的数据加载和处理时间过长，特别是htop中CPU核心都达到满载状态，这说明在CPU上进行的数据预处理和转换的计算量过重，在三方库的实现中很常见。</p><p>这时也还有优化空间，比如： -使用更高效的第三方算法或库，如使用Pillow-SIMD（单指令多数据）替换Pillow；- 把计算密集型的数据处理转为离线预处理，把转换后的数据存储在硬盘上备用；-如果有一些确实无法预先进行处理的操作，可以考虑将该操作从CPU移至计算能力更强大的GPU进行。</p><h3 id="减少不必要的cpu线程">减少不必要的CPU线程</h3><h4 id="numpy的线程池">Numpy的线程池</h4><p>Numpy的计算操作默认会使用CPU的多线程，往往会把CPU全部核心集中在自己身上，挤占其他进程的计算资源。</p><p>一种常见情况是<strong>NumPy与PyTorchDataLoader的数据加载进程发生冲突</strong>。DataLoader使用多个进程加载数据，每个数据加载进程在importnumpy的时候都会为NumPy独立地创建N（N =CPU核数）个线程，过多的CPU线程会导致内存使用增加、不必要的上下文切换开销和资源的争用，从而降低程序的执行效率。</p><p>因此我们建议使用NumPy进行预处理的读者适当地限制NumPy的线程数量，这可以通过设置环境变量来实现，但请注意该操作一定要在importnumpy之前，代码如下： </p><pre class="line-numbers language-python" data-language="python"><code class="language-python">from os import environ# 控制NumPy底层库创建的线程数量N_THREADS = "4"environ["OMP_NUM_THREADS"] = N_THREADSenviron["OPENBLAS_NUM_THREADS"] = N_THREADSenviron["MKL_NUM_THREADS"] = N_THREADSenviron["VECLIB_MAXIMUM_THREADS"] = N_THREADSenviron["NUMEXPR_NUM_THREADS"] = N_THREADSimport numpy as npimport pdbpdb.set_trace()x = np.zeros((1024, 1024))</code></pre> #### I/O优化有的时候CPU本身计算负载并不高，但是大部分占用CPU的进程状态为“D”，表示处于不可中断的睡眠状态，通常于某种类型的系统调用有关，如等待I/O操作完成。<p></p><p>可以通过<code>iostat</code>工具检测磁盘的I/O负载： </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">iostat -xtck 2             # 每隔2秒输出一次磁盘I/O负载</code></pre><p></p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250311214416.png"></p><p>I/O占比过高，说明存储设备成为瓶颈，可以考虑以下思路缓解： -(1)<strong>用内存来换取显著提高的数据加载速度</strong>：例如使用<strong>mmap</strong>将文件的一部分直接映射到内存中，然后通过指针访问文件中的数据，而无须显式的I/O操作。这减少了I/O操作的开销，提高了数据访问速度。特别是在随机访问文件的不同部分时，mmap表现出色。当然在内存容量允许的情况下甚至可以考虑使用<strong>内存盘(RAMDisk)技术</strong>，使用RAM来虚拟磁盘，用内存来换取显著提高的数据加载速度。-(2)<strong>优化硬盘的读写模式</strong>：在2.2小节中，我们介绍了硬盘的两种读写模式，其中<strong>连续读写模式的性能远超随机读写模式</strong>。因此，我们可以通过将离散数据合并到少量的二进制文件或TFRecord中，将随机读写转化为连续读写，从而成倍地提高读写效率。- (3)<strong>更换更快的SSD硬件</strong>：如NVMe SSD等。</p><h2 id="小结">小结</h2><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250311215250.png"></p>]]></content>
      
      
      <categories>
          
          <category> 《大模型动力引擎》阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《大模型动力引擎》 </tag>
            
            <tag> 数据加载和预处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型动力引擎-定位性能瓶颈的工具</title>
      <link href="/2025/03/10/da-mo-xing-dong-li-yin-qing-ding-wei-xing-neng-ping-jing-de-gong-ju/"/>
      <url>/2025/03/10/da-mo-xing-dong-li-yin-qing-ding-wei-xing-neng-ping-jing-de-gong-ju/</url>
      
        <content type="html"><![CDATA[<h2 id="性能瓶颈定位工具">性能瓶颈定位工具</h2><p>为了保证分析结果的可靠性，对测试环境的稳定性有一定要求。由于程序运行的软件和硬件环境中影响因素较多，本节将根据重要性排序，依次介绍提升测试稳定性的方法。</p><h3 id="减少无关程序的干扰">减少无关程序的干扰</h3><p>为了保证分析结果的可靠性，对测试环境的稳定性有一定要求。由于程序运行的软件和硬件环境中影响因素较多，本节将根据重要性排序，依次介绍提升测试稳定性的方法。</p><h4 id="gpu使用情况">GPU使用情况</h4><p>首先可以通过<code>nvidia-smi</code>监控GPU使用情况（<code>nvtop</code>命令提供了更友好的交互界面，适合监督训练过程）<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250310190914.png"></p><p>查看相应进程的更多信息：<code>ps aux | grep &lt;PID&gt;</code></p><p>杀掉进程：<code>kill -9 &lt;PID&gt;</code> （-9表示强制杀死进程）</p><p>有条件的读者还应<strong>避免在GPU的性能优化过程中使用图形界面</strong>，因为图形界面也会占用GPU资源，比如上图中的/usr/bin/Xorg.bin就对应了图形界面进程。可以考虑停止UNIX系统的图形用户界面服务，之后通过另一台机器进行SSH远程登录。</p><h4 id="cpu使用情况">CPU使用情况</h4><p>可以通过<code>htop</code>这一系统监视工具来查看CPU使用情况。</p><p>如果发现CPU使用率偏高——比如大部分CPU核心的占用率长时间在60%～70%以上，还可以通过进程列表来查询使用CPU最多的进程，并决定是否将其终止。</p><h3 id="提升pytorch程序的可重复性">提升PyTorch程序的可重复性</h3><p>前面着重讨论了如何避免其他程序的干扰，尽可能让目标程序独占计算资源，降低性能波动。但是程序内部也存在着一定的随机性，如模型权重的随机初始化/Dropout层随机失活，在本小节中，我们会专门介绍约束程序随机性的方法。</p><p>在Python生态系统中缺乏一个统一的随机数生成标准库。NumPy、PyTorch和TensorFlow等库在处理随机数生成时，各自有不同的实现和优化方式，因此，没有一个统一的方法可以集中控制Python程序中所有库的随机数种子，在写程序时，需要考虑不同库的随机数生成方式。</p><p>这里只介绍Pytorch、NumPy和python标准库random的随机数种子设置方法。</p><h4 id="设置pytorch随机数种子">设置PyTorch随机数种子</h4><p>PyTorch提供了一个<code>torch.manual_seed()</code>接口可以“一键设置”所有后端的随机数生成种子，以确保每次运行代码时，生成的随机数序列都是相同的。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchdef generate_random_seq(device):    return torch.rand((3, 3), device=device)print(    f"""不设置随机种子时，每次运行生成的序列都是不同的CPU: {generate_random_seq('cpu')}CUDA: {generate_random_seq('cuda')}""")# 为所有PyTorch后端设置生成随机数的种子seed = 32torch.manual_seed(seed)print(    f"""设置随机种子后，每次运行都会生成相同的序列CPU: {generate_random_seq('cpu')}CUDA: {generate_random_seq('cuda')}""")</code></pre><p></p><h4 id="设置numpy随机数种子">设置NumPy随机数种子</h4><p>numpy提供了<code>numpy.random.seed()</code>接口，可以设置NumPy的随机数种子。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as npdef generate_random_seq():    return ", ".join([f"{np.random.random():.2f}" for _ in range(10)])print(f"不设置随机种子时，每次运行生成的序列都是不同的: {generate_random_seq()}")np.random.seed(32)print(f"设置随机种子后，每次运行都会生成相同的序列: {generate_random_seq()}")</code></pre><h4 id="设置python随机数种子">设置python随机数种子</h4><p>python的random模块提供了<code>random.seed()</code>接口，可以设置python标准库的随机数种子。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import osimport randomdef generate_random_seq():    return ", ".join([f"{random.random():.2f}" for _ in range(10)])print(f"不设置随机种子时，每次运行生成的序列都是不同的: {generate_random_seq()}")seed = 32random.seed(seed)print(f"设置随机种子后，每次运行都会生成相同的序列: {generate_random_seq()}")</code></pre><h4 id="其他随机性因素">其他随机性因素</h4><p>虽然设置随机数种子能控制一部分Python代码的随机性，但并不能完全消除随机性。因为随机性的来源很多，比如<strong>哈希算法就是其中之一</strong>。如果用到了基于哈希算法的如hash()函数，或者set的遍历，为了确保程序的可复现性，就需要<strong>设置环境变量PYTHONHASHSEED</strong>，代码如下。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">python -c 'print(hash("hello"))' # 跑多次结果是不一样的PYTHONHASHSEED=0 python -c 'print(hash("hello"))' #跑多次结果是一样的</code></pre><p>甚至有一些随机性是无法控制的，例如使用<strong>Python的glob模块获取的文件列表顺序可能是不确定的</strong>，这个顺序会受到操作系统和文件系统类型等多种因素的影响。如果在调试过程中需要可复现性，要求文件以特定顺序出现，那么我们就需要<strong>在获取文件列表后，手动对这些文件进行排序</strong>。</p><h4 id="约束gpu算子的随机性">约束GPU算子的随机性</h4><p>GPU的计算特点与CPU存在一些差别，这尤其体现在数值精度方面。</p><p>首先在硬件底层，浮点运算的机制及其硬件实现可能导致结合律在某些情况下不适用，由于其并行处理的特性，进行大量的数值累加时，累加的顺序可能不固定，从而可能进一步放大这种数值差异。</p><p>其次，NVIDIA提供的cuDNN加速库还在软件层面上进一步加重了数值的不确定性。以卷积算法为例，cuDNN提供了不同版本的卷积实现方法，会根据情况临时选择其中性能最高的一种进行卷积计算，这就导致其计算结果更加的不可控。除此以外，如果算子实现过程中用到了随机采样的算法，那么采样的随机性同样也会对结果产生影响。</p><p>不过对于cuDNN相关的操作，PyTorch还是提供了<code>torch.backends.cudnn.deterministic</code>和<code>torch.backends.cudnn.benchmark</code>接口，二者结合使用可以最大程度提高GPU计算结果的稳定性，使用方法如下所示：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">torch.backends.cudnn.deterministic = True # 确保算子使用确定性算法torch.backends.cudnn.benchmark = False # 禁用自动优化（如卷积算法）</code></pre><p>但是这两个接口都可能导致GPU计算性能下降，所以尽量只在需要调试程序或者进行性能分析时开启。</p><h4 id="完整的随机性约束脚本">完整的随机性约束脚本</h4><p>使用以下脚本可以一键设置前文介绍的所有随机种子，代码如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">def set_seed(seed: int = 37) -&gt; None:    np.random.seed(seed)    random.seed(seed)    torch.manual_seed(seed)  # 适用于所有PyTorch后端，包括CPU和所有CUDA设备    torch.backends.cudnn.deterministic = True    torch.backends.cudnn.benchmark = False    os.environ["PYTHONHASHSEED"] = str(seed)    print(f"设置随机数种子为{seed}")</code></pre><p></p><h3 id="控制gpu频率">控制GPU频率</h3><p>GPU在实际运行过程中会根据芯片状态动态调整显存频率和基础频率来自动平衡性能和功耗。对于性能分析而言，我们需要<strong>GPU始终保持在相同的频率进行测试，从而降低数据波动</strong>。可以使用如下指令来锁定GPU的频率：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"># 查询 GPU显存频率、流处理器频率、图形频率nvidia-smi --query-gpu=pstate,clocks.mem,clocks.sm,clocks.gr --format=csv# clocks.current.memory [MHz], clocks.current.sm [MHz], clocks.current.graphics [MHz]# 9751 MHz, 1695 MHz, 1695 MHz# 查询GPU支持的clock组合nvidia-smi --query-supported-clocks=gpu_name,mem,gr --format=csv# 设置persistent mode，可以减少GPU初始化延迟sudo nvidia-smi -pm 1# 固定GPU时钟nvidia-smi -ac 9751,1530 # &lt;memory, graphics&gt;</code></pre><p>注意AIGPU如V100、A100等是支持锁频功能的，但一些家用级别GPU可能不支持锁频（如RTX3090、4090系列）​，这时锁频指令会显示下述信息：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">Setting applications clocks is not supported for GPU 00000000:1A:00.0.Treating as warning and moving on.</code></pre><h3 id="控制cpu的状态和频率">控制CPU的状态和频率</h3><p>一个CPU的性能状态被划分为不同的等级，称为性能状态(Performance state,P-state)。每个P-state对应于一组特定的工作频率和电压。</p><p>在进行性能测量时，我们希望固定P-state和CPU频率，确保在每次运行基准测试时处理器以相同的性能状态运行。</p><p>首先需要安装<code>cpufrequtils</code>软件包，并通过设置最大、最小频率来间接控制住CPU的状态，代码如下：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo apt-get install cpufrequtils# 设置最大、最小频率sudo cpufreq-set -r -g performance # 设置为性能模式， -r 表示所有CPU核心 -g 表示性能模式sudo cpufreq-set -r -d 2Ghz # 设置最小频率为2GHzsudo cpufreq-set -r -u 2Ghz # 设置最大频率为2GHz</code></pre>然后可以查询当前CPU的性能信息和工作状态，来验证改动是否生效，代码如下：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"># 查询cpufreq-info# 或者cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_cur_freqcat /sys/devices/system/cpu/cpu0/cpufreq/scaling_min_freqcat /sys/devices/system/cpu/cpu0/cpufreq/scaling_max_freq</code></pre><p></p><p>另外，还可以在驱动或BIOS层面进一步关闭一些对性能有影响的CPU特性，例如：-设置max_cstate为1来阻止CPU进入低功耗状态，在计算机系统中C-state是指处理器的不同功耗状态，其中C0表示处理器处于活动状态，而C1、C2、C3等表示不同的睡眠状态，功耗逐渐降低。- 可以关闭超线程和睿频等高级功能。 </p><pre class="line-numbers language-none"><code class="language-none"># 查询C_statecat /sys/module/intel_idle/parameters/max_cstate# 查询turbo状态cat /sys/devices/system/cpu/intel_pstate/no_turbo</code></pre><p></p><h2 id="精确测量程序运行时间">精确测量程序运行时间</h2><h3 id="计量cpu程序的运行时间--time模块">计量CPU程序的运行时间--time模块</h3><p>time.time()或者time.perf_counter()计算两个时间戳的间隔。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import timestart = time.perf_counter()# 程序运行end = time.perf_counter()print(f"程序运行时间: {end - start}秒")</code></pre><h3 id="warmup和多次运行取平均">warmup和多次运行取平均</h3><p>计算机程序具有<strong>冷启动效应</strong>。具体到训练过程来说，一般最初几轮训练的单轮耗时要显著多于平稳运行时每轮的耗时，这主要是设备初始化、缓存命中、代码初次加载等诸多因素导致的。因此如果希望得到一致性更强的性能测试结果，就需要在正式测量前先进行几轮热身(warmup)，从而让系统达到稳定状态。除了程序预热以外，还应该测试多轮训练取平均值，进一步增加测试结果的稳定性。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import timeimport torchdef my_work():    # 需要计时的操作    sz = 64    x = torch.randn((sz, sz))if __name__ == "__main__":    # 热身    num_warmup = 5    for i in range(num_warmup):        start = time.perf_counter()        my_work()        end = time.perf_counter()        t = end - start        print(f"热身#{i}: {t * 1000 :.6f}ms")    # 多次运行取平均    repeat = 30    start = time.perf_counter()    for _ in range(repeat):        my_work()    end = time.perf_counter()    t = (end - start) / repeat    print(f"{repeat}次取平均: {t * 1000:.6f}ms")# 热身#0: 0.317707ms# 热身#1: 0.023586ms# 热身#2: 0.016913ms# 热身#3: 0.016409ms# 热身#4: 0.015868ms# 30次取平均: 0.014164ms</code></pre><h3 id="利用cpugpu同步计量gpu的运行时间--torch.cuda.synchronize">利用CPU/GPU同步计量GPU的运行时间--torch.cuda.synchronize()</h3><p>如<a href="https://baoblei.github.io/2025/03/07/da-mo-xing-dong-li-yin-qing-shen-du-xue-xi-bi-bei-de-pytorch">大模型动力引擎-PyTorch</a>所述，CPU和GPU之间的操作是异步的。由于Python解释器在CPU上运行，使用time.perf_counter()记录的时间实际上是CPU的时间戳。若CPU没有等待GPU任务完成就记录时间的话，测量结果就是错的，通常会比实际程序运行时间短很多。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250310224253.png"></p><p>因此必须让CPU先等待GPU运行完成，也就是<code>调用torch.cuda.synchronize()</code>结束再记录时间戳。代码如下所示：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import timeimport torchsz = 512N = 10shape = (sz, sz, sz)x = torch.randn(dtype=torch.float, size=shape, device="cuda")y = torch.randn(dtype=torch.float, size=shape, device="cuda")torch.cuda.synchronize()start = time.perf_counter()for _ in range(N):    z = x * y# 同步torch.cuda.synchronize()end = time.perf_counter()print(f"{N}次运行取平均: {(end - start) / N}s")</code></pre><h3 id="精确计量gpu的运行时间--torch.cuda.event">精确计量GPU的运行时间--torch.cuda.Event()</h3><p>联合使用torch.cuda.synchronize()和time.perf_counter()来计量GPU程序的运行时间会因为同步过程产生一定延迟，所以使用上面的CPU时间戳间隔测量的GPU耗时是要略长于GPU算子实际运行时间的。</p><p>可以通过建立<code>torch.cuda.Event()</code>对象，随后利用其record()方法在GPU队列中进行标记，最后通过两个CUDAEvent的时间间隔来测量GPU运行时间： </p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchsz = 512shape = (sz, sz, sz)x = torch.randn(dtype=torch.float, size=shape, device="cuda")y = torch.randn(dtype=torch.float, size=shape, device="cuda")start = torch.cuda.Event(enable_timing=True) # -enable_timing=True 表示启用时间测量end = torch.cuda.Event(enable_timing=True)start.record()z = x + yend.record()print(f"用时{start.elapsed_time(end)}ms")</code></pre><p></p><h2 id="pytorch的性能分析器--torch.profiler">PyTorch的性能分析器--torch.profiler</h2><p>PyTorch原生的torch.profiler在与PerfettoUI进行联用之后能够很好地兼顾使用的易用性和性能指标的信息量。</p><h3 id="性能分析">性能分析</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchimport torchvision.models as modelsfrom torch.profiler import profile, record_function, ProfilerActivitymodel = models.resnet18().cuda()inputs = torch.randn(5, 3, 224, 224, device="cuda")with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof: # 指定分析CPU和GPU    with record_function("model_inference"): # 对不同代码段进行标记        model(inputs)print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10)) # 打印分析结果</code></pre><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250310231130.png"></p><h3 id="显存分析">显存分析</h3><p>torch.profiler可以追踪每个算子在执行时分配的显存大小，可以间接用来定位显存峰值出现的位置：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchimport torchvision.models as modelsfrom torch.profiler import profile, record_function, ProfilerActivitymodel = models.resnet18().cuda()inputs = torch.randn(5, 3, 224, 224, device="cuda")with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], profile_memory=True) as prof:    model(inputs)print(prof.key_averages().table(sort_by="self_cuda_memory_usage", row_limit=5))</code></pre> <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250310231453.png"><p></p><p>不过这里的信息比较笼统，只能对每个算子的内存占用有个大致了解，如果想要专门对显存进行优化，则推荐使用PyTorch原生的显存工具torch.cuda.memory._record_memory_history()。(后续讲解)</p><h3 id="可视化性能图谱">可视化性能图谱</h3><p>导出用于可视化分析的文件： </p><pre class="line-numbers language-python" data-language="python"><code class="language-python">prof.export_chrome_trace("profiler_export_trace.json")</code></pre> 导出的文件可以通过PerfettoUI打开浏览：通过在浏览器中打开<a href="https://ui.perfetto.dev/">Perfetto UI</a>，选择“OpenTrace”，然后选择导出的文件即可。 <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250310231914.png"><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250310232334.png"><p></p><h3 id="如何定位性能瓶颈">如何定位性能瓶颈</h3><p>PyTorchProfiler还可以记录调用栈信息，对于将问题定位回Python代码非常有帮助。对于绝大多数场景而言，我们可以通过如下标准流程来查找性能瓶颈：-<strong>(1)观察GPU队列，如果GPU队列整体都非常稀疏，那么性能瓶颈在CPU上。</strong>-<strong>(2)观察GPU队列，如果任务队列密集，而没有显著空白区域，说明GPU满载，那么性能瓶颈在GPU算子。</strong>-<strong>(3)观察GPU队列，如果任务队列密集，同时存在GPU空闲区域，则需要放大空闲区域进行进一步观察。</strong>-<strong>(4)观察GPU空闲区域，查看空闲前后GPU任务以及CPU任务详情，并以此推断导致GPU队列阻塞的原因。</strong></p><blockquote><p>定位瓶颈的示例： <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250310232649.png"></p></blockquote><h2 id="gpu专业分析工具">GPU专业分析工具</h2><p>对于绝大多数性能分析的场景使用PyTorchProfiler就绰绰有余了，然而当需要进行更高级的优化时，我们可能希望拿到更加深入底层的性能数据。在这些情况下，我们需要转向NVIDIA官方提供的更为专业的性能分析工具。因此在本节我们重点介绍NsightSystems和Nsight Compute两种专业分析工具。</p><h3 id="nsight-systems"><a href="https://developer.nvidia.com/nsight-systems">NsightSystems</a></h3><ul><li>NsightSystems是<strong>非侵入式的性能分析工具</strong>，不需要对代码进行任何改动；PyTorchProfiler则需要在代码中添加torch.profiler等函数。</li><li>Nsight Systems能够显示更详细的信息，包括操作系统、CUDAAPI、通信等层面的信息，对多GPU性能分析的支持也更加完善。</li></ul><h3 id="nsight-compute"><a href="https://developer.nvidia.com/nsight-compute">NsightCompute</a></h3><p>Nsight Systems是对PyTorchProfiler的补充，二者还是属于相同层级的分析工具；<strong>NsightCompute则是完全专注于底层GPU内核函数</strong>的性能指标的分析工具。NsightCompute提供的信息多而庞杂，同时收集性能信息的速度非常慢，所以往往只用来分析较小的代码段。具体到训练过程来说，一般只有在优化CUDA算子时才会考虑使用NsightCompute用于定位算子内部的性能瓶颈。</p><p>我们用NsightCompute分析一个完整模型的训练过程，尽管它多用于分析单个算子，代码如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchimport torch.nn as nnimport torch.optim as optimclass SimpleCNN(nn.Module):    def __init__(self):        super(SimpleCNN, self).__init__()        self.conv1 = nn.Conv2d(1, 20, 5)        self.pool = nn.MaxPool2d(2, 2)        self.conv2 = nn.Conv2d(20, 50, 5)        self.fc1 = nn.Linear(50 * 4 * 4, 500)        self.fc2 = nn.Linear(500, 10)    def forward(self, x):        x = self.pool(torch.relu(self.conv1(x)))        x = self.pool(torch.relu(self.conv2(x)))        x = x.view(-1, 50 * 4 * 4)        x = torch.relu(self.fc1(x))        x = self.fc2(x)        return xnet = SimpleCNN().to("cuda")criterion = nn.CrossEntropyLoss()optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)for i in range(10):    inputs = torch.randn(32, 1, 28, 28, device="cuda")    labels = torch.randint(0, 10, (32,), device="cuda")    optimizer.zero_grad()    outputs = net(inputs)    loss = criterion(outputs, labels)    loss.backward()    optimizer.step()</code></pre> 直接用Nsight Compute的图形界面来启动PyTorch程序: <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250310234626.png"><p></p><p>当Nsight Compute完成分析后，首先出现的界面是一个总结界面(summary):<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250310234748.png"></p><p>让我们双击其中任意一个函数名称，进入细节(details)视图，这时我们会看到大量硬件相关的性能信息:<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250311000423.png">首先是最上方的区域，这个区域反映的是执行过程中，GPU不同硬件单元的吞吐量。一般来说吞吐量最高的硬件单元就是该CUDA函数的性能瓶颈，以此可以判断CUDA函数属于计算密集型还是访存密集型。</p><p>具体来说，如果我们观察一个池化函数(pooling)的硬件使用率情况，如图所示，就会发现其计算吞吐量(ComputeThroughput)要远高于访存吞吐量(MemoryThroughput)，说明当前这个池化函数是计算密集型的，但这有些反直觉。从计算特点来看，池化函数每次需要读取一块很大的数据区域，但是对这些数据的计算却比较简单，因此理应是访存密集型为主。从这个例子我们就可以看出NsightCompute带来的价值，它能够定位表现异常的算子、提供性能分析数据，并最终帮助我们完成算子优化：<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250311000651.png"></p><p>接下来让我们将注意力移动到CUDA函数启动信息(LaunchStatistics)部分，这里展示的是每个CUDA函数在执行时的相关配置，比如说格点数量(GridSize)、线程块大小(BlockSize)、总线程数(Threads)等，这部分信息可以用来反映算子是否充分利用了GPU的资源：<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250311000749.png">比如说图中算子的问题就在于配置的格点数量太少，没能充分使用GPU的流式多处理器(SM)。在图中蓝色框的位置处，还可以看到NsightCompute对此给出的提示，GPU支持128个并行的多处理器核心，但是这个CUDA函数只调用了40个，所以还有性能优化的空间。</p><p>最后让我们观察占用率区域(WarpOccupancy)。这部分信息反映了多线程在GPU上执行时，实际的并行程度。比如图中存在的问题是线程使用率不高，理论上可以同时并行48个线程组，实际上只有10个，仅达到了理论并行度的20%。从提示中可以看出，导致该现象的原因可能有两类。一种可能是每个线程中的计算任务过于简单，导致线程束的创建和调度开销要显著大于线程计算任务的开销，所以优化方向是让多个简单线程合并成一个计算量较大的线程。另一种可能性则是线程束的负载不均衡，比如CUDA代码中存在大量线程发散，导致不同分支的线程束无法同步执行——这就需要我们结合CUDA代码进行具体分析。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250311001135.png"></p><h2 id="cpu性能分析工具">CPU性能分析工具</h2><h3 id="py-spy">Py-Spy</h3><p>Py-Spy是非侵入式的，意味着我们不需要修改任何一行Python代码，即可通过如下命令开启CPU分析：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">py-spy record -o profile.svg python train.py</code></pre> Py-Spy可以生成一种名为火焰图(flamegraph)的可视化文件。将上面指令产生的profile.svg文件拖到浏览器中，就可以看到如图所示的火焰图了。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250311002144.png"><p></p><p>火焰图的每个竖条都表示一组调用栈，上面是栈底、下面是栈顶。一般来说Python函数名称会出现在靠近栈底的位置，而栈顶一般是一些底层的C++函数名称。当把鼠标移动到火焰图的某个函数上时，还会在后面显示该函数对应的代码位置以及采样数。采样数本质上和运行时间只差一个系数，这个系数就是采样间隔，默认情况下采样间隔是100samples/s，因此采样数除以100就是函数实际运行的时间了。从图中可以快速找到numpy_heavy_computation对应的运行时间，这就是我们代码中NumPy计算对应的CPU调用。Py-Spy是对所有Python原生程序以及第三方库都适用的分析工具，非常适合用来专门对CPU任务进行分析。</p><h3 id="strace">strace</h3><p>有时候在Py-Spy的火焰图中可以观测到一些函数明显占用了过长的时间，却不知道系统在干什么。这时，使用strace来查看程序与操作系统之间的实时交互，如文件操作、内存管理和网络通信等，通常能带来极大的帮助。<strong>strace是一个在Linux环境中极其实用的诊断和调试工具，它能够追踪并记录程序执行的所有系统调用，包括每个调用的函数名、传递的参数以及返回值。</strong>strace的使用方法也很直接，既可以通过strace启动一个程序，也可以追踪一个正在运行的进程，代码如下所示。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"># 通过strace运行一个程序strace python train.py# 追踪一个正在运行的进程strace -p &lt;pid&gt;</code></pre><p></p><p>在使用strace追踪程序时，有一些常见的与性能相关的系统调用值得我们特殊关注：- 文件相关的系统调用，如open/close/read/write/lseek等 -网络通信相关的系统调用，如socket/bind/listen/send/recv等 -进程控制相关的系统调用，如fork/execve/wait等 -内存管理相关的系统调用，如mmap/munmap/brk等</p><p>strace可以有效地帮助开发者了解程序在运行时的行为，特别是用于诊断程序的性能异常等。</p><h2 id="小结">小结</h2><p>本章主要介绍如何定位性能瓶颈，具体包括三个步骤： -配置一个稳定且可复现的软硬件环境。 -通过计时或观察PyTorch性能图谱来发现性能问题。 -使用底层硬件和系统相关的性能分析工具来剖析问题的根本原因。一旦找到了性能问题并理解其原因，就可以参考第6章中的性能优化方法进行优化。此外，还可以参考第9章中的高级优化方法进行进一步优化。</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250311002659.png"></p>]]></content>
      
      
      <categories>
          
          <category> 《大模型动力引擎》阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《大模型动力引擎》 </tag>
            
            <tag> 性能瓶颈 </tag>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性代数-半正定与正定矩阵</title>
      <link href="/2025/03/10/xian-xing-dai-shu-ban-zheng-ding-yu-zheng-ding-ju-zhen/"/>
      <url>/2025/03/10/xian-xing-dai-shu-ban-zheng-ding-yu-zheng-ding-ju-zhen/</url>
      
        <content type="html"><![CDATA[<h2 id="基本定义">基本定义</h2><blockquote><p>正定矩阵：一个 n×n 的实对称矩阵 <span class="math inline">\(A\)</span>，如果对于任何非零向量 <span class="math inline">\(x \in \mathbb{R}^n\)</span>，都满足：<span class="math inline">\(x^TAx &gt; 0\)</span>，则称 <span class="math inline">\(A\)</span> 为正定矩阵。</p></blockquote><blockquote><p>半正定矩阵：一个 n×n 的实对称矩阵 <span class="math inline">\(A\)</span>，如果对于任何向量 <span class="math inline">\(x \in \mathbb{R}^n\)</span>，都满足：<span class="math inline">\(x^TAx \geq 0\)</span>，则称 <span class="math inline">\(A\)</span> 为半正定矩阵。</p></blockquote><h3 id="例1">例1</h3><p>单位矩阵 <span class="math inline">\(I \in \mathbb{R}^{2 \times2}\)</span> 是否是正定矩阵？</p><p>解：设向量 <span class="math inline">\(\boldsymbol{x} =\begin{bmatrix}x_1\\x_2\end{bmatrix} \in \mathbb{R}^2\)</span>为非零向量，则 <span class="math display">\[\boldsymbol{x}^T I\boldsymbol{x} = \boldsymbol{x}^T\boldsymbol{x} =x_1^2 + x_2^2\]</span> 由于 <span class="math inline">\(\boldsymbol{x} \neq\boldsymbol{0}\)</span>，故 <span class="math inline">\(\boldsymbol{x}^TI\boldsymbol{x} &gt; 0\)</span> 恒成立，即单位矩阵 <span class="math inline">\(I \in \mathbb{R}^{2 \times 2}\)</span>是正定矩阵。</p><p>单位矩阵是正定矩阵 (positive definite)。</p><h4 id="简单证明">简单证明</h4><p>对于任意单位矩阵 <span class="math inline">\(I \in \mathbb{R}^{n\times n}\)</span> 而言，给定任意非零向量 <span class="math inline">\(\boldsymbol{x} \in \mathbb{R}^n\)</span>，恒有<span class="math display">\[\begin{align*}\boldsymbol{x}^T I\boldsymbol{x} &amp;= \boldsymbol{x}^T\boldsymbol{x}\\&amp;= x_1^2 + x_2^2 + \cdots + x_n^2 &gt; 0\end{align*}\]</span></p><h3 id="例2">例2</h3><p>实对称矩阵 <span class="math inline">\(A =\begin{bmatrix}2&amp;-1&amp;0\\-1&amp;2&amp;-1\\0&amp;-1&amp;2\end{bmatrix}\in \mathbb{R}^{3 \times 3}\)</span> 是否是正定矩阵？</p><p>解：设向量 <span class="math inline">\(\boldsymbol{x} =\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix} \in \mathbb{R}^3\)</span>为非零向量，则 <span class="math display">\[\begin{align*}\boldsymbol{x}^T A\boldsymbol{x} &amp;= \begin{bmatrix}(2x_1 -x_2)&amp;(-x_1 + 2x_2 - x_3)&amp;-x_2 +2x_3\end{bmatrix}\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}\\&amp;= x_1^2 + (x_1 - x_2)^2 + (x_2 - x_3)^2 + x_3^2 &gt; 0\end{align*}\]</span> 因此，矩阵 <span class="math inline">\(A\)</span>是正定矩阵。</p><h2 id="结合二次函数的解释">结合二次函数的解释</h2><p>在初中我们学过二次函数，形如 <span class="math inline">\(f(x) =ax^2\)</span>，该函数的曲线会经过坐标原点，且开口向上，所有的函数值都大于等于0。</p><p>实际上，我们可以将 <span class="math inline">\(y=x^TAx\)</span>看作是二次函数 <span class="math inline">\(f(x) = ax^2\)</span>的多维推广。</p><p>因此，如果希望 <span class="math inline">\(y=x^TAx\)</span>对所有向量 <span class="math inline">\(x\)</span> 都大于等于0，则需要<span class="math inline">\(A\)</span> 为半正定矩阵。</p><p>另外在<span class="math inline">\(y=ax^2\)</span>中，若<span class="math inline">\(a&gt;0\)</span>，则对于任意<span class="math inline">\(x!=0\)</span>，有<span class="math inline">\(y&gt;0\)</span>恒成立。</p><p>同样，如果希望 <span class="math inline">\(y=x^TAx\)</span>对所有非零向量 <span class="math inline">\(x\)</span> 都大于0，则需要<span class="math inline">\(A\)</span> 为正定矩阵。</p><h2 id="直观解释">直观解释</h2><blockquote><p>若给定任意一个正定矩阵<span class="math inline">\(A\in\mathbb{R}^{n\times n}\)</span> 和一个非零向量<span class="math inline">\(x\in\mathbb{R}^n\)</span>，则两者相乘得到的向量<span class="math inline">\(y=Ax \in \mathbb{R}^n\)</span> 与向量<span class="math inline">\(x\)</span> 的夹角恒小于等于90度。（等价于<span class="math inline">\(x^TAx&gt;0\)</span>）</p></blockquote><h3 id="例3">例3</h3><p>给定向量 <span class="math inline">\(\boldsymbol{x} =\begin{bmatrix}2\\1\end{bmatrix} \in \mathbb{R}^2\)</span>，对于单位矩阵<span class="math inline">\(I =\begin{bmatrix}1&amp;0\\0&amp;1\end{bmatrix} \in \mathbb{R}^{2 \times2}\)</span>，则 <span class="math display">\[\boldsymbol{y} = I\boldsymbol{x} = \boldsymbol{x} =\begin{bmatrix}2\\1\end{bmatrix}\]</span> 向量 <span class="math inline">\(\boldsymbol{x},\boldsymbol{y} \in \mathbb{R}^2\)</span> 之间的夹角为 <span class="math display">\[\begin{align*}\cos\langle\boldsymbol{x}, \boldsymbol{y}\rangle &amp;=\frac{\boldsymbol{x}^T\boldsymbol{y}}{\|\boldsymbol{x}\|\cdot\|\boldsymbol{y}\|}\\&amp;= \frac{2\times2 + 1\times1}{\sqrt{2^2 + 1^2}\cdot\sqrt{2^2 +1^2}}\\&amp;= 1\end{align*}\]</span> 即两个向量之间的夹角为 <span class="math inline">\(0^{\circ}\)</span>。</p><h3 id="例4">例4</h3><p>给定向量 <span class="math inline">\(\boldsymbol{x} =\begin{bmatrix}2\\1\\1\end{bmatrix} \in\mathbb{R}^3\)</span>，对于实对称矩阵 <span class="math inline">\(A =\begin{bmatrix}2&amp;-1&amp;0\\-1&amp;2&amp;-1\\0&amp;-1&amp;2\end{bmatrix}\in \mathbb{R}^{3 \times 3}\)</span>，则 <span class="math display">\[\boldsymbol{y} = A\boldsymbol{x} = \begin{bmatrix}0\\2\\0\end{bmatrix}\]</span> 向量 <span class="math inline">\(\boldsymbol{x},\boldsymbol{y} \in \mathbb{R}^3\)</span> 之间的夹角为 <span class="math display">\[\cos\langle\boldsymbol{x}, \boldsymbol{y}\rangle =\frac{\boldsymbol{x}^T\boldsymbol{y}}{\|\boldsymbol{x}\|\cdot\|\boldsymbol{y}\|}= \frac{\sqrt{6}}{3}\]</span> 即两个向量之间的夹角小于 <span class="math inline">\(\frac{\pi}{2}\)</span>。</p><h2 id="为什么协方差矩阵是半正定矩阵">为什么协方差矩阵是半正定矩阵</h2><p>在概率论与数理统计中，协方差矩阵被定义为：</p><blockquote><p>对于任意多元随机变量 <span class="math inline">\(\boldsymbol{t}\)</span>，协方差矩阵为 <span class="math inline">\(C = \mathbb{E}[(\boldsymbol{t} -\overline{\boldsymbol{t}})(\boldsymbol{t} -\overline{\boldsymbol{t}})^T]\)</span></p></blockquote><p>现给定任意一个向量 <span class="math inline">\(\boldsymbol{x}\)</span>，则 <span class="math display">\[\begin{align*}\boldsymbol{x}^T C\boldsymbol{x} &amp;=\boldsymbol{x}^T\mathbb{E}[(\boldsymbol{t} -\overline{\boldsymbol{t}})(\boldsymbol{t} -\overline{\boldsymbol{t}})^T]\boldsymbol{x}\\&amp;= \mathbb{E}[\boldsymbol{x}^T(\boldsymbol{t} -\overline{\boldsymbol{t}})(\boldsymbol{t} -\overline{\boldsymbol{t}})^T\boldsymbol{x}]\\&amp;= \mathbb{E}(s^2) = \sigma_s^2\end{align*}\]</span></p><p>其中， <span class="math inline">\(\sigma_s =\boldsymbol{x}^T(\boldsymbol{t} - \overline{\boldsymbol{t}}) =(\boldsymbol{t} -\overline{\boldsymbol{t}})^T\boldsymbol{x}\)</span></p><p>由于 <span class="math inline">\(\sigma_s^2 \geq0\)</span>，因此，<span class="math inline">\(\boldsymbol{x}^TC\boldsymbol{x} \geq 0\)</span>，协方差矩阵 <span class="math inline">\(C\)</span> 是半正定的。</p><p>参考</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/44860862">浅谈「正定矩阵」和「半正定矩阵」</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 线性代数 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线性代数 </tag>
            
            <tag> 正定/半正定矩阵 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型动力引擎-深度学习必备的PyTorch</title>
      <link href="/2025/03/07/da-mo-xing-dong-li-yin-qing-shen-du-xue-xi-bi-bei-de-pytorch/"/>
      <url>/2025/03/07/da-mo-xing-dong-li-yin-qing-shen-du-xue-xi-bi-bei-de-pytorch/</url>
      
        <content type="html"><![CDATA[<p>本章将从PyTorch的核心概念——张量和算子讲起，逐步深入PyTorch的内存分配、基于动态图的运行机制，以及作为训练框架的杀手锏级特性——自动微分系统的底层原理</p><h2 id="pytorch的张量数据结构">PyTorch的张量数据结构</h2><p>Pytorch通过张量(torch.Tensor)作为数据容器提供了统一的方式来处理不同维度和形状的数据(标量、向量、矩阵、张量)。</p><h3 id="张量的基本属性及创建">张量的基本属性及创建</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchx = torch.tensor((3,2), dtype=torch.float32, device=torch.device('cuda'))print(x.dtype)   # torch.float32print(x.device)  # cuda:0print(x.shape)   # torch.Size([3,2])</code></pre><h3 id="访问张量的数据">访问张量的数据</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 创建一个10行20列的连续张量, 并使用contiguous()方法确保张量在内存中是连续的x = torch.arange(200).reshape(10,20).contiguous()# 访问张量中的数据print(x[0, 0])  # tensor(0)print(x[0, -1]) # tensor(19)print(x[2, :])  # tensor([40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])print(x[0, 1:9:3]) # tensor([1, 4, 7])print(x[..., 1])   # tensor([1, 21, 41, 61, 81, 101, 121, 141, 161, 181])print(x[:, None, :]) # 使用None 插入一个新维度，返回一个形状为(10, 1, 20)的张量</code></pre><h3 id="张量的存储方式">张量的存储方式</h3><p>torch.Storage是用于表示数据在物理内存中的存储方式，其实就是一块连续的一维内存空间。每个<strong>torch.Storage对象负责维护存储数据的类型和总长度信息</strong>。在此基础上，<strong>torch.Tensor添加了如形状(shape)、步长(stride)和偏移量(offset)等额外的属性，定义了torch.Tensor访问底层数据的具体方式</strong>。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250307170840.png"></p><p><strong>步长</strong>指定了在遍历张量数据时，必须在内存中跳过多少元素才能到达下一个元素。比如我们可以通过改变步长属性来实现一个高效的张量转置操作: </p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torch# 创建一个3*4的张量, 使用contiguous()确保其连续性x = torch.arange(12).reshape(3, 4).contiguous()print(f"x = {x}\nx.stride = {x.stride()}")# x = tensor([[ 0,  1,  2,  3],#         [ 4,  5,  6,  7],#         [ 8,  9, 10, 11]])# x.stride = (4, 1)y = torch.as_strided(x, size=(4, 3), stride=(1, 4))print(f"y = {y}\ny.stride = {y.stride()}")# y = tensor([[ 0,  4,  8],#         [ 1,  5,  9],#         [ 2,  6, 10],#         [ 3,  7, 11]])# y.stride = (1, 4)# 张量x和y共享同一块底层存储assert id(x.untyped_storage()) == id(y.untyped_storage())</code></pre><strong>as_strided函数</strong>的作用由两方面构成：一方面它重新规定了x张量的访问方式，将其步长从(4,1)改为了(1, 4)，形状为(4,3)。这意味着在遍历x的数据时，在第二个维度上每访问一个数据会向后跳四步再访问下一个数据，而在第一个维度上则每访问一个数据向后跳一步。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250307172705.png"><p></p><p>使用stride属性来访问张量是非常高效的，因为它无需复制，直接操作同一个张量的底层数据存储。但它也是把双刃剑，因为会导致“<strong>张量不连续</strong>”​。其次许多PyTorch算法在设计时就预设了张量在内存中是连续存储的，如果遇到不连续的张量，可能会抛出错误提示甚至得到错误的计算结果。PyTorch提供了<strong>tensor.is_contiguous()</strong>方法用于检测张量是否为连续。对于不连续的张量，可以通过调用<strong>tensor.contiguous()</strong>方法生成一个连续的副本。然而天下没有免费的午餐，这个调用会将原始数据复制到一块新的连续内存空间，增加内存占用，因此读者在开发时需要时刻注意内存和性能之间的平衡。</p><h3 id="张量的视图">张量的视图</h3><p>不同的张量可以共享同一块底层存储，当这些共享存储的张量互不重叠时，影响较小。但如果它们<strong>不仅共享底层存储，还存在重叠，我们称其中一个张量为另一个张量的视图(view)</strong>。</p><p>以下是pytorch中常见的几种<strong>视图操作</strong>，它们都共享同一块底层存储，但返回的张量形状可能不同。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250307174015.png">基于<strong>基础索引的张量读取操作</strong>返回的也是视图：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchx = torch.zeros(3,3)y = x[0]y[0] = 1print(f"x = {x}\ny = {y}")# x = tensor([[1., 0., 0.],#         [0., 0., 0.],#         [0., 0., 0.]])# y = tensor([1., 0., 0.])</code></pre><p></p><p>注意一些接口如 <strong>reshape()和flatten()</strong>的行为较为特殊，它们根据具体的使用场景返回一个视图张量或一个全新内存的张量。这种行为的不确定性导致这些接口并不是最理想的API设计。</p><h2 id="pytorch中的算子">Pytorch中的算子</h2><h3 id="算子库">算子库</h3><p>PyTorch提供了大量用于构建神经网络的算子，这些算子可以分为以下几类： -(1)基础数学运算：涵盖了加法(+)、减法(-)、 **乘法(*/mul)<strong>、除法(/)、指数(exp)、幂次方(pow)等基本数学操作。（</strong>element-wise操作<strong>）-(2)线性代数运算：包括矩阵乘法(</strong>matmul/@)**、点乘(dot)、转置(t)、逆矩阵(inverse)等线性代数相关的运算。-(3)逻辑和比较运算：例如逻辑与(logical_and)、逻辑或(logical_or)、等于(eq)、大于(gt)、小于(lt)等用于比较和逻辑判断的操作。-(4)张量操作：涉及张量的索引、切片、拼接(cat)、调整形状(reshape)、调整维度(permute)等操作，用于张量的形状和内容调整。-(5)其他特殊运算：包括深度学习中使用的各种层（如卷积层、池化层、注意力层）以及损失函数等特定于应用的复杂运算。</p><p><strong>矩阵乘法</strong>矩阵乘法是深度学习中非常常见的操作，PyTorch提供了多种矩阵乘法运算符，包括：- dot：用于计算两个1D向量之间的点积，返回一个标量。 -mm：用于计算两个2D矩阵之间的矩阵乘积。 -matmul：用于计算两个张量之间的矩阵乘积。(1D时等价dot，2D时等价mm) -bmm：用于批量计算矩阵乘积。</p><h3 id="pytorch算子的返回值内存分配">Pytorch算子的返回值内存分配</h3><p>PyTorch算子操作的输入和返回值都是张量，但返回值是否创建新的内存取决于具体的算子。#### 原地操作PyTorch也为一些算子提供了原位(inplace)操作，它们直接修改输入张量的数据并返回同一个张量，无须创建新的内存。- 原位操作通常在方法名后加下划线<strong>(_)表示，例如add_()是add()的原位版本</strong>。 -某些语法也会隐式地触发原位操作。比如x+=y将触发原位加法操作，而x = x +y就只是普通的加法操作和赋值操作。</p><h4 id="视图操作">视图操作</h4><p>输出张量与输入张量共享底层内存，因此不会造成额外的内存分配。</p><h4 id="读取操作">读取操作</h4><ul><li>基础索引操作：返回的是视图。</li><li>高级索引操作：类似于NumPy的高级索引，也就是使用布尔或者整数张量作为索引。基于高级索引的读取操作会创建新的内存存储。<pre class="line-numbers language-python" data-language="python"><code class="language-python">import torch# 创建一个10*20的张量, 使用contiguous()确保其连续性x = torch.arange(200).reshape(10, 20).contiguous()# 基础索引，读取x的第0行y_basic_index = x[0]# (1) 基于基础索引进行读取的返回张量和x共享底层存储assert y_basic_index.data_ptr() == x.data_ptr()# 使用整数张量对x进行高级索引，返回位置在[0, 2], [1, 3], [2, 4]位置的元素z_adv_index_int = x[torch.tensor([0, 1, 2]), torch.tensor([2, 3, 4])]# z_adv_index_int = tensor([ 2, 23, 44])# 对张量x中的每个元素进行判断，如果元素的值小于10，则对应位置的ind为True，否则为Falseind = x &lt; 10# 使用布尔张量对x进行高级索引，返回x中所有对应ind位置为True的元素z_adv_index_bool = x[ind]# z_adv_index_bool = tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])# (2) 基于高级索引进行读取的返回张量和x的底层存储是分开的assert z_adv_index_int.data_ptr() != x.data_ptr()assert z_adv_index_bool.data_ptr() != x.data_ptr()</code></pre> #### 赋值操作张量赋值操作是指使用基础索引、高级索引、广播等方式将新的值赋给张量的特定位置。赋值操作直接修改输入张量的内容，没有返回值。<pre class="line-numbers language-python" data-language="python"><code class="language-python">import torch# 创建一个10*20的张量, 使用contiguous()确保其连续性x = torch.arange(200).reshape(10, 20).contiguous()# 对张量x中的每个元素进行判断，如果元素的值小于10，则对应位置的ind为 True，否则为Falseind = x &lt; 10# 通过高级索引对x的部分元素进行赋值x[ind] = 1.0print(x)  # x的对应位置也被更新成1.0</code></pre> ### 算子的调用 以下面代码为例分析一下PyTorch中y = x1 @x2在整个调用中大致经历了哪些过程： <pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchx1 = torch.rand(32, 32, dtype=torch.float32, device="cuda:0")x2 = torch.rand(32, 32, dtype=torch.float32, device="cuda:0")y = x1 @ x2</code></pre></li><li>(1)函数入口：这个表达式会首先调用Python中Tensor类的__matmul__方法作为“矩阵乘法”算子的入口。</li><li>(2)定位算子：PyTorch核心的<strong>分发系统(dispatcher)</strong>会根据算子类型、输入张量的数据类型、存储后端来找到可以承担该算子计算的<strong>底层算子实现</strong>。比如这个例子中，分发系统找到GPU上float32类型矩阵乘法计算对应的CUDA函数实现。</li><li>(3)创建张量：创建所需的输出张量。</li><li>(4)底层调用：调用我们找到的算子函数，进行类型转换、输出张量的创建等必要步骤，计算并将结果写入输出张量。</li><li>(5)函数返回：创建输出张量的Python对象并返回给用户。 <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250307190932.png"></li></ul><p>图3-5展示了在PyTorch调用一个矩阵乘法算子的调用栈，这里面最核心的步骤是底层调用也就是算子计算，但是前后还做了一系列准备工作，这些准备工作统一称为<strong>调用延迟</strong>。虽然少数算子的调用延迟可以接受，但如果频繁调用算子，则累积起来的总调用延迟就不能忽视了。后续通过<strong>CUDA Graph</strong>降低调用延迟。</p><p>在日常开发中应该紧绷一根弦，尽量减少不必要的操作，如<strong>能对张量整体进行操作的时候尽量避免手动操作数组中的单个元素</strong>。因为单个数组元素的读取和赋值都是一次算子调用。比如对于张量加和运算，如果通过在Python中手写循环来完成“读取单个元素→加和→存储回张量”这个过程，所需要的计算时间要远远高于直接使用张量的加法操作。这是因为张量的加法和归约操作能够充分地利用GPU的并行计算能力，在性能上会显著优于对单个张量元素进行的串行操作。</p><h2 id="pytorch动态图机制">Pytorch动态图机制</h2><h3 id="计算图">计算图</h3><p>计算图是深度学习框架中用于描述和执行计算过程的抽象模型：一个有向无环图，其中的节点(node)代表各种算子操作，比如加法、乘法或更复杂的操作如卷积等，而边(edge)则代表数据（指张量数据）的流动。### 动态图与静态图的区别 - 动态图：运行时构建 </p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchx = torch.tensor(2)  # 可以尝试不同的值，如 torch.tensor(1.0)y = x % 2if y == 0:    z = x * 10else:    z = x + 10print(z)</code></pre> -静态图：运行前构建，运行时不能修改(TensorFlow 早期版本) <pre class="line-numbers language-python" data-language="python"><code class="language-python">import tensorflow.compat.v1 as tfx = tf.placeholder(tf.float32, shape=())def true_fn():    return tf.multiply(x, 10)def false_fn():    return tf.add(x, 10)y = x % 2z = tf.cond(tf.equal(y, 0), true_fn, false_fn)with tf.Session() as sess:    print(sess.run(z, feed_dict={x: 2}))  # 输出 10 (2 * 10)    print(sess.run(z, feed_dict={x: 1}))  # 输出 11 (1 + 10)</code></pre>可以注意到，TensorFlow1.0的代码逻辑需要完全由TensorFlow的接口拼接而成，与原生的Python代码写法有很大差别——写的虽然是Python语言，但是却不那么“Pythonic”​。除此以外，代码中的张量y、z在很长一段时间里都只是单纯的符号，没有具体的数值，也因此没有办法打印出来。这个情况一直持续到在TensorFlow的会话(tf.Session)中，<strong>通过sess.run()执行构建出来的计算图。计算图一旦被执行后才会往y、z中填入数值</strong>。但是这时候计算图已经完全固定下来了，后续不能再继续对x、y、z进行任何修改了。<p></p><h4 id="差别">差别</h4><ul><li>(1)执行方式：静态图有明确图的定义和图的运行两个阶段。而动态图则是在定义的同时就执行，立即得到结果。例如，在TensorFlow中，y= x %2只是向计算图中添加一个运算节点，该运算不会立即执行。但在动态图中，这个语句在添加节点的同时也执行了该运算。</li><li>(2)数据表示：在静态图的定义阶段，x、y、z都是符号，不含具体数据。只有在运行时，我们才对输入x赋值。而在PyTorch的代码中，x、y、z一开始就是带有具体数据的张量。</li><li>(3)中断与调试：静态图一旦运行就不能中断。要在静态图中打印中间变量y的值进行调试，<strong>需要插入tf.Print()语句并重新运行图</strong>。但是在PyTorch中，执行过程可以中断，比如可以用importpdb; pdb.set_trace()使运行暂停，并可以自由地打印或修改张量的内容。</li><li>(4)代码执行：在TensorFlow1.0中，计算图的执行全都是由TensorFlow的底层运行处理的——包括print语句和条件语句在内。而在PyTorch代码中，条件语句和print语句是由Python解释器执行的，只有与张量相关的操作是由PyTorch的运行处理的。这种设计保持了Python作为解释型语言的灵活性，从而可以支持动态修改代码和交互式编程。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250307204736.png"></li></ul><p>由于静态图在执行前就获得了完整的图信息，使得它能够应用更复杂的优化策略，如移除无用操作、进行跨操作优化，甚至执行算子融合等。对于追求极致性能的部署工程师来说，像TensorFlow1.0这样的框架仍然是首选之一。而对于研究人员，快速迭代和易于调试的特性使得PyTorch具有显著优势。</p><h2 id="pytorch-的自动微分系统">Pytorch 的自动微分系统</h2><h3 id="自动微分">自动微分</h3><ul><li><p>基于有限差分的数值微分法：通过对输入变量添加一个微小扰动，比如设定h=0.000001，然后观察输出的变化来近似计算梯度值。精度不高，但是实现简单。</p></li><li><p>基于符号微分的梯度求导公式：基于封闭形式表达式(closed-formexpression)的梯度求导公式。但是对于程序中涉及基于动态输入或条件变化的循环、复杂递归、动态内存分配及涉及大量非线性数据处理等复杂逻辑，将其转化为封闭形式通常是不可能的。因此，尽管符号微分在计算效率上非常高，其适用性却受到很大限制。</p></li></ul><p>而自动微分(automaticdifferentiation)则是一种介于数值微分和符号微分之间的方法： -(1)它定义了一系列的“基本操作”​（如加、减、乘、除）​，并且根据手动推导的结果定义了这些操作的梯度。例如，在手动推导z=y×x的微分形式时，我们可以得到<span class="math inline">\(\frac{\partial z}{\partial x} = y\)</span>类似这样的基础操作梯度公式会被硬编码在自动微分系统中，是它的核心组成元素。-(2)在程序运行时，自动微分系统会基于链式法则将复杂运算拆解成基础操作的组合，一步步计算所有中间结果的梯度，并最终计算出输入参数的梯度。注意在运算过程中对于同一个张量的梯度是累加而不是覆写的。而且这里累加的是具体数值而非符号表达式，这一点至关重要，它使得自动微分系统能够自然地兼容程序中出现的逻辑判断，如分支、循环和递归等，而这对符号微分系统是非常困难的。</p><h3 id="自动微分的实现">自动微分的实现</h3><p>PyTorch的自动微分系统中默认使用<strong>反向微分模式</strong>。它以某个输出张量的梯度作为起点，反向逐层计算出每个输入参数对应的梯度，计算图的执行次数与输出张量的数量有关——M个输出张量就需要执行M次计算图。绝大多数深度学习模型训练的场景都是有w1,w2,...,wN个模型参数，但只有数个甚至一个损失函数(loss)。</p><p>还有一种<strong>前向模式的自动微分</strong>。它以某个输入参数的梯度作为起点，向前逐层计算出每个输出张量对应的梯度，计算图的执行次数与输入参数的数量有关——N个输入参数就需要执行N次计算图。所以前向微分适合输入参数少而输出张量多的场景，比如模型只有一个输入参数w，但是输出M个loss的情况，这一般多见于科学计算相关的场景，尤其在计算高阶导数时。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250307211040.png"></p><p>PyTorch自动微分机制依赖于torch.Tensor上的两个额外属性，即grad和requires_grad。- grad：存储了张量对应的梯度值，初始值为None。 -requires_grad：表示是否需要计算梯度。</p><h3 id="示例">示例：</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torch# 创建一个需要计算梯度的张量x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)# 前向传播：# 1. 构建并执行前向图# 2. 构建反向图t = x * 10z = t * tloss = z.mean()# 反向传播，计算梯度loss.backward()# 查看x的梯度print(x.grad)</code></pre><p>注意每一个算子在前向调用时，就会当场在反向图中构建一个反向算子，前向计算图是当场构建、当场执行的，但反向计算图则是当场构建、延迟执行的——直到我们调用loss.backward()时才会开始执行反向图。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250307212745.png"></p><p>需要注意梯度是累加的，如果多次调用backward()，每次调用计算出的梯度也会累积到对应张量的grad属性中，这也是为什么我们需要在每轮训练循环开始前调用optimizer.zero_grad()来手动清零梯度的原因。</p><h3 id="autograd扩展自定义算子">Autograd扩展自定义算子</h3><p>在实际开发过程中，当我们遇到需要使用一些非标准或特殊的数学操作，而这些操作又不在PyTorch库的支持范围内时，可以利用PyTorch自动微分系统提供的扩展模块来自定义新的算子。新定义的操作能够像PyTorch中的任何其他操作一样被使用，并能自然而然地融入PyTorch的自动微分机制中。</p><p>举例来说，假如我们要实现一个计算 <span class="math inline">\(input1\times input1 \times input2\)</span> 的算子，其实现方法如下所示：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchclass MyMul(torch.autograd.Function):    @staticmethod    def forward(ctx, input1, input2):        ctx.save_for_backward(input1, input2)        return input1 * input1 * input2    @staticmethod    def backward(ctx, grad_output):        input1, input2 = ctx.saved_tensors        grad_input1 = grad_output * 2 * input1 * input2        grad_input2 = grad_output * input1 * input1        return grad_input1, grad_input2# 使用自定义的乘法操作x = torch.tensor([2.0, 3.0], requires_grad=True)y = torch.tensor([3.0, 4.0], requires_grad=True)z = MyMul.apply(x, y)z.backward(torch.tensor([1.0, 1.0]))print(f"x.grad={x.grad}, y.grad={y.grad}")# x.grad=tensor([12., 24.]), y.grad=tensor([4., 9.])</code></pre><p></p><h3 id="pytorch的异步执行">Pytorch的异步执行</h3><p>PyTorch支持不同的计算后端，模型训练主要依赖其中的CPU和GPU后端。在使用PyTorch的不同计算后端时，主要会影响以下三个方面： -<strong>张量的存储位置</strong>。 - <strong>算子的执行硬件</strong>。 -<strong>执行机制</strong>。</p><p>前两者比较容易理解：使用CPU后端时张量存放在内存中，算子在CPU上执行；而使用GPU后端时张量存放在显存里，算子在GPU上执行。但是第三点“执行机制”的变化则更为复杂。 #### CPU后端执行机制每一个Python指令都对应一个CPU算子任务。每个算子在CPU上完全执行完毕，得到输出结果后，才跳转到下一条Python指令，开始执行后续的算子任务。这种执行机制被称为<strong>同步执行机制</strong>。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250307224514.png">#### GPU后端执行机制PyTorch也只会把核心的算子计算任务放在GPU上，而类型提升、输出信息推导、输出张量的创建、定位算子等任务依然还留在CPU上。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250307224705.png"></p><p>GPU内部维护了一系列任务队列(stream)，CPU会将算子任务（一般是一个CUDA函数）提交到GPU的任务队列上，之后就可以撒手不管了，GPU会自行从任务队列中依次拿出计算任务然后执行。不仅如此，CPU将算子任务提交给GPU之后，不会等GPU完成计算，而是直接返回并开始执行下一条Python指令。这种执行机制被称为<strong>异步执行机制</strong>。 <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250307225018.png">#### 运行时间陷阱如果我们在CPU任务结束后立即停止计时，并惊讶地发现程序运行得出奇地快，这就不免中了CPU的圈套。如图3-13所示，当我们打印出“CPUFinished”的时候，GPU还在后台默默地负重前行呢。 上述CPU -GPU协同工作机制被称为异步执行机制，其核心特点在于CPU提交任务给GPU后，不等待GPU任务完成而直接返回，继续执行下一个CPU任务或下一条Python代码。然而在很多任务中我们还是希望等待GPU完成计算的，包括而不限于测试程序的运行时间、打印计算结果等。这时我们就需要手动调用PyTorch提供的CPU-GPU同步接口，比如<strong>torch.cuda.synchronize()</strong>，如图3-14所示。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250307225226.png"></p><p>除了来自用户的手动显式调用同步操作，PyTorch的一些操作也会隐式地调用同步操作。比如，当我们调用torch.nn.functional.softmax()时，PyTorch会自动将计算任务提交给GPU，并隐式地调用torch.cuda.synchronize()。</p>]]></content>
      
      
      <categories>
          
          <category> 《大模型动力引擎》阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLMs </tag>
            
            <tag> 《大模型动力引擎》 </tag>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型动力引擎-深度学习必备硬件知识</title>
      <link href="/2025/03/06/da-mo-xing-dong-li-yin-qing-shen-du-xue-xi-bi-bei-ying-jian-zhi-shi/"/>
      <url>/2025/03/06/da-mo-xing-dong-li-yin-qing-shen-du-xue-xi-bi-bei-ying-jian-zhi-shi/</url>
      
        <content type="html"><![CDATA[<h2 id="gpu与内存">GPU与内存</h2><p>深度学习模型的基础计算单元是"算子"，本质上是将输入映射为输出的计算过程，对于古老的CPU-内存体系来说，算子遵循一下步骤：- 从内存中读取输入数据 - 对独到的数据调用CPU指令，完成算子计算 -将计算结果写回内存</p><h3 id="内存">内存</h3><p>通常指的是随机访问存储器（RAM），“随机”意味着访问任何位置的数据所需的时间是相同的，与位置无关。RAN可以分为两类： -静态随机访问存储器（SRAM）：基于双稳态触发器（bistableflip-flop），读取时不会丢失数据（<strong>非破坏性读取</strong>）。常用于Cache。-动态随机访问存储器（DRAM）：基于电容存储，读取数据会丢失数据，需要重写（<strong>破坏性读取</strong>，需刷新）。常用于主存。</p><p>不管是SRAM还是DRAM，都是临时存储，断电后数据会丢失的，数据不会保留（<strong>易失性</strong>）。</p><p>在选用内存时往往会看到DDR4、DDR5等字样，指的是基于DDR的第几代产品，涉及技术标准、芯片结构及控制算法的升级，但是要看主板是否支持最新一代内存规格。</p><h3 id="cpu">CPU</h3><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250306190238.png">CPU的核心结构包括负责计算的算数逻辑单元(ALU)、负责加速数据读写速度的多级缓存，在此基础上还会采用CPU多核心设计来增加并行度。</p><h2 id="硬盘">硬盘</h2><p>与RAM不同，硬盘是长期存储设备，断电后数据不会丢失，数据会保留（<strong>非易失性</strong>）。一般常用的硬盘有机械硬盘HDD和固态硬盘SSD。SSD的读写速度远高于HDD，但是价格也更高。</p><p>硬盘的读写行为大致分为两种模式： -随机读写模式：高频率读写小规模数据。IOPS（Input/Output Operations PerSecond）是衡量硬盘随机读写速度的指标。 -连续读写模式：需要读取大文件或顺序访问数据。MB/s（Megabytes persecond）是衡量硬盘连续读写速度的指标。</p><p>其中连续读写速度有硬盘连续读写速度和数据传输速度共同决定。随着硬盘读取效率的不断提高，读写效率的瓶颈慢慢开始出现在了数据传输阶段，所以在SATA固态硬盘的基础上，又发展出了NVMe固态硬盘。<strong>NVMe固态硬盘使用传输效率更高的PCIe传输通道</strong>，在读写性能上往往超出SATA固态硬盘许多。</p><h2 id="显卡">显卡</h2><h3 id="cpu局限性">CPU局限性</h3><p>CPU可以“全能”的处理各种指令（计算指令、访存指令，也能够处理好跳转等逻辑指令），但在处理计算密集型任务时，CPU的计算速度不如GPU或其他异构芯片高效。</p><p>因此针对计算密集型的额任务，应该让CPU处理复杂的交互逻辑，而采用一个专门的芯片处理密集的计算，这需要我们在软件层面上对程序任务进行划分，将一个程序涉及的所有任务都分为两种类型：- 外围任务：业务代码、API接口、用户交互等逻辑复杂的任务。 -内核任务：高度内聚的计算密集型任务，逻辑分支很少。</p><p>而GPU可以处理密集的计算，例如一个3D游戏，其中用户交互、用户界面(UI)、音频等部分由CPU负责处理，而图形渲染和物理仿真等核心计算任务则由GPU执行。在深度学习领域，核心的计算任务被封装成算子并在GPU上执行，而复杂的逻辑调度、数据搬运等任务则由CPU完成。</p><h3 id="gpu硬件结构">GPU硬件结构</h3><p>GPU的计算核心并不能直接从内存读取数据，于是在二者之间又加入了<strong>显存(VRAM)</strong>这个额外的存储元件。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250306193624.png"></p><p>GPU自顶向下可以分为以下几个部分： - 最外层：显存、L2缓存 -高层封装单元：<strong>流式多处理器(SM)</strong> - L1缓存 -多个<strong>流式处理器(SP)</strong>，共享一个SM的L1缓存，每个SP包括： -若干标量计算核心(<strong>CUDA core</strong>) -若干张量计算核心(<strong>tensor core</strong>) - 若干寄存器 -线程束调度器(<strong>warp scheduler</strong>)</p><p>这里面真正重要的是图中标记为蓝色的几个硬件单元。其中张量计算核心、标量计算核心决定了GPU的整体计算效率；L1缓存、显存决定了GPU存取数据的效率；线程束调度器则与CUDA编程模型中的线程束(warp)概念直接对应，负责线程间的通信和调度。</p><p>A100（Ampere架构）中的SM如下图： <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250306195211.png"></p><p>A100整体架构图： <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250306195334.png"></p><h3 id="gpu编程模型及其硬件对应">GPU编程模型及其硬件对应</h3><p>Python、C++等程序能够在计算机上运行，是因为它们通过编译器转换成硬件能识别的指令，从而在CPU上执行。基于这一逻辑，如果存在一个能将C++或Python编译成GPU指令的编译器，我们就可以直接在GPU上运行这些代码。然而，遗憾的是，NVIDIA没有提供这种直接的编译器。相反，NVIDIA开发了一套专门的GPU编程模型，称为CUDA语言，来支持在GPU上进行编程。CUDA编程模型本质是对GPU硬件的抽象。</p><p>CUDA编程模型的核心是要求程序员将算法的实现代码，拆分成多个可以独立执行的软件任务。我们将算法拆分得到的每个独立任务，称为一个<strong>线程</strong>。线程是软件层面最小的执行单元。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250306212256.png"></p><p>为了尽可能提高并行度，SP中的线程束调度器(warpscheduler)会将每32个线程打包成一个线程束(warp)。在一个时钟周期内，每组线程束会被调度到一个流式处理器上，执行一条相同的GPU指令。SM对应CUDA编程语言中线程块(block)的概念。 <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250306212820.png"></p><p>GPU的并行能力其实分为软件并行和硬件并行两层： -软件并行：将算子拆分成多个可以独立执行的线程，使用CUDA语言实现。 -硬件并行：对线程进行分组，然后以线程束(warp)或者线程块(block)为单元，并行执行这些线程。</p><h3 id="gpu的关键指标">GPU的关键指标</h3><ul><li>Tensor core和CUDA core性能：决定了计算效率。</li><li>显存大小：决定了可以承载的模型、数据规模上限。</li><li>显存带宽：决定了显存读写效率。</li><li>卡间通信带宽：决定了多块GPU卡之间数据通信的效率。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250306213416.png"></p><h3 id="显存vram与内存dram之间的数据传输">显存VRAM与内存DRAM之间的数据传输</h3><p>一般来说，VRAM与DRAM之间的传输依靠CPU进行调度，但是如果DRAM中的数据存储在锁页内存（PinnedMemory）中，CPU无法直接访问，此时就需要使用DMA(Direct MemoryAccess)技术。</p><p>另外，NVIDIA还提供了依靠DMA的VRAM与硬盘/网络之间数据传输： - GPUDirect Storage技术：允许GPU借助DMA直接访问NVMe固态硬盘； -RDMA技术：允许GPU借助DMA直接访问网络存储。</p><h2 id="分布式系统">分布式系统</h2><h3 id="单机多卡">单机多卡</h3><p>多卡在单机上的通信方式有两种： - 通过<strong>PCIe</strong>总线连接 -通过<strong>NVLink</strong>连接</p><p>NVLink是NVIDIA开发的一种高速互连技术，专门用于GPU之间的通信（CPU与GPU之间的通信使用PCIe总线）。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250306215759.png"></p><p>NVLink第一代在Pascal架构中被引入，后面逐代升级。 <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250306215956.png"></p><h3 id="多机多卡">多机多卡</h3><p>在涉及多机多卡的分布式训练中，不同机器上的GPU通信的硬件不再是NVLink或者PCIe这种高带宽低延迟的互联，而是基于网络设备的传输。两类主流的解决方案分别基于<strong>Ethernet</strong>以及<strong>InfiniBand</strong>。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250306220225.png"></p><h3 id="分布式系统的数据存储">分布式系统的数据存储</h3><p>对于分布式训练系统来说，通过每台服务器的本地硬盘存储大规模训练数据并不现实。基于网络的存储方案在分布式系统中更受欢迎。通常也分为两类，即<strong>NFS</strong>（networkfile system，网络文件系统）和基于<strong>云服务</strong>的存储方案。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250306220724.png"></p><p>分布式系统的硬件示意图: <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250306220853.png"></p>]]></content>
      
      
      <categories>
          
          <category> 《大模型动力引擎》阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLMs </tag>
            
            <tag> 《大模型动力引擎》 </tag>
            
            <tag> 深度学习硬件知识 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3D重建-深度图/网格/体素/点云</title>
      <link href="/2025/03/06/3d-chong-jian-shen-du-tu-wang-ge-ti-su-dian-yun/"/>
      <url>/2025/03/06/3d-chong-jian-shen-du-tu-wang-ge-ti-su-dian-yun/</url>
      
        <content type="html"><![CDATA[<h2 id="深度图">深度图</h2><h3 id="定义">定义</h3><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250306162901.png">Depth map深度图是一张2D图片，每个像素都记录了从视点（viewpoint）到遮挡物表面（遮挡物就是阴影生成物体）的距离，这些像素对应的顶点对于观察者而言是“可见的”。</p><h3 id="z-buffer">Z-buffer</h3><p>Depthmap中像素点记录的深度值为lenth1；然后从视点出发，计算物体顶点v到视点的距离，记为lenth2；比较二者大小，来确定“v”是否被遮挡。该术语的同义词有depthbuffer, Z-buffer, Z-buffering 和Z-depth。这里的"Z"是相对于相机（即视点）视图中心轴线而言的，也就是相机的z轴线，而不是场景的绝对坐标中的z轴线。#### 应用 -模拟在一个场景中的密度均匀的半透明介质效果-如雾，烟或大量的水； -模拟场景表面的深度域（depth of field (DOF)）； -可用于高效的变形体碰撞检测。</p><h2 id="体素">体素</h2><p>体素或立体像素(voxel)，是体积像素(volumepixel)的简称。概念上类似二维空间的最小单位——像素，像素用在二维电脑图像的视频数据上。体积像素一如其名，是数字数据于三维空间分区上的最小单位，应用于三维成像、科学数据与医学视频等领域。有些真正的三维显示器运用体素来描述它们的分辨率，举例来说：可以显示512×512×512体素的显示器。</p><p><strong>如同像素，体素本身并不含有空间中位置的数据(即它们的座标)</strong>，然而却可以从它们相对于其他体素的位置来推敲，意即它们在构成单一张体积视频的数据结构中的位置。</p><h2 id="网格">网格</h2><h3 id="定义-1">定义</h3><p>多边形网格（Polygonmesh）是三维计算机图形学中表示多面体形状的顶点与多边形的集合，它也叫作非结构网格。</p><p>这些网格通常由三角形、四边形或者其它的简单凸多边形组成，这样可以简化渲染过程。但是，网格也可以包括带有空洞的普通多边形组成的物体。</p><h3 id="类型">类型</h3><ul><li>一组顶点的简单列表，它们带有表示那些顶点组成多边形的信息列表；另外可能带有表示空洞的附加信息。</li><li>顶点列表 + 边界列表（一对索引信息）+ 连接边界的多边形列表</li><li>翼边数据结构</li></ul><p>根据应用程序的不同所选择的数据结构也有所不同：三角形的处理要比普通多边形的处理更加简单，尤其是在计算几何中更是这样。对于优化的算法，可能需要快速访问边线或者相邻表面这样的拓扑信息，这样就需要如翼边表示这样更加复杂的结构。</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250306164354.png"></p><h2 id="点云">点云</h2><h3 id="定义-2">定义</h3><p>点云（point cloud）是指透过3D扫描器所取得之资料型式。</p><p>扫描资料以点的型式记录，每一个点包含有三维座标，有些可能含有色彩资讯（R,G,B）或物体反射面强度。</p><p>点云数据除了具有几何位置以外，还有强度（Intensity）信息，强度信息的获取是激光扫描仪接受装置采集到的回波强度，此强度信息与目标的表面材质、粗糙度、入射角方向，以及仪器的发射能量，激光波长有关。点云也是逆向工程中通过仪器测量外表的点数据集合。</p><p>在电脑动画领域，皮克斯的玩具总动员3使用了点云技术[2]</p><h3 id="点云应用深度学习面临的挑战">点云应用深度学习面临的挑战：</h3><p>非结构化数据， 不变性排列，点云数据量上的变化(不同传感器上点云的数量变化很大)</p><p>点云数据方面的挑战： - 缺少数据： 扫描的模型通常被遮挡，部分数据丢失- 噪音：所有传感器都是嘈杂的。有几种类型的噪声，包括点云扰动和异常值。这意味着一个点有一定的概率位于它被采样的地方(扰动)附近的某一半径范围内，或者它可能出现在空间的任意位置(异常值)- 旋转： 一辆车向左转，同一辆车向右转，会有不同的点云代表同一辆车</p><p><strong>在点云上直接用深度学习的方法是将数据转换成体积表示，比如体素网格，然后就可以用3D滤波器来训练CNN，但是体积数据会变得非常大</strong>，3DCNN处理会非常慢，所以需要妥协到较低的分辨率，就会带来量化误差的代价</p><p>针对无序点云数据的深度学习方法研究进展缓慢，主要有三个方面： -点云具有无序性 -点云具有稀疏性:在KITTI数据集中，如果把原始的激光雷达点云投影到对应的彩色图像上，大概只有3%的像素才有对应的雷达点。这种极强的稀疏性让基于点云的高层语义感知变得尤其困难。-点云信息量有限:点云的数据结构就是一些三维空间的点坐标构成的点集，本质是对三维世界几何形状的<strong>低分辨率重采样</strong>，因此只能提供片面的几何信息</p>]]></content>
      
      
      <categories>
          
          <category> 3D重建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3D重建 </tag>
            
            <tag> 深度图 </tag>
            
            <tag> 网格 </tag>
            
            <tag> 体素 </tag>
            
            <tag> 点云 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>transformers-源码学习</title>
      <link href="/2025/01/21/transformers-yuan-ma-xue-xi/"/>
      <url>/2025/01/21/transformers-yuan-ma-xue-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>Huggingface的transformers库是NLP领域最常用的库之一，其建立在 Pytorch框架之上（Tensorflow 的版本功能并不完善），基本上 所有的Transformer模型都可以在 Hugging Face Hub中找到并且加载使用，包括训练、推理、量化等。</p><p>Huggingface官方提供了<a href="https://transformers.run/">transformers快速入门</a>的官方教程，可以快速上手。</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250121004750.png"></p><h2 id="huggingface主要部分">HuggingFace主要部分</h2><ul><li>Models Hub：托管和分享模型。</li><li>Datasets Hub：托管和分享数据集。</li><li>Spaces：托管和运行应用（基于 Gradio 或 Streamlit）。</li><li>Posts：帖子，可以查看最新的一些更新动态。</li><li>Collections：收藏夹，可以查看最新的热门收藏内容。</li><li>Community：社区，提供一些免费学习课程和博客。</li></ul><p>每个hub提供了对应的标签，按内容分为： -tasks：按照Multimodal/Computer Vision/Natural LanguageProcessing/Audio/Tabular/Reinforcement Learning等内容又做了具体的细分。-libraries：指出改仓库使用了哪些库，如Pytorch、Transformers、PEFT、Diffusers等。- datasets：指出改仓库使用了哪些数据集，如GLUE、SQuAD、CommonCrawl、Wikipedia等。 -languages：指出改仓库使用了哪些语言，如English、Chinese、Spanish、French等。- licenses：指出改仓库使用了哪些许可证，如Apache 2.0、MIT、CC-BY-NC-SA4.0等。 - other：如inference status，misc杂项等</p><h2 id="transformers库">Transformers库</h2><p>一个自然语言处理的库，提供了简单易用的API，可以快速地加载、训练和部署模型。并且提供统一的代码风格来使用BERT、XLNet和GPT等等各种不同的模型。包含configuration，models和tokenizer三个主要类，并基于这三个类提供更上层的pipeline和Trainer，从而用更少的代码实现模型的预测和微调。</p><ul><li><strong>pipeline</strong> 在底层是由 AutoModel 和 AutoTokenizer类来实现的。AutoClass（即像 AutoModel 和 AutoTokenizer这样的通用类）是加载模型的快捷方式，它可以从其名称或路径中自动检索预训练模型。</li><li>需要为每个模型系列使用特定的Transformers和Tokenizer（例如，如果使用T5模型系列，则对应T5Tokenizer和T5ForConditionalGeneration），对于所有预训练模型，您可以声明一个简单的语句：<pre class="line-numbers language-python" data-language="python"><code class="language-python">from transformers import AutoTokenizer, AutoModelForCausalLMtokenizer = AutoTokenizer.from_pretrained("databricks/dolly-v2-3b")model = AutoModelForCausalLM.from_pretrained("databricks/dolly-v2-3b")</code></pre></li></ul><h2 id="下载模型文件">下载模型文件</h2><p>如果直接使用transformers 库，默认保存位置在：～/.cache文件夹下。</p><p><a href="https://hf-mirror.com/">HF-Mirror</a>提供了国内的镜像下载，推荐使用“方法三：使用hfd”更加稳定并且下载进度条更直观。下载时记得使用tmux或screen等工具防止下载中断。</p><h2 id="pipeline">pipeline</h2><p>开箱即用的 pipelines，它封装了预训练模型和对应的前处理和后处理环节。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250124012049.png">1. 预处理(preprocessing)，<strong>Transformer模型无法直接处理原始文本字符串</strong>；具体地，我们会使用每个模型对应的分词器(tokenizer) 来进行。 -将输入切分为词语、子词或者符号（例如标点符号），统称为 tokens； -根据模型的词表将每个 token 映射到对应的 token编号/id；映射通常是通过创建文本语料库中标记的词汇表，并根据每个标记在语料库中的出现频率为其分配一个整数值来执行的(如BPE)。最常见的tokens被分配较低的整数值，而不太常见的标记被分配较高的值。- 根据模型的需要，添加一些额外的输入。 - [PAD] - 用于填充序列到固定长度- [CLS] - 分类任务中的起始标记 - [SEP] - 用于分隔不同文本段落 - [MASK] -用于掩码语言模型预训练 - [BOS] / <s> - 序列开始标记 - [EOS] / </s> -序列结束标记 - [UNK] - 表示词表外的未知词</p><pre><code>PS:除了上述训练推理时用到的特殊token，在chat任务时还有有其他的输入token（看promt template）：- "&lt;|system|&gt;"    # 系统提示- "&lt;|user|&gt;"      # 用户输入- "&lt;|assistant|&gt;" # AI助手回复- "&lt;|human|&gt;"     # 人类对话者还有一些控制相关的token（多模态）：- "&lt;|endoftext|&gt;"  # 文本结束- "&lt;|im_start|&gt;"   # 图像开始- "&lt;|im_end|&gt;"     # 图像结束</code></pre><ol start="2" type="1"><li>将处理好的输入送入模型；</li><li>对模型的输出进行后处理(postprocessing)，将其转换为人类方便阅读的格式。 <pre class="line-numbers language-python" data-language="python"><code class="language-python">from transformers import pipeline# 使用pipeline加载模型pipeline = pipeline("text-generation", model="gpt2")# 输入文本input_text = "Once upon a time"# 生成文本output_text = pipeline(input_text)print(output_text)</code></pre></li></ol><h2 id="三大组件">三大组件</h2><ol type="1"><li>「Conﬁguration」：配置类，通常继承自「PretrainedConﬁg」，保存model或tokenizer的超参数，例如词典大小，隐层维度数，dropoutrate等。配置类主要可用于复现模型。</li><li>「Tokenizer」：Model只能处理数字，因此Tokenizer需要将我们的文本输入转换为数字。总体上做三件事情：分词；扩展词汇表；识别并处理特殊token。<ol type="1"><li>通常继承自「PreTrainedTokenizer」，主要存储词典（也就是<code>from_pretrained()</code>的部分），token到index映射关系等。</li><li>此外，还会有一些model-specific的特性，如特殊token，<code>[SEP]</code>,<code>[CLS]</code>等的处理，token的type类型处理，语句最大长度等，<strong>因此tokenizer通常和模型是一对一适配的</strong>。</li></ol></li><li>「Model」:模型类。封装了预训练模型的计算图过程，遵循着相同的范式，如根据tokenids进行embeddingmatrix映射（将token的one-hot编码转换成更dense的embedding编码），紧接着多个self-attention层做编码，最后一层task-specific做预测。</li></ol><p>针对上述三大类，transformer还额外封装了AutoConfig,AutoTokenizer,AutoModel，可通过模型的命名来定位其所属的具体类，比如’bert-base-cased’，就可以知道要加载BERT模型相关的配置、切词器和模型。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">from transformers import BertConfig, BertModel# Building the configconfig = BertConfig()# Building the model from the config,使用随机值对其进行初始化model = BertModel(config)print(config)BertConfig {  [...]  "hidden_size": 768,               # 定义了hidden状态向量的大小  "intermediate_size": 3072,  "max_position_embeddings": 512,  "num_attention_heads": 12,  "num_hidden_layers": 12,          # 定义了Transformer模型的层数  [...]}# 加载已经训练过的Transformers模型model = BertModel.from_pretrained("bert-base-cased")</code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 加载与保存分词器from transformers import BertTokenizertokenizer = BertTokenizer.from_pretrained("bert-base-cased")tokenizer.save_pretrained("./models/bert-base-cased/")# 加载与保存模型from transformers import AutoModel# 所有存储在 HuggingFace Model Hub 上的模型都可以通过 Model.from_pretrained() 来加载权重，参数可以是 checkpoint 的名称，也可以是本地路径（预先下载的模型目录）model = AutoModel.from_pretrained("bert-base-cased")model.save_pretrained("./models/bert-base-cased/") # 保存模型inputs = tokenizer(["来到美丽的大自然，我们发现"], return_tensors="pt")# {'input_ids': tensor([[    1, 68846, 68881, 67701, 67668, 98899, 91935]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}gen_kwargs = {"max_length": 128, "top_p": 0.8, "temperature": 0.8, "do_sample": True, "repetition_penalty": 1.1}output = model.generate(**inputs, **gen_kwargs) # 这种生成方式更加可控，结果需要decode# decode the new tokensoutput = tokenizer.decode(output[0].tolist(), skip_special_tokens=True)print(output)</code></pre><p>Transformers还将模型分为两个部分： 1.基础架构：就像一个通用的引擎，负责理解输入的文本，将文本转换成计算机能理解的向量形式，所有任务都用相同的基础结构。2.任务相关的head：根据任务的不同，将基础架构的输出转换为任务需要的输出变量。</p><p>tranformers中常见的模型类型： </p><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 举例说明不同任务的模型models = {    "*Model": "只输出基础理解，像原材料",    "*ForCausalLM": "预测下一个词（像GPT）",    "*ForMaskedLM": "完形填空（像BERT）",    "*ForMultipleChoice": "选择题",    "*ForQuestionAnswering": "回答问题",    "*ForSequenceClassification": "文本分类",    "*ForTokenClassification": "标注词语（如实体识别）"}</code></pre> <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250127032047.png"><p></p><h2 id="datasets">Datasets</h2><ol type="1"><li><p>介绍 </p><pre class="line-numbers language-python" data-language="python"><code class="language-python">from datasets import load_datasetraw_datasets = load_dataset("glue", "mrpc") # GLUE基准测试中的MRPC任务print(raw_datasets)# 包含训练集、验证集和测试集。每一个集合都包含几个列(sentence1, sentence2, label, and idx)以及一个代表行数的变量#DatasetDict({#    train: Dataset({#        features: ['sentence1', 'sentence2', 'label', 'idx'],#        num_rows: 3668#    })#    validation: Dataset({#        features: ['sentence1', 'sentence2', 'label', 'idx'],#        num_rows: 408#    })#    test: Dataset({#        features: ['sentence1', 'sentence2', 'label', 'idx'],#        num_rows: 1725#    })#})</code></pre><p></p></li><li><p>简单使用 </p><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 1. 加载内置数据集dataset = load_dataset("glue", "mrpc")  # GLUE基准测试中的MRPC任务# 2. 加载本地文件dataset = load_dataset("csv", data_files="my_file.csv")dataset = load_dataset("json", data_files="my_file.json")# 3. 访问数据train_data = dataset['train']first_example = train_data[0]</code></pre><p></p></li><li><p>为了预处理数据集，我们需要将文本转换为模型能够理解的数字，使用Dataset.map()方法</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"># example 是一个dict，对应数据集的每个元素，并返回一个包含input_ids、attention_mask 和token_type_ids为key的新dict# 在机器学习任务中，一个example通常定义为模型的输入（也成为特征集合）def tokenize_function(example):    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)print(raw_datasets)#DatasetDict({#    train: Dataset({#        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],#        num_rows: 3668#    })#    validation: Dataset({#        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],#        num_rows: 408#    })#    test: Dataset({#        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],#        num_rows: 1725#    })#})</code></pre><p></p></li><li><p>为了解决句子长度统一的问题，我们必须定义一个collate函数，该函数会将每个batch句子填充到正确的长度。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">from datasets import load_datasetfrom transformers import AutoTokenizer, DataCollatorWithPaddingraw_datasets = load_dataset("glue", "mrpc")tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")def tokenize_function(example):    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)data_collator = DataCollatorWithPadding(tokenizer=tokenizer)</code></pre><p></p></li><li><p>加载本地数据集 </p><pre class="line-numbers language-python" data-language="python"><code class="language-python">from datasets import load_dataset#{#    "data": [#        {#            "title": "Terremoto del Sichuan del 2008",#            "paragraphs": [{...}]#        }#    ],#     "version": "1.1"#}squad_it_dataset = load_dataset("json", data_files="SQuAD_it-train.json", field="data")print(squad_it_dataset) # 加载本地文件会创建一个带有train的DatasetDict 对象# DatasetDict({#    train: Dataset({#        features: ['title', 'paragraphs'],#        num_rows: 442#    })#})data_files = {"train": "SQuAD_it-train.json", "test": "SQuAD_it-test.json"}squad_it_dataset = load_dataset("json", data_files=data_files, field="data")print(squad_it_dataset) # 包括 train 和 test 的 DatasetDict 对象#DatasetDict({#    train: Dataset({#        features: ['title', 'paragraphs'],#        num_rows: 442#    })#    test: Dataset({#        features: ['title', 'paragraphs'],#        num_rows: 48#    })#})</code></pre><p></p></li><li><p><code>streaming=True</code>： </p><pre class="line-numbers language-python" data-language="python"><code class="language-python">dataset = load_dataset("json", data_files="my_file.json", streaming=True)# 返回一个迭代器，可以逐个处理数据</code></pre><p></p></li></ol><h2 id="源码分析">源码分析</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">tokenizer = AutoTokenizer.from_pretrained(checkpoint)model = AutoModelForSequenceClassification.from_pretrained(checkpoint)sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")print(tokens)#{#    'input_ids': tensor([#        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,2607,  2026,  2878,  2166,  1012,   102],#        [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0, 0,     0,     0,     0,     0,     0]#    ]),     #    'attention_mask': tensor([#        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],#        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]#    ])#}output = model(**tokens) # model(**tokens, labels=labels), 如果提供labels，则计算损失print(output)# SequenceClassifierOutput(#     loss=None,  # 损失值，在没有提供标签时为None#     logits=tensor([#         [-1.5607,  1.6123],  # 第一个序列的预测分数（两个类别）#         [-3.6183,  3.9137]   # 第二个序列的预测分数#     ]), #     hidden_states=None,  # 隐藏状态，默认不返回#     attentions=None      # 注意力权重，默认不返回# )</code></pre><h3 id="tokenizer">tokenizer</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">tokens = tokenizer(    sequences,          # 输入文本，可以是单个字符串或字符串列表    padding=True,       # 填充设置    truncation=True,    # 截断设置    return_tensors="pt" # 返回格式)</code></pre><ul><li><p>padding </p><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 相关参数tokens = tokenizer(    ["Hello", "Hi there!"],    padding=True, # 填充设置    max_length=10, # 最大长度限制    padding_side='right' # 填充位置：'left'或'right')</code></pre><p></p></li><li><p>truncation： 截断设置</p></li><li><p>return_tensors： 返回格式(pt, tf)</p></li><li><p>return_token_type_ids： 是否返回token_type_ids(默认False,用于区分不同句子)</p></li></ul><p><code>output = model(**tokens)</code>对于文本分类来说，整段文本输入返回一个标签，也就是SequenceClassifierOutput.logits，</p><p>如果是文本生成模型，整段文本输入返回CausalLMOutputWithPast， 对于input_ids = [v1,v2,v3] 输出为 [v20,v30,v4]。 v20 是根据v1生成的下一个token（大概率跟真实的v2 不一样），v4 是根据v1,v2,v3 生成的。1. 生成式模型的训练是并行的，靠的attentionmask操作。训练的时候只需要调用一次即可，因为attentionmask的机制可以保证当前token的loss不包含后面的token。inputs则是一个dict，包含input_ids 和 attention_mask。 2.推理的时候，只能一个字一个字的推理，所以会调用多次。 output =model(inputs) 或者 output = model.forward(inputs)的时候，inputs其实也不需要包含 attention_mask，inputs 仅仅是token 就可以</p><p>model(xx) ==&gt; <code>Module.__call__</code> ==&gt;Module.forward/model.forward，几乎每一个llm 都会自定义forward方法，如果向forward 方法传入 labels，还会自动计算loss。</p>]]></content>
      
      
      <categories>
          
          <category> transformer </category>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> transformer </tag>
            
            <tag> huggingface </tag>
            
            <tag> qwen </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vscode 快捷键</title>
      <link href="/2025/01/16/vscode-kuai-jie-jian/"/>
      <url>/2025/01/16/vscode-kuai-jie-jian/</url>
      
        <content type="html"><![CDATA[<p>MacOS 系统上 Visual Studio Code (VSCode) 一些常用的快捷键</p><p>参考：</p><p>https://www.dute.org/vscode-shortcut</p><p>https://hughfenghen.github.io/posts/2023/03/29/vscode-shortcut/</p><p>因为一般都会搭配vim使用，所以有些操作更习惯通过vim方式，对于一些vim不方便完成的常用快捷键使用加粗标记。</p><ul><li><code>command + p</code><ul><li>打开一个文件在新标签页中</li><li><code>+ &gt;</code> 命令</li><li><code>+ @</code> 转到本文件中的符号</li><li><code>+ #</code> 显示所有符号</li></ul></li></ul><h2 id="常用快捷键">常用快捷键</h2><table><thead><tr class="header"><th>快捷键</th><th>说明</th></tr></thead><tbody><tr class="odd"><td><strong><code>Command + Shift + N</code></strong></td><td>新建窗口/实例</td></tr><tr class="even"><td><code>Command + W</code></td><td>关闭窗口/实例</td></tr><tr class="odd"><td><code>Command + ,</code></td><td>编辑器设置</td></tr><tr class="even"><td><code>Command + Shift + j</code></td><td>设置(cursor 设置)</td></tr><tr class="odd"><td><code>Command + F</code></td><td>搜索</td></tr><tr class="even"><td><strong><code>Command + Shift + F</code></strong></td><td>全局搜索</td></tr><tr class="odd"><td><code>Command + Option + F</code></td><td>替换</td></tr><tr class="even"><td><strong><code>Option + 点击鼠标</code></strong></td><td>在所点击位置插入光标</td></tr><tr class="odd"><td><strong><code>Command + N</code></strong></td><td>新建文件</td></tr><tr class="even"><td><strong><code>Command + O</code></strong></td><td>打开文件...</td></tr><tr class="odd"><td><strong><code>Command + S</code></strong></td><td>保存文件</td></tr><tr class="even"><td><strong><code>Command + Shift + S</code></strong></td><td>文件另存为...</td></tr><tr class="odd"><td><code>Command + Option + S</code></td><td>保存全部</td></tr><tr class="even"><td><strong><code>Command + Shift + Space</code></strong></td><td>触发参数提示</td></tr><tr class="odd"><td><code>Ctrl + number</code></td><td>跳转标签</td></tr><tr class="even"><td><code>Ctrl + tab</code></td><td>查看编辑历史</td></tr><tr class="odd"><td><code>Alt + left</code></td><td>跳转代码位置(win?)</td></tr><tr class="even"><td><strong><code>Control + -</code></strong></td><td>后退</td></tr><tr class="odd"><td><strong><code>Control + Shift + -</code></strong></td><td>前进</td></tr></tbody></table><h2 id="光标操作">光标操作</h2><table><thead><tr class="header"><th>快捷键</th><th>说明</th></tr></thead><tbody><tr class="odd"><td><strong><code>Option + 点击鼠标</code></strong></td><td>在所点击位置插入光标</td></tr><tr class="even"><td><code>Command + Option + ↑</code></td><td>在上方插入光标</td></tr><tr class="odd"><td><code>Command + Option + ↓</code></td><td>在下方插入光标</td></tr><tr class="even"><td><code>Command + U</code></td><td>撤消上一个光标操作</td></tr><tr class="odd"><td><code>Shift + Option + I</code></td><td>在所选的每一行的末尾插入光标</td></tr><tr class="even"><td><code>Command + L</code></td><td>选择当前行</td></tr><tr class="odd"><td><code>Command + F2</code></td><td>选择所有出现的当前单词</td></tr><tr class="even"><td><code>Command + Control + Shift + →</code></td><td>扩展选择</td></tr><tr class="odd"><td><code>Command + Control + Shift + ←</code></td><td>收缩选择</td></tr><tr class="even"><td><code>Shift + Option + 拖拽鼠标</code></td><td>列（框）选择</td></tr><tr class="odd"><td><code>Command + Shift + Option + ↑</code></td><td>向上列（框）选择</td></tr><tr class="even"><td><code>Command + Shift + Option + ↓</code></td><td>向下列（框）选择</td></tr></tbody></table><h2 id="代码编辑">代码编辑</h2><table><thead><tr class="header"><th>快捷键</th><th>说明</th></tr></thead><tbody><tr class="odd"><td><strong><code>Command + Shift + Space</code></strong></td><td>触发参数提示</td></tr><tr class="even"><td><code>Shift + Option + F</code></td><td>格式化文件</td></tr><tr class="odd"><td><code>Command + K, Command + F</code></td><td>格式化选择</td></tr><tr class="even"><td><strong><code>F12</code></strong></td><td>转到定义</td></tr><tr class="odd"><td><strong><code>Option + F12</code></strong></td><td>查看定义</td></tr><tr class="even"><td><code>Command + K, F12</code></td><td>在侧面打开定义</td></tr><tr class="odd"><td><code>Command + .</code></td><td>快速修复</td></tr><tr class="even"><td><code>Shift + F12</code></td><td>显示参考</td></tr><tr class="odd"><td><code>F2</code></td><td>重命名符号</td></tr></tbody></table><h2 id="窗口显示">窗口显示</h2><table><thead><tr class="header"><th>快捷键</th><th>说明</th></tr></thead><tbody><tr class="odd"><td><code>Command + Control + F</code></td><td>切换全屏</td></tr><tr class="even"><td><code>Command + Option + 0</code></td><td>切换编辑器布局（水平/垂直）</td></tr><tr class="odd"><td><code>Command + =</code></td><td>放大编辑器</td></tr><tr class="even"><td><code>Command + -</code></td><td>缩小编辑器</td></tr><tr class="odd"><td><code>Command + B</code></td><td>显示/隐藏侧边栏</td></tr></tbody></table><h2 id="终端">终端</h2><table><thead><tr class="header"><th>快捷键</th><th>说明</th></tr></thead><tbody><tr class="odd"><td><code>Command + j</code></td><td>打开/关闭终端</td></tr></tbody></table><h2 id="导航">导航</h2><table><thead><tr class="header"><th>快捷键</th><th>说明</th></tr></thead><tbody><tr class="odd"><td><code>F8</code></td><td>转到下一个错误或警告</td></tr><tr class="even"><td><code>Shift + F8</code></td><td>转到上一个错误或警告</td></tr><tr class="odd"><td><code>Control + Shift + Tab</code></td><td>显示编辑器历史记录</td></tr><tr class="even"><td><strong><code>Control + -</code></strong></td><td>后退</td></tr><tr class="odd"><td><strong><code>Control + Shift + -</code></strong></td><td>前进</td></tr><tr class="even"><td><code>Control + M</code></td><td>切换标签可移动焦点</td></tr></tbody></table><h2 id="基本编辑vim为主vscode不怎么用">基本编辑(vim为主，vscode不怎么用)</h2><table><thead><tr class="header"><th>快捷键</th><th>说明</th></tr></thead><tbody><tr class="odd"><td><code>Option + ↑</code></td><td>把当前行往上移动</td></tr><tr class="even"><td><code>Option + ↓</code></td><td>把当前行往下移动</td></tr><tr class="odd"><td><code>Shift + Option + ↑</code></td><td>在上面复制行</td></tr><tr class="even"><td><code>Shift + Option + ↓</code></td><td>在下面复制行</td></tr><tr class="odd"><td><code>Command + Shift + K</code></td><td>删除一行</td></tr><tr class="even"><td><code>Command + Enter</code></td><td>在下面插入行</td></tr><tr class="odd"><td><code>Command + Shift + Enter</code></td><td>在上方插入行</td></tr><tr class="even"><td><code>Command + Shift + \</code></td><td>跳转到匹配的括号, vim中可以使用<code>%</code></td></tr><tr class="odd"><td><code>Command + ]</code></td><td>增加缩进</td></tr><tr class="even"><td><code>Command + [</code></td><td>减少缩进</td></tr><tr class="odd"><td><code>Home</code></td><td>转到行首</td></tr><tr class="even"><td><code>End</code></td><td>转到行尾</td></tr><tr class="odd"><td><code>Command + ↑</code></td><td>转到文件开头</td></tr><tr class="even"><td><code>Command + ↓</code></td><td>转到文件末尾</td></tr><tr class="odd"><td><code>Control + Fn + ↑</code></td><td>向上滚动行</td></tr><tr class="even"><td><code>Control + Fn + ↓</code></td><td>向下滚动行</td></tr><tr class="odd"><td><code>Command + Fn + ↑</code></td><td>向上滚动页面</td></tr><tr class="even"><td><code>Command + Fn + ↓</code></td><td>向下滚动页面</td></tr><tr class="odd"><td><code>Command + Option + [</code></td><td>折叠区域</td></tr><tr class="even"><td><code>Command + Option + ]</code></td><td>展开区域</td></tr><tr class="odd"><td><code>Command + K, Command + 0</code></td><td>折叠所有区域</td></tr><tr class="even"><td><code>Command + K, Command + J</code></td><td>展开所有区域</td></tr><tr class="odd"><td><code>Command + K, Command + C</code></td><td>添加行注释</td></tr><tr class="even"><td><code>Command + K, Command + U</code></td><td>删除行注释</td></tr><tr class="odd"><td><code>Command + /</code></td><td>注释</td></tr><tr class="even"><td><code>Option + Z</code></td><td>切换编辑器的自动换行</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> vscode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> vscode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Otsu 阈值算法</title>
      <link href="/2025/01/15/otsu-yu-zhi-suan-fa/"/>
      <url>/2025/01/15/otsu-yu-zhi-suan-fa/</url>
      
        <content type="html"><![CDATA[<p>Otsu 阈值算法的简单实现</p><h2 id="什么是-otsu-阈值算法">1. 什么是 Otsu 阈值算法</h2><p>Otsu阈值算法是一种自适应阈值分割算法，用于将图像分割为前景和背景。核心思想是通过最大化类间方差来确定最佳阈值。</p><p>Otsu阈值算法的基本思想是：通过计算图像的灰度直方图，找到一个阈值，使得前景和背景的类间方差最大。</p><h2 id="算法步骤">2. 算法步骤</h2><ol type="1"><li>计算图像的灰度直方图:<ul><li>图像的像素值被划分为若干个灰度级，计算每个灰度级的像素数。</li><li>计算每个灰度级的像素数占总像素数的比例。</li></ul></li><li>计算前景和背景的类间方差<ul><li>对每个可能的阈值，将图像的像素值划分为前景和背景。</li><li>计算这两部分的灰度均值、像素比例和类间方差。</li><li>寻找使类间方差最大的阈值。</li></ul></li></ol><h2 id="数学表达">3. 数学表达</h2><ul><li>假设总像素数为 <span class="math inline">\(N\)</span>，灰度值的范围为 <span class="math inline">\([0, L-1]\)</span>，灰度级 <span class="math inline">\(k\)</span> 的像素数为 <span class="math inline">\(n_k\)</span>，灰度级 <span class="math inline">\(k\)</span> 的像素数占总像素数的比例为 <span class="math inline">\(p_k\)</span>。</li><li>对于x给定的阈值<span class="math inline">\(t\)</span>，灰度值被分为两类：<ul><li>前景类<span class="math inline">\(C_1\)</span>，灰度值在<span class="math inline">\([0, t]\)</span>之间</li><li>背景类<span class="math inline">\(C_2\)</span>，灰度值在<span class="math inline">\([t+1, L-1]\)</span>之间</li></ul></li><li>前景类的像素数比例：<span class="math inline">\(w_1 = \sum_{i=0}^{t}p_i\)</span></li><li>背景类的像素数比例：<span class="math inline">\(w_2 =\sum_{i=t+1}^{L-1} p_i\)</span></li><li>前景类的灰度均值：<span class="math inline">\(u_1 = \sum_{i=0}^{t} ip_i / w_1\)</span></li><li>背景类的灰度均值：<span class="math inline">\(u_2 =\sum_{i=t+1}^{L-1} i p_i / w_2\)</span></li><li>类间方差：<span class="math inline">\(\sigma_b^2 = w_1 w_2 (u_1 -u_2)^2\)</span></li></ul><h2 id="代码实现">4. <a href="https://github.com/baoblei/otsu">代码实现</a></h2><ul><li><p>简洁实现，使用opencv计算灰度直方图 </p><pre class="line-numbers language-python" data-language="python"><code class="language-python">def otsu_threshold(image):    # 计算灰度直方图    hist = cv2.calcHist([image], [0], None, [256], [0, 256]) # 计算灰度直方图    hist = hist.ravel() / hist.sum() # 计算灰度直方图的概率    # 计算前景和背景的类间方差    max_var = 0    best_threshold = 0    for threshold in range(256):        w1 = np.sum(hist[:threshold])        w2 = np.sum(hist[threshold+1:])        u1 = np.sum(hist[:threshold] * np.arange(256)) / w1        u2 = np.sum(hist[threshold+1:] * np.arange(threshold+1, 256)) / w2        var = w1 * w2 * (u1 - u2) ** 2        if var &gt; max_var:            max_var = var            best_threshold = thresholdreturn best_threshold</code></pre><p></p></li><li><p>一般数据的分割 </p><pre class="line-numbers language-python" data-language="python"><code class="language-python">def _otsu_threshold(data):    hist, bins = np.histogram(data, bins=50) # 可以指定bins的数量t    bin_centers = (bins[:-1] + bins[1:]) / 2        # OTSU算法实现    total = hist.sum()    sum_total = sum(bin_centers * hist)        max_variance = 0    threshold = 0        sum_b = 0    count_b = 0        for i in range(len(hist)):        count_b += hist[i]        if count_b == 0:            continue                    count_f = total - count_b        if count_f == 0:            break                    sum_b += bin_centers[i] * hist[i]        mean_b = sum_b / count_b        mean_f = (sum_total - sum_b) / count_f                variance = count_b * count_f * (mean_b - mean_f) ** 2        if variance &gt; max_variance:            max_variance = variance            threshold = bin_centers[i]                return threshold</code></pre><p></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> image-processing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> otsu </tag>
            
            <tag> threshold </tag>
            
            <tag> image-processing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>硬菜收集</title>
      <link href="/2025/01/11/ying-cai-shou-ji/"/>
      <url>/2025/01/11/ying-cai-shou-ji/</url>
      
        <content type="html"><![CDATA[<h2 id="菜肴">菜肴</h2><h3 id="酱汁扒鸡">酱汁扒鸡</h3><p>(6.97 F@u.Sl 03/28 Rxs:/2025年夜饭第二道：酱汁扒鸡！吃肉又喝汤不浪费一点点！主页搜“酱汁扒鸡”有更详细的版本，这个版本更适合下载转发到“一家亲”哦~#老饭骨 # 舌尖上的抖音 # 北方美食 https://v.douyin.com/iysoJKdm/复制此链接，打开Dou音搜索，直接观看视频！) <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250111202836.png"></p><h4 id="食材">食材</h4><ul><li>鸡肉</li><li>甜面酱</li><li>葱姜蒜、八角、盐、糖</li><li>香油</li></ul><h4 id="做法">做法</h4><ul><li>炖<ul><li>鸡洗净切块，加入两片姜放入锅中炖一小时。</li><li>炖好的鸡肉捞出，鸡汤留着，鸡进行拆骨</li></ul></li><li>蒸<ul><li>准备酱料：甜面酱+鸡汤+糖 混合均匀</li><li>鸡肉放入准备好的酱料中装盘，再放入两片姜、两块葱、八角、香油、盐少许（根据买的甜面酱的咸度）</li><li>放入锅中蒸15-20分钟</li></ul></li><li>煎<ul><li>将蒸好的鸡肉取出拿掉其中的大料</li><li>放入锅中收汁，可以加入适量的水淀粉勾芡，也可以加入一些香油，尽量不要拿勺子炒</li><li>一面煎好后翻面继续煎，煎到两面金黄即可</li></ul></li><li>装盘：可以再加入一些青蒜点缀 <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250111202744.png"></li></ul><h3 id="蒜蓉粉丝虾">蒜蓉粉丝虾</h3><p>7.12 odN:/ 06/30 J@i.pq年夜饭大菜！蒜蓉粉丝虾，寓意腰缠万贯！家里来客人一定少不了！# 蒜蓉粉丝虾# 年夜饭 # 年夜饭菜谱 # 腰缠万贯虾 https://v.douyin.com/iyGesNSt/复制此链接，打开Dou音搜索，直接观看视频！ <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250111205942.png">#### 食材 - 基围虾 - 粉丝 - 蒜蓉酱（也可以自制） - 蒸鱼豉油 -葱花、盐、白糖、料酒、白胡椒粉</p><h4 id="做法-1">做法</h4><ul><li>蒜蓉酱做法<ul><li>蒜剁碎</li><li>一半放入水中浸泡</li><li>剩下一半大蒜中放一点盐进行腌制（这个时候可以先去处理虾了）</li><li>锅中放油（需要没过准备的蒜蓉），烧至三成热</li><li>水中浸泡的蒜蓉先捞出下锅，大火搅动至蒜蓉变色，捞出</li><li>盐腌过的蒜蓉不放锅炒，加入一下白糖、白胡椒粉</li><li>炸锅的蒜放入腌过的蒜中，抄拌</li><li>油重新烧热，倒入蒜中</li></ul></li><li>白虾处理：<ul><li>去头，后背平片一刀，尾巴可以顺着腹部传过来做个造型</li><li>放入少许盐、料酒、白糖、白胡椒粉简单腌制一下</li></ul></li><li>蒸：<ul><li>粉丝泡软，剪短</li><li>粉丝可以卷成圈，圈中间部分放虾</li><li>蒜蓉酱浇在虾上</li><li>蒸锅中蒸10分钟</li></ul></li><li>装盘：撒上葱花、倒入蒸鱼豉油，最后浇上热油</li></ul><h3 id="红烧肉">红烧肉</h3><p>9.41 n@q.eb fod:/ 06/01红烧肉一次成功！中秋节团圆菜单！最好吃的家常菜做法！不用炒糖色！超下饭的红烧五花肉！#妈呀太香了 # 红烧肉 # 家常菜 # 下饭菜 # 抖音美食推荐官 #遇见懂你的心动好味 # 美好生活尽在抖音电商 # 提前祝大家中秋节快乐 #中秋节 <span class="citation" data-cites="抖音小助手">@抖音小助手</span><span class="citation" data-cites="DOU+小助手">@DOU+小助手</span>https://v.douyin.com/iyGNQA37/复制此链接，打开Dou音搜索，直接观看视频！</p><h4 id="食材-1">食材</h4><ul><li>五花三层瘦六肥四肉</li><li>大葱、葱姜</li><li>冰糖</li><li>老抽、生抽、黄酒</li><li>八角、香叶、桂皮</li></ul><h4 id="做法-2">做法</h4><ul><li>猪肉处理<ul><li>把肉皮部分放入热锅里，在锅底烫皮，大约两分钟后拿出</li><li>用刀把烫皮刮干净</li><li>肉倒入冷水的锅里定型，放入葱姜、黄酒等水沸腾后五分钟后取出</li><li>肉进行切块</li></ul></li><li>红烧<ul><li>锅中少许油（抹在锅面就行），五花肉块大火煎至四面金黄，煎出来的油倒出</li><li>一点花生油放入冰糖（10颗左右，勇敢放甜口的好吃），最小火炒到红色冒泡</li><li>把肉放入锅中，稍微炒个糖色，再加入半碗黄酒，以及三勺生抽、三勺老抽</li><li>加入香料：八角3个、桂皮一段、香叶2片、姜片和一个葱结</li><li>倒入冷水（基本满锅）盖上锅盖焖煮45分钟，中间如果水干了要再加入水防止煮干了</li></ul></li><li>收汁<ul><li>把香料取出，加少许盐提味</li><li>大火收汁</li><li>最后转小火慢炒</li></ul></li></ul><h3 id="可乐鸡翅">可乐鸡翅</h3><p>2.56 y@t.eb 11/30 Wzt:/ 爷俩儿版的可乐鸡翅，看看和你做的有什么不同？#美食趣胃计划# 可乐鸡翅# 抖音小助手 https://v.douyin.com/iyG6DNCv/复制此链接，打开Dou音搜索，直接观看视频！</p><h4 id="食材-2">食材</h4><ul><li>鸡翅</li><li>可乐</li><li>姜</li><li>生抽、老抽、黄酒</li><li>盐、白糖</li><li>芝麻</li></ul><h4 id="做法-3">做法</h4><ul><li>鸡翅改刀（片切或者牙签扎洞）</li><li>煎鸡翅至双面微黄</li><li>放入姜片，大火，放入些许黄酒去腥</li><li>倒入一瓶可乐，两勺生抽，半勺盐，一勺白糖</li><li>煮沸后撇去浮沫，盖上盖子中小火焖20分钟</li><li>开锅拿掉姜片，中小火收汁，最后也可以撒上一把芝麻，然后出锅</li></ul><h3 id="辣椒炒肉">辣椒炒肉</h3><p>2.02 m@d.nq 10/26 Kjp:/ 你们觉得这样炒的辣椒炒肉正宗不咯#美食趣胃计划 # 搞点新品778 https://v.douyin.com/iyoUW8dj/复制此链接，打开Dou音搜索，直接观看视频！</p><h4 id="食材-3">食材</h4><ul><li>五花肉</li><li>前后腿瘦肉</li><li>螺丝椒，尽量嫩一点的</li><li>蒜、豆豉</li><li>生抽、老抽、蚝油、味精、盐</li></ul><h4 id="做法-4">做法</h4><ul><li>处理肉<ul><li>五花肉切薄片，瘦肉切薄片</li><li>瘦肉加蚝油酱油搅拌腌制</li><li>锅中加油，五花肉煸出油，加入四五瓣蒜，也可以加入一点豆豉，最后上酱油</li><li>锅中加入腌好的瘦肉，九成熟一起捞出</li></ul></li><li>炒<ul><li>辣椒滚刀切，锅中加油（少量或者不加），辣椒在锅中镭（用勺子压炒）干水汽</li><li>再放入炒肉中的油，翻炒入味</li><li>倒入炒好的肉，加一点味精和盐，翻炒均匀，出锅</li></ul></li></ul><h2 id="糕点">糕点</h2><h3 id="戚风蛋糕空气炸锅版">戚风蛋糕（空气炸锅版）</h3><p>3.33 dNj:/ D@h.Ox 12/03想做蛋糕又没烤箱的看过来啦！今天做的是空气炸锅版戚风！很详细的步骤分享给大家！#戚风蛋糕 # 烘焙新手 # 家庭烘焙 https://v.douyin.com/iyGhDMX4/复制此链接，打开Dou音搜索，直接观看视频！ #### 准备 - 空气炸锅 -电动打蛋器、刮刀、蛋抽 - 鸡蛋、面粉、白糖、食用油 -两个器皿，一个蛋糕模具（4寸/6寸） #### 做法 -25克玉米油、35克牛奶搅拌至乳化 - 低筋面粉45g加入其中，搅拌至无干粉的状态-分离三个蛋清蛋黄，蛋黄加入刚刚的面糊中搅拌均匀，至滴落有流动性，但滴落下去形状不会立刻消失- 空气炸锅放入一碗温水，140度预热15min。 -蛋清中加入30克糖，打发，要求最后提起后有直立的尖角 -去一部分蛋白和蛋糊混合均匀，混合好后倒入剩余的蛋白霜中 -<strong>翻拌（不能搅）</strong>，倒入模具中，整理表面平整 -包上锡纸，上方用牙签扎几个洞，放入炸锅，200度烤40min，然后快速揭掉锡纸180度再烤5分钟表面上色- 烤好后拿出倒扣，凉了后再脱模</p>]]></content>
      
      
      <categories>
          
          <category> 生活 </category>
          
          <category> 美食 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 菜谱 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pip和conda的镜像配置</title>
      <link href="/2025/01/10/pip-he-conda-de-jing-xiang-pei-zhi/"/>
      <url>/2025/01/10/pip-he-conda-de-jing-xiang-pei-zhi/</url>
      
        <content type="html"><![CDATA[<h2 id="pip">pip</h2><ul><li>文件路径：<code>~/.pip/pip.conf</code></li><li>配置内容： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple[install]trusted-host = https://pypi.tuna.tsinghua.edu.cn</code></pre></li><li>查看 镜像地址： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">pip config list</code></pre></li></ul><h2 id="conda">conda</h2><ul><li>文件路径：<code>~/.condarc</code></li><li>配置内容： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">channels:  - defaultsshow_channel_urls: truedefault_channels:  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2custom_channels:  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud  pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud  deepmodeling: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/</code></pre></li><li>查看 镜像地址： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">conda config --show</code></pre></li></ul><p><strong>注意</strong></p><p>镜像地址有可能会变化，当无法使用时记得最新的镜像地址。</p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pip </tag>
            
            <tag> conda </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>在docker容器中使用宿主机密钥</title>
      <link href="/2025/01/10/zai-docker-rong-qi-zhong-shi-yong-su-zhu-ji-mi-yao/"/>
      <url>/2025/01/10/zai-docker-rong-qi-zhong-shi-yong-su-zhu-ji-mi-yao/</url>
      
        <content type="html"><![CDATA[<p>在 Docker 容器中与远程 GitHub 建立安全连接时，直接在容器内生成新的SSH密钥存在一定的泄漏风险，特别是如果容器生命周期较短或需要共享容器的情况下。以下是几种安全的方法来处理这种场景：## 1. 使用宿主机已有的 SSH 密钥 将宿主机中已经配置好的 SSH密钥挂载到容器中，而不是在容器内生成新的密钥。这种方式可以避免密钥泄漏。- 在宿主机上确认已有的 SSH 密钥，通常路径是 ~/.ssh/id_rsa 和~/.ssh/id_rsa.pub。 - 在启动容器时将宿主机的 SSH 密钥挂载到容器中：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker run -it \    -v ~/.ssh:/root/.ssh:ro \     your_image_name</code></pre> -v ~/.ssh:/root/.ssh:ro：将宿主机的 .ssh目录挂载到容器中的/root/.ssh，并设置为只读（ro），避免容器篡改密钥文件。 - 在容器中验证SSH 连接： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">ssh -T git@github.com</code></pre> ## 2. 使用 SSH Agent Forwarding 通过 SSH Agent转发（SSH_AUTH_SOCK），让容器使用宿主机的 SSHAgent，而不直接暴露密钥文件。 - 启动 SSH Agent，确保宿主机中 SSH Agent正在运行： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">eval "$(ssh-agent -s)"ssh-add ~/.ssh/id_rsa</code></pre> - 添加密钥到 SSH Agent： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">ssh-add ~/.ssh/id_rsa</code></pre> -启动容器时，将宿主机的 SSH Agent Socket 挂载到容器： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker run -it --rm \    -v $SSH_AUTH_SOCK:/ssh-agent \    -e SSH_AUTH_SOCK=/ssh-agent \    your_image_name</code></pre><p></p><h2 id="使用环境变量传递-git-token">3.使用环境变量传递 GIT Token</h2><p>如果只需要使用 Git 而不需要完整的 SSH 功能，可以使用 GitHub 提供的Personal Access Token (PAT)。 - Settings &gt; Developer settings &gt;Personal access tokens。在 GitHub 上生成 Personal Access Token，勾选repo 权限。 - 在运行容器时，将 Token 作为环境变量传递： </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker run -it --rm \    -e GITHUB_TOKEN=your_personal_access_token \    your_image_name</code></pre> -在容器中，使用 HTTPS URL 结合 Token： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">git clone https://&lt;your_username&gt;:${GITHUB_TOKEN}@github.com/&lt;repo_owner&gt;/&lt;repo_name&gt;.git</code></pre><p></p>]]></content>
      
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
            <tag> github </tag>
            
            <tag> docker </tag>
            
            <tag> ssh </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git&amp;GitHub日常使用方法</title>
      <link href="/2025/01/09/git-github-ri-chang-shi-yong-fang-fa/"/>
      <url>/2025/01/09/git-github-ri-chang-shi-yong-fang-fa/</url>
      
        <content type="html"><![CDATA[<p>应该学过计算机的人都知道git和github，基本上伴随我们的整个编程生涯。但是博主由于并非科班出身，虽然日常都在用，但是一直对里面的一些概念和操作不是很清楚，每次遇到问题都是百度一下，解决了就忘记了。所以想通过这篇文章来梳理记录一下git和github的日常使用方法，方便自己查阅，也希望能帮助到有需要的人。</p><h2 id="git和github的概念">Git和GitHub的概念</h2><ul><li>Git 是一个分布式版本控制系统，用于管理代码版本和协作开发。</li><li>GitHub 是一个基于 Git的代码托管平台，提供了代码托管、版本管理、协作开发等功能。同时增加了界面化的功能，如Pull Requests、Issues 和 Wiki。</li></ul><h2 id="git与github的联合配置">git与github的联合配置</h2><ol type="1"><li>安装Git</li></ol><ul><li>访问 <a href="https://git-scm.com/">Git 官方网站</a>下载并安装适合操作系统的版本。</li><li>验证安装：在命令行输入<code>git --version</code>，显示版本号则安装成功。</li></ul><ol start="2" type="1"><li>配置Git</li></ol><ul><li>配置用户名：<code>git config --global user.name "Your Name"</code></li><li>配置邮箱：<code>git config --global user.email "your_email@example.com"</code></li></ul><ol start="3" type="1"><li>创建 GitHub 账户并设置 SSH 密钥</li></ol><ul><li>注册 GitHub 账户</li><li>生成 SSH密钥：<code>ssh-keygen -t ed25519 -C "your_email@example.com"</code></li><li>将 SSH 密钥添加到 GitHub 账户：将 <code>~/.ssh/id_ed25519.pub</code>文件中的内容添加到 GitHub 账户的 SSH keys 中。</li><li>验证GitHub连接：<code>ssh -T git@github.com</code><strong>如果你是在docker容器中连接github，为了避免SSH密钥的泄漏风险应该<a href="">使用宿主机已有的 SSH 密钥</a></strong><strong>注意.ssh目录中的权限不能修改，如果出现权限太过开放，则需要修改.ssh目录的权限</strong><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">chmod 700 ~/.sshchmod 600 ~/.ssh/id_ed25519chmod 644 ~/.ssh/id_ed25519.pub</code></pre></li></ul><ol start="4" type="1"><li>Git 与 GitHub 的连接</li></ol><ul><li>在 GitHub 上创建仓库，获取仓库地址（https或ssh地址）</li><li>在项目的本地目录中初始化仓库：<code>git init</code></li><li>添加远程仓库地址：<code>git remote add origin &lt;仓库地址&gt;  # 添加远程仓库</code></li><li>基本命令： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">git add .          # 添加所有文件到暂存区git commit -m "提交信息"  # 提交到本地仓库git push origin main  # 推送到远程仓库</code></pre> ### submodule 在 Git中，子模块（submodule） 是一个 Git 仓库嵌套在另一个 Git仓库中的工具。将另一个仓库的特定分支作为子模块加入当前仓库的流程如下：</li><li>在主仓库中添加子模块：<code>git submodule add -b &lt;分支名&gt; &lt;目标仓库地址&gt; &lt;子模块路径&gt;</code><ul><li>-b &lt;分支名&gt;: 指定目标仓库的分支。</li><li>&lt;目标仓库地址&gt;: 如：https://github.com/example/target-repo.gitsubmodules/target-repo</li><li>&lt;子模块路径&gt;：子模块在当前仓库中的存储路径,如submodules/target-repo</li></ul></li><li>初始化并更新子模块：<code>git submodule update --init --recursive</code></li><li>提交更改，添加子模块后，需要提交子模块的元信息到主仓库：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">git add .gitmodules submodules/target-repogit commit -m "Add submodule target-repo"</code></pre></li><li>如果目标仓库的指定分支发生更改，你可以更新子模块到最新状态：<code>git submodule update --remote</code></li><li>如果你是克隆一个包含子模块的仓库，可以使用<code>git clone --recurse-submodules &lt;仓库地址&gt;</code>选项来自动初始化和更新子模块。或者在正常克隆后执行<code>git submodule update --init --recursive</code>。</li><li>子模块也可以切换分支，需要进入子模块目录，执行：<code>git checkout &lt;自模块的分支名&gt;</code></li></ul><h3 id="典型工作流程">典型工作流程</h3><ol type="1"><li>克隆仓库：<code>git clone &lt;仓库地址&gt;</code></li><li>同步更新：<code>git pull origin main</code></li><li>分支管理：<ul><li>创建分支：<code>git branch &lt;分支名&gt;</code></li><li>切换分支：<code>git checkout &lt;分支名&gt;</code></li><li>合并分支：<code>git merge &lt;分支名&gt;</code> #要先切换到主分支</li><li>删除分支：<code>git branch -d &lt;分支名&gt;</code></li></ul></li><li>提交更改： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">git add .git commit -m "提交信息"git push origin main</code></pre></li></ol><h2 id="github高级功能">GitHub高级功能</h2><h3 id="fork">fork</h3><p>在 GitHub上，如果你直接clone了一个别人的仓库，会没有原仓库的写入权限（例如为他人开源项目贡献代码）。即使有权限，但团队页也可能有明确要求或想避免在原仓库中直接创建分支。所以fork就是为了解决这个问题的，fork的作用是在你的仓库中创建一个原仓库的副本，你可以在这个副本上自由修改，然后通过pullrequest向原仓库提交修改。 1. 在 GitHub 上点击仓库页面右上角的 Fork按钮，创建一个你的副本仓库。 2. 将 Fork 的仓库克隆到本地： </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">git clone &lt;你的 Fork 仓库地址&gt;</code></pre>3. 创建新分支： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">git checkout -b feature-branch # -b 表示创建并切换到新分支</code></pre> 4. 提交更改： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">git add .git commit -m "添加新功能"git push origin feature-branch</code></pre> 5. 在 GitHub上进入你的 Fork 仓库，点击 “New PullRequest”，选择将你的分支合并到原仓库的主分支（如 main）。<p></p><p><strong>Fork 的仓库能否跟踪原仓库的更改？</strong> Fork仓库可以跟踪原仓库的更改，但需要手动同步更新。 1. 添加原仓库的远程地址：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">git remote add upstream &lt;原仓库地址&gt;</code></pre> 2. 验证是否添加成功：<code>git remote -v</code> 输出类似：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">origin    &lt;你的 Fork 仓库地址&gt; (fetch)upstream  &lt;原仓库地址&gt; (fetch)</code></pre> 3. 同步原仓库的更新： - 从 upstream 拉取最新代码：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">git fetch upstream</code></pre> - 合并原仓库的主分支代码到你的主分支： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">git checkout maingit merge upstream/main</code></pre> -推送更新到你的 Fork 仓库： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">git push origin main</code></pre> <strong>如果 Fork仓库是用作完全独立的项目，且不再需要原仓库的更新或提交历史，可以清除.git重新初始化</strong>：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">rm -rf .gitgit initgit remote add origin &lt;你的 Fork 仓库地址&gt;git add .git commit -m "初始化"git push origin main# 此时Fork 仓库已经是一个全新的仓库，你没有办法再跟踪原仓库的更新了，也不能提交pull request</code></pre><p></p><h3 id="pull-requests">Pull Requests</h3><p>Pull Request (PR)是协作开发的重要工具，用于在分支之间或不同开发者的仓库之间合并代码改动，同时提供审阅、讨论和测试的机会。1. 创建 Pull Request - 确保你的改动已经推送到远程仓库中的某个分支（如feature-branch）。 - 在 GitHub 网站上，进入目标仓库，点击 “PullRequests”。 - 点击 “New Pull Request”。 - 选择： - BaseBranch：要合并到的主分支（如 main） - Compare Branch：你的开发分支（如feature-branch） 2. 审阅和讨论 - 代码审阅者会看到你的PR，提出评论或修改建议。 -你可以根据反馈进行改动，并通过提交新的代码到分支自动更新 PR。 3. 合并 PR- 审阅通过后，仓库管理员或 PR 创建者可以点击 “Merge Pull Request”。 -GitHub 提供多种合并方式： - Merge Commit（默认方式）：保留所有提交历史。- Squash and Merge：将所有提交合并为一个提交。 - Rebase andMerge：将分支历史与主分支合并，保留线性历史。 4. 关闭 PR - 如果 PR不再需要，可以选择关闭 PR，而不合并改动。</p><h3 id="issues">Issues</h3><p>Issues 是用于跟踪任务、记录 Bug或建议新功能的工具。它是协作和项目管理的重要部分。</p><ol type="1"><li>创建 Issue</li></ol><ul><li>在仓库主页，点击 “Issues”。</li><li>点击 “New Issue”。</li><li>填写 Issue 标题和描述。</li><li>添加标签（Labels）标记类型（如Bug、Enhancement）、分配人员（Assignees）和里程碑（Milestones）。</li></ul><ol start="2" type="1"><li>管理 Issue</li></ol><ul><li>可以对 Issue 进行评论和讨论。</li><li>可以将 Issue 关联到 PR 或者 Issue。</li></ul><ol start="3" type="1"><li>关闭 Issue</li></ol><ul><li>当 Issue 解决后，可以点击 “Close Issue” 关闭。</li><li>如果与 PR 关联，PR 合并时可以自动关闭 Issue（在 PR 描述中写 Fixes#<issue编号>）。</issue编号></li></ul><h3 id="actions">Actions</h3><p>GitHub Actions 是 GitHub提供的持续集成/持续部署（CI/CD）工具，用于自动化构建、测试和部署流程。1. 创建 Workflow - 在仓库中创建 <code>.github/workflows</code> 目录。 -在目录中创建 <code>.yml</code> 文件，定义 Workflow。示例：</p><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml">name: CIon:push:    branches:    - mainjobs:build:    runs-on: ubuntu-latest    steps:    - name: Checkout code        uses: actions/checkout@v2    - name: Set up Node.js        uses: actions/setup-node@v3        with:        node-version: 16    - name: Install dependencies        run: npm install    - name: Run tests        run: npm test</code></pre> 2. 触发 Workflow - 每次推送代码或创建 PR，Actions会自动运行。 - 运行结果可以在仓库的 “Actions” 标签页中查看。 3.使用现成模板 - GitHub 提供了许多现成的 Workflow模板，如代码格式检查、构建部署等，可以直接使用。<p></p><h3 id="projects">Projects</h3><p>Projects 是 GitHub 提供的项目管理工具，用于组织和跟踪任务、Issue 和PR。 - 创建项目 - 在仓库页面，点击 “Projects” - 点击 “New Project”。 -选择模板（如 Kanban Board）或自定义项目布局。 - 添加任务 - 将 Issues 或Pull Requests 直接添加到项目中。 -可以在项目中拖动任务，调整任务状态或优先级。 - 管理任务 -通过拖放操作移动任务到不同阶段（如 “To Do” -&gt; “In Progress” -&gt;“Done”）。 - 设置优先级、截止日期和责任人。</p><h3 id="wiki">Wiki</h3><p>Wiki 是一个仓库附带的文档存储工具，适用于记录项目文档、API说明或团队协作指南。</p><ul><li>在仓库页面，点击 “Wiki”。</li><li>点击 “Create the first page”。</li><li>编辑页面内容，支持 Markdown 语法。</li></ul>]]></content>
      
      
      <categories>
          
          <category> git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker常见命令</title>
      <link href="/2025/01/09/docker-chang-jian-ming-ling/"/>
      <url>/2025/01/09/docker-chang-jian-ming-ling/</url>
      
        <content type="html"><![CDATA[<ol type="1"><li><p>拉取镜像 </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker pull [镜像地址]#【镜像地址]一般由[镜像名]:[标签]组成，标签一般指的版本号，默认为latest。例如：ubuntu:20.04</code></pre><p></p></li><li><p>创建并运行新容器</p></li></ol><p>使用拉取的镜像创建并运行容器： </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker run -d --name &lt;容器名&gt; --gpus all -p &lt;主机端口&gt;:&lt;容器端口&gt; -v &lt;宿主机目录或卷名&gt;:&lt;容器目录&gt; &lt;镜像名&gt; sleep infinity# -d: 后台运行容器# --name: 指定容器名称# --gpus: 指定GPU# -p: 将主机端口映射到容器端口，格式为 主机端口:容器端口# -v: 将宿主机目录或数据卷挂载到容器目录。宿主机目录用于直接挂载本地文件系统，而数据卷提供更高的灵活性和隔离性。# sleep infinity: 保持容器运行# 如果容器以创建可以使用以下命令启动容器：docker start &lt;容器名或ID&gt;</code></pre><p></p><ol start="3" type="1"><li><p>查看容器 </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker ps # 查看正在运行的容器docker ps -a # 查看所有容器，包括已停止的</code></pre><p></p></li><li><p>进入容器 </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker exec -it &lt;容器名&gt; /bin/bash # 或者 /bin/sh # it: 是两个选项的组合: --interactive --tty# -i: 交互式操作# -t: 为容器分配一个伪终端</code></pre><p></p></li><li><p>停止和删除容器 </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker stop &lt;容器名或ID&gt;docker rm &lt;容器名或ID&gt;docker stop $(docker ps -aq) # 停止所有容器docker rm $(docker ps -aq) # 删除所有容器</code></pre><p></p></li><li><p>保存容器状态为新镜像 </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker commit &lt;容器名或ID&gt; &lt;新镜像名&gt;:&lt;标签&gt;</code></pre><p></p></li><li><p>查看镜像 </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker images</code></pre><p></p></li><li><p>删除镜像 </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker rmi &lt;镜像名或ID&gt;docker rmi -f &lt;镜像名或ID&gt; # 强制删除</code></pre><p></p></li><li><p>导出和导入镜像</p></li></ol><ul><li>导出镜像为文件： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker save -o &lt;文件名&gt;.tar &lt;镜像名或ID&gt;</code></pre></li><li>从文件导入镜像： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker load -i &lt;文件名&gt;.tar</code></pre></li></ul><ol start="10" type="1"><li>检查容器日志</li></ol><ul><li>查看容器运行日志： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker logs &lt;容器名或ID&gt;</code></pre></li><li>实时查看日志： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker logs -f &lt;容器名或ID&gt;</code></pre></li><li>查看容器内进程： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker top &lt;容器名或ID&gt;</code></pre></li><li>检查容器的详细信息： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker inspect &lt;容器名或ID&gt;</code></pre></li></ul><ol start="11" type="1"><li>清理无用的资源</li></ol><ul><li>清理未使用的镜像、容器、网络和卷： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker system prune</code></pre></li><li>清理未使用的卷： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker volume prune</code></pre></li></ul>]]></content>
      
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker的存储</title>
      <link href="/2025/01/09/docker-de-cun-chu/"/>
      <url>/2025/01/09/docker-de-cun-chu/</url>
      
        <content type="html"><![CDATA[<p><a href="https://docs.docker.com/engine/storage/">Docker的存储功能</a>分为几个主要部分，每个部分都有不同的作用和适用场景，以下是对这些部分的详细说明：## Volumes（卷） Volumes 是 Docker推荐的持久化存储方式，可以将容器数据独立存储在主机文件系统中。即使容器被删除，数据也不会丢失。<strong>设置卷的存储位置</strong> 卷会被存储在 Docker 的默认数据目录中：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">/var/lib/docker/volumes/&lt;volume_name&gt;/_data # LinuxC:\ProgramData\Docker\volumes\&lt;volume_name&gt;\_data # Windows</code></pre> 运行以下命令查看某个卷的具体路径： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker volume inspect my_volume</code></pre>如果希望将卷存储到非默认路径，可以通过修改 Docker 的配置文件（通常位于/etc/docker/daemon.json，没有的话可以新建）来实现： <pre class="line-numbers language-json" data-language="json"><code class="language-json">{    # 将 Docker 数据目录修改为 /home/docker。    "data-root": "/home/docker"}</code></pre>其实这里本质上是修改了 Docker的数据目录，卷的数据也会存储在这个目录中，我们可以通过<code>docker info</code>查看Docker 的根目录。<p></p><p><strong>切记</strong>如果之前已经有数据，需要将数据从<code>/var/lib/docker</code>迁移到新的目录<code>/home/docker</code>中。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">systemctl stop docker # 停止 Docker 服务rsync -avz /var/lib/docker /home/docker # 迁移数据systemctl start docker # 启动 Docker 服务</code></pre><p></p><p>设置后重启 Docker服务:<code>sudo systemctl restart docker</code>，所有新创建的卷都会存储在新的数据目录<code>/home/docker/volumes</code>中。可以在通过<code>docker volume inspect my_volume</code>查看卷的具体路径，或者<code>docker info</code>查看Docker 的根目录。</p><p><strong>卷可以被不同的容器共享，也可以在多个容器之间传递数据。</strong></p><p><strong>注意</strong> 1.不要直接修改卷目录中的文件：如果直接修改主机上的卷目录中的数据，可能导致数据损坏或无法被容器识别。2. 备份与迁移：可以通过主机上的路径对卷数据进行备份或迁移。</p><p><strong>使用方式</strong> - 创建一个 volume </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker volume create my_volume</code></pre><p></p><ul><li>在容器中挂载volume，<strong>通过挂载卷可以实现数据持久化，即使容器被删除，卷中的数据仍然保留。</strong><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker run -d --name my_container -v my_volume:/app/data my_image# -d: 分离模式，后台运行容器# --name: 容器名称# -v: 挂载 volume# my_image: 镜像名称</code></pre></li><li>查看 volume <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker volume ls</code></pre></li><li>删除 volume <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker volume rm my_volume</code></pre></li></ul><h2 id="bind-mounts绑定挂载">Bind mounts（绑定挂载）</h2><p>Bind Mount将主机上的文件或目录直接挂载到容器中，主机上的数据可以实时更新到容器中，反之亦然。</p><p><strong>使用方式</strong>-在运行容器时指定绑定挂载： </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker run -d --name my_container -v /path/on/host:/path/in/container my_image</code></pre><strong>适用场景</strong>： 1. 开发环境中需要实时同步主机和容器数据。 2.在主机上有特定目录需要容器访问（如配置文件）。<p></p><p><strong>注意</strong>：绑定挂载没有卷的隔离性和安全性，需谨慎操作主机文件。</p><h2 id="tmpfs-mounts临时文件系统挂载">tmpfsMounts（临时文件系统挂载）</h2><p>tmpfs挂载将数据存储在内存中，而不是磁盘上。数据是临时的，容器停止后数据会丢失。</p><p><strong>使用方式</strong>-在运行容器时使用 --tmpfs 选项：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker run -d --name my_container --tmpfs /app/data my_image# /app/data 是容器内部的路径，应用程序在该路径下操作数据。</code></pre> <strong>适用场景：</strong> 1.临时数据存储，例如缓存或临时文件。 2.对性能要求较高且数据无需持久化的场景。<p></p><h2 id="storage-drivers存储驱动">Storage Drivers（存储驱动）</h2><p>存储驱动管理容器和镜像的底层存储。每种存储驱动适用于不同的文件系统和操作场景。可以详情查看<a href="https://docs.docker.com/engine/storage/drivers/">官方文档</a>，有具体的图示和说明，对镜像和容器的存储方式有更深入的了解。</p><p><strong>常用驱动：</strong> 1. OverlayFS：默认驱动，适用于大多数Linux 发行版。 2. BTRFS：支持高级功能，如快照和压缩。 3.ZFS：高性能存储驱动，适用于复杂存储需求。 4. Device Mapper 和AUFS：较老的存储驱动。</p><p><strong>使用方式：</strong>- 检查当前存储驱动： </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker info | grep Storage</code></pre><p></p><h2 id="建议">建议</h2><p>Docker 提供了一些存储管理的建议： 1. 优先使用 Volumes：推荐用 Volumes来替代 Bind Mount，确保数据隔离性和安全性。 2.避免直接使用容器层存储：容器层的存储是临时的，适用于短期任务，不建议用于持久化数据。3. 定期清理未使用的卷：<code>docker volume prune</code>命令可以清理未使用的卷，释放磁盘空间。</p>]]></content>
      
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用CodeWithGPU实现模型的开箱即用</title>
      <link href="/2025/01/08/shi-yong-codewithgpu-shi-xian-mo-xing-de-kai-xiang-ji-yong/"/>
      <url>/2025/01/08/shi-yong-codewithgpu-shi-xian-mo-xing-de-kai-xiang-ji-yong/</url>
      
        <content type="html"><![CDATA[<h2 id="codewithgpu简介">CodeWithGPU简介</h2><p><a href="https://www.codewithgpu.com/image">CodeWithGPU</a>是一个AI镜像、模型技术社区（今后文档中简称CG)，它以GitHub为基础，在GitHub托管的代码下游，提供额外的镜像环境、算法模型等，解决以往构建环境难、管理与分享模型难的问题，为算法开发者、研究者等提供开箱即用的内容体验。&gt;CodeWithGPU和AutoDL同属于视拓云研发和运营的产品，两者账户通用，可以互相登录使用。### 镜像 CG上的镜像指能运行Github上某一具体项目代码所需的dockerimage环境。主要目的为方便其他用户免去构建环境安装依赖的过程，而直接在此基础之上训练模型。镜像来自用户在AutoDL上保存的dockerimage。(AutoDL是一个GPU算力租用平台，它和CG共用账户，即一个帐号可在两个平台中登录)</p><h3 id="模型">模型</h3><p>CG上的模型指用户使用算法在某数据集上训练出来的模型文件。使用的话首先需要安装：<code>pip install codewithgpu</code>可以由以下两种方式下载模型： 1. cli </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">cg down GuoFeng3/GuoFeng3.3.safetensors</code></pre> 2. 代码中集成<pre class="line-numbers language-python" data-language="python"><code class="language-python">import codewithgpu as cg cg.model.download("模型文件的KEY", target_directory="下载目标路径，可选"))</code></pre><p></p><h2 id="安装">安装</h2><h3 id="cuda检查">cuda检查</h3><p>检查电脑的nvidia驱动以及cuda toolkit。 </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">nvidia-sminvcc -V</code></pre>如果显示有：<code>Failed to initialize NVML: Driver/library version mismatch。</code>则说明驱动和cudatoolkit版本不匹配，需要重新安装驱动或者cuda toolkit。<p></p><p>或者你的cuda版本过低，建议也升级一下，不然CG很多新的镜像都无法使用。</p><p>如果你的cuda版本ok且驱动都正常的话，那么跳过这里直接看下面的安装cg-client。</p><h3 id="卸载旧版本nvida驱动和cuda-toolkit">卸载旧版本nvida驱动和cudatoolkit</h3><ul><li><p>卸载nvidia驱动：<code>sudo nvidia-uninstall</code></p></li><li><p>卸载cuda toolkit：找到 CUDA 工具包的安装目录（例如/usr/local/cuda-10.1/），然后执行： </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">cd /usr/local/cuda-10.1/bin/sudo ./cuda-uninstaller</code></pre> 如果你忘记了 CUDA工具包的安装目录，可以使用以下命令找到： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">where nvcc</code></pre>或者通过之前安装cuda设置的环境变量查看： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">echo $PATH | tr ':' '\n' | grep cudaecho $LD_LIBRARY_PATH | tr ':' '\n' | grep cuda</code></pre>如果没有找到cuda-uninstaller，可以手动删除： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo rm -rf /usr/local/cuda-11.8/   # 删除目录# 清理其他残留文件：sudo apt-get remove --purge '^nvidia-.*'sudo apt-get remove cuda*sudo apt-get autoremove# 查看剩余残留sudo dpkg -l |grep cuda# 继续卸载哈哈sudo dpkg -P 残留文件全称# 环境变量删除：编辑～/.bashrc 或者  /etc/profile文件中是否有 CUDA 相关路径， 删除以下内容export PATH=/usr/local/cuda-11.8/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH# 保存source ~/.bashrc</code></pre><p></p></li><li><p>完成卸载后，删除 CUDA 工具包的目录： </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo rm -rf /usr/local/cuda-10.1/</code></pre> ###安装新版本nvidia驱动<p></p></li><li><p>前往 NVIDIA <a href="https://www.nvidia.cn/Download/index.aspx">官方网站</a>，根据显卡型号（例如2080Ti）下载最新的驱动程序。</p></li><li><p>将下载的驱动程序（例如NVIDIA-Linux-x86_64-550.54.14.run）上传到服务器的某个目录，然后执行：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo sh NVIDIA-Linux-x86_64-550.54.14.run</code></pre><p></p></li><li><p>安装完成后，重启服务器： </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo reboot</code></pre><p></p></li><li><p>重启后，检查驱动是否安装成功： </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">nvidia-smi</code></pre><p></p></li></ul><h3 id="安装新版本cuda-toolkit">安装新版本cuda toolkit</h3><p>注意安装的cuda toolkit版本要和nvidia驱动版本匹配： </p><pre class="line-numbers language-none"><code class="language-none">- nvidia-smi属于driver API、nvcc属于runtime API。- 用于支持driver API的必要文件(如libcuda.so)是由GPU driver installer安装的。- 用于支持runtime API的必要文件(如libcudart.so以及nvcc)是由CUDA Toolkit installer安装的。- 如果只安装driver API，不安装runtime API（cuda toolkit），也能正常使用pytorch，但涉及到一些需要编译安装的更底层的工具，比如apex、deepspeed，就会报错，这时还需要床runtime API。</code></pre> -前往 NVIDIA <a href="https://developer.nvidia.com/cuda-downloads">开发者网站</a>，根据操作系统和需求下载与新驱动兼容的CUDA 工具包（例如 CUDA 12.1.1）。 - 将下载的 CUDA 安装包（例如cuda_12.1.1_530.30.02_linux.run）上传到服务器的某个目录，然后执行：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo sh cuda_12.1.1_530.30.02_linux.run</code></pre> <strong>注意</strong>：由于已经有CUDADriver12.4了，因此在安装过程中不需要勾选CUDA Driver。 -安装完成后，设置环境变量：（如果之前安装过旧版本记得删除旧版本的环境变量） <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">echo 'export PATH=/usr/local/cuda-12.1/bin:$PATH' &gt;&gt; ~/.bashrcecho 'export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH' &gt;&gt; ~/.bashrcsource ~/.bashrc</code></pre> -检查CUDA是否安装成功： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">nvcc -V</code></pre><p></p><h3 id="cg-client">cg-client</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"># 下载客户端并添加可执行权限wget -O cg-client https://codewithgpu.ks3-cn-beijing.ksyuncs.com/cg-clientchmod +x cg-client# 启动客户端，需要使用到sudo权限否则有docker使用上的异常sudo ./cg-client</code></pre><h3 id="web上使用">web上使用</h3><p>启动<code>cg-client</code>后会输出类似如下包含访问地址的日志：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">**************cg-client is running on http://localhost:2022/container?token=E5PR11vT9MMbrpwFeIdLcltRNnUmaY**************</code></pre>在浏览器中打开该地址后，客户端会自动检测是否有Docker环境，点击安装nvidia-docker按钮自动安装相关环境<p></p><p><strong>docker的权限配置</strong></p><p>在使用docker时，如果不想每次都使用sudo，可以将当前用户加入docker组：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo groupadd docker # 添加docker组sudo gpasswd -a ${USER} docker # 将当前用户加入docker组sudo systemctl restart docker # 重启docker服务sudo chmod a+rw /var/run/docker.sock # 修改docker.sock权限</code></pre><p></p><p><strong>如果使用ssh连接的服务器使用则需要设置端口转发</strong></p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">ssh -CNg -L 2022:localhost:2022 user@host -p port</code></pre> 参数说明： -C:启用压缩。这对于减少传输数据量有帮助，特别是在带宽较低的情况下。<p></p><p>-N: 不执行远程命令，只建立隧道。适合仅用于端口转发的场景。</p><p>-g: 允许其他主机通过本机（运行此命令的机器）的 7890 端口访问隧道。如果不需要其他机器访问，可以省略这个参数。</p><p>-L: 本地端口转发。格式为 -L [本地端口]:[目标地址]:[目标端口]</p><p>CG开始使用时需要安装docker/nvidia-docker环境，web界面会自动检测是否有Docker环境，点击安装nvidia-docker按钮自动安装相关环境，但是我使用时发现我原本安装的docker环境在使用CG提供的脚本安装后就失效了，所以建议还是卸载原有的docker环境后再使用CG提供的脚本安装</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250108033158.png">这样就成功啦</p><h2 id="使用">使用</h2><p>在web界面可以实现容器的创建、启动、停止、删除等操作，也可以上传文件到容器中，也可以在容器中打开终端，执行命令，查看日志等操作。</p><h3 id="docker的存储">docker的存储</h3><p>在容器内我们会发现系统盘大小为本地根目录的大小，这是因为Docker默认安装的情况下，会使用<code>/var/lib/docker/</code>目录作为存储目录，用以存放拉取的镜像和创建的容器等。</p><p>而autodl-tmp未挂载，虽然CG提供了网盘的支持，但是如果我们想适用宿主机中的磁盘分区的话，还得会一些docker的存储知识（见博客-<a href="https://baoblei.github.io/2025/01/09/docker-de-cun-chu/">docker的存储</a>）</p><p>在了解了docker的存储后，我们可以通过<code>docker run</code>命令的<code>-v</code>参数来挂载宿主机的磁盘分区到容器中，这样就可以在容器中使用宿主机的磁盘分区了。在web界面创建容器时也提供了挂载的选项，可以直接在web界面中挂载。</p><h3 id="镜像与容器的基本用法">镜像与容器的基本用法</h3><p>具体见博客-<a href="https://baoblei.github.io/2025/01/09/docker-chang-jian-ming-ling/">docker的常见命令</a>，这里简单通过本地终端创建并运行一个容器来说明：- 拉取镜像 </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker pull registry.cn-beijing.aliyuncs.com/codewithgpu2/hvision-nku-storydiffusion:n3iFx8w7UJ</code></pre> - 查看镜像 <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker images</code></pre> - 创建并运行容器<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo docker run -d --name quizzical_wilson --gpus all -v data80:/root/autodl-tmp registry.cn-beijing.aliyuncs.com/codewithgpu2/hvision-nku-storydiffusion:n3iFx8w7UJ sleep infinity# 这里我挂载了一个卷data80到容器的/root/autodl-tmp目录sudo docker exec -it quizzical_wilson /bin/bash</code></pre>到这里就可以在容器中进行操作了，可以使用<code>exit</code>退出容器。 -停止容器 <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker stop quizzical_wilson</code></pre><p></p><p><strong>参考:</strong> &gt;CG官方文档：https://www.codewithgpu.com/docs/</p><blockquote><p>docker engine文档：https://docs.docker.com/engine/</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> CG </category>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> CG </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>通过公网访问局域网内服务器</title>
      <link href="/2025/01/07/tong-guo-gong-wang-fang-wen-ju-yu-wang-nei-fu-wu-qi/"/>
      <url>/2025/01/07/tong-guo-gong-wang-fang-wen-ju-yu-wang-nei-fu-wu-qi/</url>
      
        <content type="html"><![CDATA[<p>分为两种情况： ## 拥有路由器管理权限 1.MAC地址绑定：在路由器中绑定服务器的MAC地址，使其获得固定的内网IP地址。</p><ol start="2" type="1"><li>端口映射：在路由器中设置端口映射，将公网IP的某个端口映射到服务器的内网IP的某个端口上。</li></ol><ul><li>找到路由器管理页面中的 转发规则、NAT 设置或类似选项（名称因品牌而异）。</li><li>添加一条新的端口转发规则：<ul><li>外部端口（External Port）：在公网访问时使用的端口（例如 80）。</li><li>内部端口（Internal Port）：服务器监听的端口（例如 80）。</li><li>内部 IP 地址（Internal IP）：服务器的 IP 地址（例如</li><li>协议类型（Protocol）：选择 TCP、UDP 或 TCP/UDP（通常选择 TCP 或TCP/UDP）。</li><li>保存设置。</li></ul></li></ul><ol start="3" type="1"><li><p>查看公网ip地址：在路由器状态页或通过IP 检测网站查看路由器的公网IP 地址。</p></li><li><p>测试，例如ssh登录：<code>ssh -p &lt;公网端口&gt; user@&lt;公网IP&gt;</code>（端口转发设置的内网端口为22）。</p></li></ol><p><strong>注意</strong> - 如果你的公网IP是一个私有IP地址（如192.168.x.x、172.16.x.x、10.x.x.x），那么你的路由器可能处于双重 NAT环境中。在这种情况下，你需要拥有公网IP地址的路由器才能进行端口映射。 -一般路由器重启后公网ip会变化。可以使用动态域名解析服务（DDNS）或者联系互联网提供商ISP来解决这个问题。或者通过一个脚本定期检查公网IP地址并通过某个外部服务器或邮件通知自己。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">#!/bin/bashIP=$(curl -s ifconfig.me) # 获取公网IPecho "当前公网IP是：$IP"# 发送邮件或更新外部服务curl -X POST -d "ip=$IP" http://yourserver.com/update_ip</code></pre><p></p><blockquote><p>动态域名解析（DDNS）服务 动态域名解析（DDNS）允许你将动态公网 IP绑定到一个固定的域名，当 IP 变化时，DDNS 服务会自动更新域名指向的 IP地址。</p></blockquote><p>配置步骤： 1. 注册 DDNS 服务 - 选择一个 DDNS提供商（通常是免费的或附带在路由器服务中），如： - No-IP - DynDNS -路由器品牌自带的 DDNS 服务（如 TP-Link、华为等）。 - 注册一个域名，例如myserver.ddns.net。</p><ol start="2" type="1"><li>配置路由器的 DDNS</li></ol><ul><li>登录路由器管理界面。</li><li>找到 DDNS 设置（通常在网络或高级设置下）。</li><li>输入 DDNS 提供商信息：<ul><li>服务提供商（例如 No-IP、DynDNS）。</li><li>注册的域名（例如 myserver.ddns.net）。</li><li>登录凭据（用户名和密码）。</li><li>启用 DDNS。</li></ul></li></ul><ol start="3" type="1"><li>验证配置</li></ol><ul><li>检查DDNS状态，确认域名已成功绑定到当前公网IP。</li><li>之后，通过 http://myserver.ddns.net:&lt;端口号&gt; 访问你的服务器。</li></ul><h2 id="无路由器管理权限">无路由器管理权限</h2><p><strong>使用第三方内网穿透服务</strong>，绕过公网 IP 限制。例如： -<strong>Ngrok</strong>：通过创建一个临时的公网地址，将流量转发到你的服务器。- <strong>Cloudflare Tunnel（Argo Tunnel）</strong>：直接通过 Cloudflare提供的域名访问你的服务器。 - <strong>Tailscale /ZeroTier</strong>：创建虚拟私有网络（VPN）实现远程访问。这些服务可以帮助你在无法访问路由器设置的情况下，实现公网访问内网服务器的需求。但一般需要收费。</p>]]></content>
      
      
      <categories>
          
          <category> 网络配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 端口转发 </tag>
            
            <tag> 内网穿透 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux文件、磁盘以及扩容方式</title>
      <link href="/2025/01/07/linux-wen-jian-ci-pan-yi-ji-kuo-rong-fang-shi/"/>
      <url>/2025/01/07/linux-wen-jian-ci-pan-yi-ji-kuo-rong-fang-shi/</url>
      
        <content type="html"><![CDATA[<h2 id="预备知识">预备知识</h2><h3 id="文件系统类型">文件系统类型</h3><p>通过<code>df -T /home</code>或者<code>lsblk -f</code>命令可以查看文件系统类型。</p><ul><li><strong>ext4</strong>：Linux常用的文件系统，支持大文件和高性能。</li><li><strong>XFS</strong>：用于高性能、高并发的场景。</li><li><strong>btrfs</strong>：支持高级功能，如快照和子卷。</li><li><strong>FAT32 和NTFS</strong>：可用作跨平台兼容，但通常用于外部设备。</li></ul><h3 id="挂载点">挂载点</h3><ul><li><strong>挂载点</strong>：将磁盘分区与目录关联，使得目录中的文件实际存储在对应的磁盘分区中。#### 挂载与卸载</li><li><strong>挂载</strong>：<code>sudo mount /dev/sdXn /mnt/new_home</code>,/dev/sdXn 是分区路径，/mnt/new_home 是临时挂载点。</li><li><strong>卸载</strong>：<code>sudo umount /mnt/new_home</code>。</li></ul><h4 id="查看当钱挂载情况">查看当钱挂载情况</h4><p><code>mount | grep /home</code></p><p><code>lsblk</code></p><h4 id="持久化挂载配置">持久化挂载配置</h4><p>编辑 /etc/fstab，添加新分区的信息，例如： </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">UUID=xxxxx-xxxxx-xxxxx /home ext4 defaults 0 2</code></pre> 使用<code>blkid</code> 获取分区 UUID。 UUID=xxxxx-xxxxx-xxxxx 是分区的UUID，/home 是挂载点，ext4 是文件系统类型，defaults 是默认挂载选项，0是备份级别，2 是文件系统检查顺序。<p></p><h3 id="分区表类型">分区表类型</h3><p>分区表类型决定了磁盘分区的布局方式 - MBR（Master Boot Record）： -支持最大 2TB 磁盘，分区数量限制为 4 个主分区（或使用扩展分区）。 -较旧的系统中常见。 - GPT（GUID Partition Table）： - 支持大于 2TB的磁盘，分区数量几乎无限制。 - 推荐使用GPT，尤其是现代系统和大容量磁盘。</p><p>如何查看分区表类型： </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo fdisk -l</code></pre>输出中会显示磁盘使用的分区表类型（MBR 或 GPT）。<p></p><h3 id="逻辑卷管理lvm">逻辑卷管理（LVM）</h3><p>LVM（Logical Volume Manager）提供了灵活的存储管理能力，是现代 Linux系统推荐的存储管理工具。 #### LVM 组成： <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250107042326.png">- Physical Volume (PV)：物理卷，对应实际磁盘分区（如 /dev/sda1）。 -Volume Group (VG)：卷组，将多个 PV 组合成一个存储池。 - Logical Volume(LV)：逻辑卷，在卷组中划分出的存储块，类似于分区。 #### LVM 的优势： -<strong>动态调整</strong>：支持动态调整（扩展或缩小）逻辑卷。 -<strong>存储池</strong>：可以将多个物理磁盘组合成一个逻辑存储池。 -<strong>快照和备份</strong>：提供快照和备份功能。 #### 如何查看 LVM 配置</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo vgdisplaysudo lvdisplay</code></pre><p></p><h3 id="块设备路径">块设备路径</h3><p>块设备路径是 Linux 用于标识磁盘和分区的名称。 #### 路径格式 -/dev/sdX：传统磁盘路径，X 是磁盘序号。 - /dev/nvmeXnY：NVMe 磁盘路径，X是设备号，Y 是分区号。 - /dev/mapper/vg_name-lv_name：LVM 的逻辑卷路径。#### 如何查看块设备路径： </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">lsblk 查看磁盘和分区的结构。blkid 查看分区的文件系统和 UUID。</code></pre>可以通过<code>fdisk / parted</code> 等工具管理分区。<p></p><h2 id="linux和windows在磁盘分区管理上的区别">Linux和Windows在磁盘分区管理上的区别</h2><h3 id="分区类型和布局">分区类型和布局</h3><table><colgroup><col style="width: 3%"><col style="width: 29%"><col style="width: 38%"><col style="width: 28%"></colgroup><thead><tr class="header"><th><strong>系统</strong></th><th><strong>分区类型</strong></th><th><strong>分区结构</strong></th><th><strong>挂载点</strong></th></tr></thead><tbody><tr class="odd"><td><strong>Linux</strong></td><td>-使用主分区（Primary）、扩展分区（Extended）和逻辑分区（Logical）的结构<br>-GPT（GUID分区表）支持 128 个以上的分区</td><td>- 分区通常用设备文件表示，如<code>/dev/sda1</code>（表示第一个硬盘的第一个分区）<br>-文件系统可以跨越多个分区（如通过 LVM 实现逻辑卷管理）</td><td>- 无固定的驱动器符号（如 Windows 的 C:、D:）<br>-所有分区整合到一个目录树下，例如根目录 <code>/</code></td></tr><tr class="even"><td><strong>Windows</strong></td><td>- 支持 MBR（最多 4 个主分区）和 GPT（支持更多分区）</td><td>- 分区通常表示为独立的驱动器符号，如 C:、D:</td><td>- 每个分区是独立的逻辑单元<br>- 支持通过挂载点功能将分区挂载为 NTFS文件系统下的文件夹，但不常用</td></tr></tbody></table><h3 id="文件系统">文件系统</h3><table><colgroup><col style="width: 5%"><col style="width: 52%"><col style="width: 41%"></colgroup><thead><tr class="header"><th><strong>系统</strong></th><th><strong>常见文件系统</strong></th><th><strong>灵活性</strong></th></tr></thead><tbody><tr class="odd"><td><strong>Linux</strong></td><td>- <strong>ext4</strong>: 现代 Linux的默认文件系统，支持大文件和高性能<br>- <strong>XFS</strong>:用于高性能、高并发的场景<br>- <strong>btrfs</strong>:支持高级功能，如快照和子卷<br>- <strong>FAT32 和 NTFS</strong>:可用作跨平台兼容，但通常用于外部设备</td><td>- 多种文件系统可选，支持格式化为几乎任何文件系统<br>-支持网络文件系统（如 NFS、CIFS）</td></tr><tr class="even"><td><strong>Windows</strong></td><td>- <strong>NTFS</strong>: 默认的主文件系统，支持高性能和安全功能<br>-<strong>FAT32 和 exFAT</strong>: 用于外部设备或某些特殊用途</td><td>- 不支持直接格式化为 Linux 原生文件系统（如 ext4）<br>-文件系统选项相对较少</td></tr></tbody></table><h3 id="引导加载">引导加载</h3><table><colgroup><col style="width: 9%"><col style="width: 90%"></colgroup><thead><tr class="header"><th><strong>系统</strong></th><th><strong>特点</strong></th></tr></thead><tbody><tr class="odd"><td><strong>Linux</strong></td><td>- 使用 GRUB（或其他引导程序）管理启动项<br>-支持从多种分区或系统启动，灵活性高<br>- 根分区（/）和引导分区（如/boot）可以分离<br>- 支持双系统或多系统并存</td></tr><tr class="even"><td><strong>Windows</strong></td><td>- 使用 Windows Boot Manager 管理引导<br>-通常依赖特定的系统分区（System Reserved 或 EFI 分区）<br>-双系统支持有限，需第三方引导管理器（如 GRUB）协助</td></tr></tbody></table><h3 id="分区工具">分区工具</h3><table><colgroup><col style="width: 8%"><col style="width: 91%"></colgroup><thead><tr class="header"><th><strong>系统</strong></th><th><strong>特点</strong></th></tr></thead><tbody><tr class="odd"><td><strong>Linux</strong></td><td>- 命令行工具丰富：<br> - <strong>fdisk</strong>: 适用于 MBR分区表<br> - <strong>gdisk</strong>: 适用于 GPT 分区表<br> -<strong>parted 和 gparted</strong>: 高级分区管理工具<br>-灵活支持命令行和图形界面<br>- 提供LVM（逻辑卷管理）支持，便于动态调整分区大小</td></tr><tr class="even"><td><strong>Windows</strong></td><td>- 图形化工具为主：<br> - <strong>磁盘管理</strong>（DiskManagement）是主要的分区工具<br> - 命令行工具如<strong>diskpart</strong><br>- 相较Linux，动态调整分区大小的灵活性较低</td></tr></tbody></table><h3 id="文件路径和符号">文件路径和符号</h3><table><colgroup><col style="width: 9%"><col style="width: 90%"></colgroup><thead><tr class="header"><th><strong>系统</strong></th><th><strong>特点</strong></th></tr></thead><tbody><tr class="odd"><td><strong>Linux</strong></td><td>- 文件路径区分大小写，例如 <code>/home</code> 和 <code>/Home</code>是不同的路径<br>- 使用正斜杠 <code>/</code> 作为路径分隔符<br>-支持符号链接和硬链接</td></tr><tr class="even"><td><strong>Windows</strong></td><td>- 文件路径不区分大小写，例如 <code>C:\Users</code> 和<code>C:\users</code> 是同一路径<br>- 使用反斜杠 <code>\</code>作为路径分隔符<br>- 支持快捷方式（功能类似于符号链接）</td></tr></tbody></table><h2 id="linux-根目录">Linux 根目录</h2><p>根据 <strong>Filesystem Hierarchy Standard (FHS)</strong>，Linux的根目录 <code>/</code>下存在一系列标准化的目录，以下是常见目录及其作用说明。</p><h3 id="bin-binary">1. <code>/bin</code> （Binary）</h3><ul><li>存放系统启动时或单用户模式下的基本用户命令。</li><li>包括常用的命令，如<code>ls</code>、<code>cp</code>、<code>mv</code>、<code>cat</code>等。</li><li>可供所有用户访问。</li><li><hr></li></ul><h3 id="boot-boot">2. <code>/boot</code> （Boot）</h3><ul><li>存放系统启动所需的文件，包括引导加载程序（如GRUB）的配置文件和内核文件。</li><li>常见文件：<ul><li><code>vmlinuz</code>：压缩内核文件。</li><li><code>initrd.img</code>：初始 RAM 磁盘镜像。</li><li><code>grub</code> 目录：GRUB 引导加载程序的配置文件。</li></ul></li></ul><hr><h3 id="dev-device">3. <code>/dev</code> （Device）</h3><ul><li>包含设备文件，表示系统中的硬件设备。</li><li>设备文件通常通过 <code>/dev</code> 提供接口，例如：<ul><li><code>/dev/sda</code>：第一个 SATA 硬盘。</li><li><code>/dev/null</code>：空设备。</li><li><code>/dev/tty</code>：终端接口。</li></ul></li></ul><hr><h3 id="etc-et-cetera">4. <code>/etc</code> （Et Cetera）</h3><ul><li>存放系统配置文件。</li><li>通常只由系统管理员（<code>root</code> 用户）修改。</li><li>常见文件和目录：<ul><li><code>/etc/passwd</code>：用户信息。</li><li><code>/etc/fstab</code>：文件系统挂载信息。</li><li><code>/etc/network/interfaces</code>：网络配置。</li></ul></li></ul><hr><h3 id="home">5. <code>/home</code></h3><ul><li>存放普通用户的主目录，每个用户一个子目录。</li><li>示例：<ul><li><code>/home/user1</code>：用户 <code>user1</code> 的主目录。</li></ul></li><li>用户可以在其主目录中存储文件、配置数据等。</li></ul><hr><h3 id="lib-library">6. <code>/lib</code> （Library）</h3><ul><li>存放系统运行所需的共享库（类似于 Windows 的 <code>.dll</code>文件）。</li><li>包括 <code>/bin</code> 和 <code>/sbin</code>中的可执行文件所依赖的库。</li></ul><hr><h3 id="media">7. <code>/media</code></h3><ul><li>挂载点目录，用于自动挂载外部设备（如 USB 驱动器、光盘）。</li><li>当插入外部存储设备时，系统会在 <code>/media</code>下创建一个挂载点。</li></ul><hr><h3 id="mnt">8. <code>/mnt</code></h3><ul><li>临时挂载点目录。</li><li>系统管理员可以手动挂载文件系统到该目录下。</li><li>用于临时挂载而非自动化。</li></ul><hr><h3 id="opt-optional">9. <code>/opt</code> （Optional）</h3><ul><li>存放可选的第三方软件包。</li><li>一些软件会安装在 <code>/opt</code> 下，例如<code>/opt/myapp</code>。</li></ul><hr><h3 id="proc">10. <code>/proc</code></h3><ul><li>虚拟文件系统，存储系统内核和进程相关信息。</li><li>动态生成，不占用实际磁盘空间。</li><li>示例：<ul><li><code>/proc/cpuinfo</code>：CPU 信息。</li><li><code>/proc/meminfo</code>：内存使用信息。</li></ul></li></ul><hr><h3 id="root">11. <code>/root</code></h3><ul><li><code>root</code> 用户的主目录，与普通用户的<code>/home/username</code> 对应。</li><li>通常只有 <code>root</code> 用户有权限访问。</li><li>需要通过<code>sudo passwd root</code>设置密码才能登录。</li></ul><hr><h3 id="run">12. <code>/run</code></h3><ul><li>临时文件系统，用于存储系统运行时生成的文件（如进程 ID文件、套接字）。</li><li>系统重启后该目录会被清空。</li></ul><hr><h3 id="sbin-system-binary">13. <code>/sbin</code> （SystemBinary）</h3><ul><li>存放系统管理员使用的基本命令，例如：<ul><li><code>fsck</code>：文件系统检查。</li><li><code>reboot</code>：重启系统。</li></ul></li><li>一般普通用户无权直接执行。</li></ul><hr><h3 id="srv-service">14. <code>/srv</code> （Service）</h3><ul><li>存储与系统提供的服务（如 HTTP、FTP）相关的数据。</li><li>示例：<code>/srv/www</code> 可能存放一个 Web 服务器的数据。</li></ul><hr><h3 id="sys">15. <code>/sys</code></h3><ul><li>虚拟文件系统，提供内核和硬件设备的信息。</li><li>和 <code>/proc</code> 类似，但更注重设备相关的信息。</li></ul><hr><h3 id="tmp-temporary">16. <code>/tmp</code> （Temporary）</h3><ul><li>用于存储临时文件。</li><li>文件通常会在系统重启时清空。</li></ul><hr><h3 id="usr-user-system-resources">17. <code>/usr</code> （User SystemResources）</h3><ul><li>包含与用户相关的文件和资源：<ul><li><code>/usr/bin</code>：用户可用的命令。</li><li><code>/usr/lib</code>：共享库。</li><li><code>/usr/share</code>：共享数据（如文档、图标）。</li><li><code>/usr/local</code>：本地安装的软件。</li></ul></li></ul><hr><h3 id="var-variable">18. <code>/var</code> （Variable）</h3><ul><li>存放经常变化的文件。</li><li>示例：<ul><li><code>/var/log</code>：日志文件。</li><li><code>/var/spool</code>：任务队列（如打印任务）。</li></ul></li></ul><hr><h2 id="ubuntu-系统分区建议">Ubuntu 系统分区建议</h2><h3 id="基本分区方案">基本分区方案</h3><p>通常情况下，建议以下几个分区：</p><h4 id="根分区">1. <code>/</code>（根分区）</h4><ul><li><strong>用途</strong>：存放操作系统的核心文件和默认安装的软件。</li><li><strong>建议大小</strong>：<ul><li><strong>20-30GB</strong>：普通桌面用户。</li><li><strong>40-50GB</strong>：需要安装大量软件或开发环境的用户。</li></ul></li><li><strong>文件系统</strong>：<code>ext4</code>（推荐）。</li><li><strong>注意</strong>：所有未分配到其他分区的内容都会默认存储在根分区中。</li></ul><h4 id="swap交换分区">2. <code>swap</code>（交换分区）</h4><ul><li><strong>用途</strong>：虚拟内存，用于内存不足时提供临时存储；也用于休眠功能。</li><li><strong>建议大小</strong>：<ul><li>如果 <strong>不使用休眠功能</strong>：<ul><li>RAM &lt; 4GB：<strong>等于2倍RAM大小</strong>。</li><li>RAM 在 4-8GB：<strong>等于RAM大小</strong>。</li><li>RAM &gt; 8GB：<strong>4-8GB</strong> 通常足够。</li></ul></li><li>如果 <strong>使用休眠功能</strong>：<ul><li>至少等于RAM大小。</li></ul></li></ul></li><li><strong>文件系统</strong>：专用交换空间，无文件系统。</li><li><strong>注意</strong>：现代 Ubuntu支持使用交换文件（无需单独分区），可在安装后配置。</li></ul><h4 id="home用户目录">3. <code>/home</code>（用户目录）</h4><ul><li><strong>用途</strong>：存储用户数据（如文档、配置、下载等），便于系统重装时保留用户数据。</li><li><strong>建议大小</strong>：<ul><li>普通桌面用户：<strong>50GB 或更多</strong>。</li><li>如果硬盘空间充裕，可分配<strong>100GB+</strong>，以支持长期使用。</li><li>服务器或开发用户：按实际需求分配。</li></ul></li><li><strong>文件系统</strong>：<code>ext4</code>（推荐）。</li></ul><h4 id="boot可选">4. <code>/boot</code>（可选）</h4><ul><li><strong>用途</strong>：存放引导加载程序和内核文件。</li><li><strong>建议大小</strong>：<ul><li><strong>500MB-1GB</strong>：普通用户。</li><li><strong>1-2GB</strong>：如果需要保留多个内核版本。</li></ul></li><li><strong>文件系统</strong>：<code>ext4</code>。</li></ul><h4 id="efi如果是-uefi-系统">5. <code>/efi</code>（如果是 UEFI系统）</h4><ul><li><strong>用途</strong>：存放 UEFI 引导程序。</li><li><strong>建议大小</strong>：<ul><li><strong>300-500MB</strong> 通常足够。</li></ul></li><li><strong>文件系统</strong>：<code>FAT32</code>。</li><li><strong>注意</strong>：如果是多系统环境，EFI 分区可以共享。</li></ul><h4 id="var可选">6. <code>/var</code>（可选）</h4><ul><li><strong>用途</strong>：存放日志文件、缓存数据、邮件队列等（特别是服务器环境）。</li><li><strong>建议大小</strong>：<ul><li><strong>5-10GB</strong>：普通用户。</li><li><strong>20-50GB</strong>：如果运行数据库、Web服务器或其他高日志生成服务。</li></ul></li><li><strong>文件系统</strong>：<code>ext4</code>。</li></ul><h4 id="tmp可选">7. <code>/tmp</code>（可选）</h4><ul><li><strong>用途</strong>：存放临时文件。</li><li><strong>建议大小</strong>：<ul><li><strong>2-4GB</strong>：普通用户。</li><li><strong>10GB或更多</strong>：需要处理大量临时文件（如视频编辑、编译大型项目）。</li></ul></li><li><strong>文件系统</strong>：<code>ext4</code> 或<code>tmpfs</code>（内存文件系统）。</li></ul><h4 id="数据分区可选">8. 数据分区（可选）</h4><ul><li><strong>用途</strong>：独立存储大文件或共享数据（如<code>/mnt/data</code>）。</li><li><strong>建议大小</strong>：根据需求调整。</li></ul><hr><h3 id="分区建议">分区建议</h3><ol type="1"><li><strong>根分区大小要合理</strong>：<ul><li>根分区存放系统和软件文件，除单独分配空间的挂载点，其余内容都存储在根分区中。</li><li>如果空间不足，可能导致系统无法更新或安装新软件。</li></ul></li><li><strong>优先为 <code>/home</code> 留出空间</strong>：<ul><li><code>/home</code>是用户数据的主要存储位置，建议分配尽可能大的空间。</li></ul></li><li><strong>使用 LVM</strong>：<ul><li>如果磁盘使用量不确定，使用 LVM 便于后续扩展。</li></ul></li><li><strong>EFI 分区共享</strong>：<ul><li>如果有多系统（如 Windows 和 Linux），可以共享一个<code>/boot/efi</code> 分区。</li></ul></li><li><strong>根据用途优化分区</strong>：<ul><li>桌面用户可以简化分区结构。</li><li>服务器或开发环境可以根据需要细化分区。</li></ul></li></ol><hr><h1 id="扩容方式">扩容方式</h1><h2 id="新建磁盘分区并挂载到新的挂载点上">新建磁盘分区并挂载到新的挂载点上</h2><p>主要使用的命令：<code>fdisk</code>、<code>mkfs</code>、<code>mount</code>、<code>umount</code>、<code>df</code>、<code>lsblk</code>1. <strong>查看磁盘</strong>： </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">lsblk</code></pre> <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250107172719.png">可以看出里面有三个硬盘：sda/nvme0n1/nvme1n1，目前我的Linux文件系统全部都在sda中的部分分区中，另外nvme0n1只设置了一个/boot/efi的挂载点。现在我想从nvme1n1中分配一部分空间给sda/ada6的/home分区。2. <strong>磁盘分区并挂载</strong>： 首先可以查看一下这个磁盘的使用情况<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo fdisk -l /dev/nvme1n1</code></pre> <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20250107173236.png">这个盘之前在windows系统上使用，目前的分区也已经占满了全部空间，所以需要删除一些分区或者直接使用之前的分区，如果要删除的话我们可以重新对这个盘进行分区，借助fdisk或者parted应该都可以：- 删除分区： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo fdisk /dev/nvme1n1</code></pre> - 输入 d， 逐一删除所有分区。 - 输入 w保存更改。 - 新建分区 <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo fdisk /dev/nvme1n1</code></pre> - 输入 n 创建新分区 -按提示选择分区类型（通常是主分区）。 -提供起始扇区和结束扇区（默认值即可使用未分配的全部空间）。 -确认并完成创建。 - 格式化分区 <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo mkfs.ext4 /dev/nvme1n1p1</code></pre> - 挂载分区 <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo mount /dev/nvme1n1 /mnt/new_home</code></pre>注意前面我们说过/mnt是临时挂载点，我们可以新建一个/home/data挂载点来实现对/home目录的扩容，但是其实这样在系统重启后依旧是临时的，每次都重启需要手动挂载才能正常使用分区内容。- 持久化挂载 - 获取分区 UUID： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo blkid /dev/nvme1n1p1</code></pre> - 编辑 /etc/fstab 文件：添加以下行，将新分区挂载到 /home/data： <pre class="line-numbers language-none"><code class="language-none">UUID=xxxx-xxxx-xxxx-xxxx /home/data ext4 defaults 0 2</code></pre> - 测试 /etc/fstab配置： <pre class="line-numbers language-none"><code class="language-none">sudo mount -a</code></pre> - 验证是否挂载：<code>df -h</code> 3.新挂载点权限设置 上面的案例中/home 下的子目录（/home/data）会独立使用新分区的存储空间，不会直接扩展 /home主目录的可用空间。 默认情况下，挂载后的 /home/data 可能由 root用户拥有。根据需要修改目录权限： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo chown -R &lt;username&gt;:&lt;group&gt; /home/data 或者chmod -R 权限 /home/data   （权限由三位数字组成，分别对应所有者/组/其他用户的权限，单个数字权限为r=4，w=2，x=1的和）</code></pre><p></p><ol start="4" type="1"><li>数据迁移 如果 /home 中已有需要迁移到 /home/data 的数据，可以使用rsync 进行迁移： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo rsync -av /home/old_data /home/data</code></pre></li></ol><h2 id="lvm扩容">LVM扩容</h2><p>更新中。。。。</p><h2 id="直接扩容现有分区">直接扩容现有分区</h2><h3 id="准备">准备</h3><ul><li>备份数据</li></ul><p>调整分区可能会导致数据丢失，建议备份 /home 分区和其他重要数据。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo rsync -aXS /home/ /backup/home/</code></pre> - 查看分区布局 <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">lsblksudo fdisk -l</code></pre> 看看 /home 分区（如/dev/sda6）的后方存在未分配空间。 - 确保分区未挂载： - 如果 /home是活动分区，需要在单用户模式或通过 Live CD/USB 执行操作。 - 在 Live环境中挂载根分区，并使用 chroot 进入系统环境。<p></p><h3 id="如果home-分区如-devsda6的后方位置存在未分配空间">如果/home分区（如 /dev/sda6）的后方位置存在未分配空间</h3><p>因为我是想在在另一个磁盘上扩充，所以后面的方法不适用于我的这个需求，如果满足挂载/home的后方还有连续空间的话优先推荐这种方式，可以直接扩容/dev/sda6 分区大小即可，有以下两种方式： #### 使用gparted（图形化界面）gparted 是调整分区的图形化工具，操作简单且更安全。 -安装：<code>sudo apt install gparted</code> -启动：<code>sudo gparted</code> - 选择 /dev/sda6 分区，右键选择Resize/Move。</p><h4 id="使用fdisk">使用fdisk</h4><ul><li>启动 fdisk： <code>sudo fdisk /dev/sda</code></li><li>删除并重新创建分区：<ul><li>输入 d，选择 /dev/sda6 分区。</li><li>输入n，创建新分区，起始扇区与旧分区相同，结束扇区扩大到未分配空间末尾。</li><li>输入 w 保存更改。</li></ul></li><li>刷新分区表： <code>sudo partprobe</code> #### 扩展文件系统在采用上面的某一种方式后调整分区后，文件系统大小仍然是原来的大小，需要扩展文件系统以使用新的空间。</li><li>扩展 ext4 文件系统</li></ul><p>如果 /home 分区是 ext4，使用 resize2fs 扩展： </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo resize2fs /dev/sda6```   - 扩展 xfs 文件系统如果 /home 分区是 xfs，确保分区已挂载，然后使用 xfs_growfs 扩展：```shellsudo mount /dev/sda6 /homesudo xfs_growfs /home</code></pre><p></p><h4 id="验证">验证</h4><ul><li>检查分区大小是否扩展成功：<code>df -h /home</code></li><li>确认分区的文件系统健康状态：<code>sudo fsck /dev/sda6</code></li></ul><h3 id="如果home-分区如-devsda6的后方位置不存在未分配空间">如果/home分区（如 /dev/sda6）的后方位置不存在未分配空间</h3><p>如果原有的留给/home挂载点的分区已经空间不足且原来空间很小，可以考虑换一个更大的硬盘然后对/home进行迁移。#### 切换用户因为我们现在要扩充/home分区，所以需要切换到root用户下进行操作，使用命令<code>sudo -i</code>切换到root用户。</p><p>另外，Live环境或者单用户模式也都可以。</p><h4 id="准备新分区">准备新分区</h4><ul><li>如果新硬盘还没有新分区的话可以新建一个分区：<code>sudo fdisk /dev/nvme1n1</code>,输入n新建分区，输入w保存更改。</li><li>格式化分区：<code>sudo mkfs.ext4 /dev/nvme1n1p1</code></li><li>挂载分区：<code>sudo mount /dev/nvme1n1p1 /mnt/new_home</code>,将其挂载到/mnt/new_home目录下。 #### 迁移数据</li><li>迁移数据：<code>sudo rsync -aXS /home/ /mnt/new_home/</code><strong>非常重要，迁移完成后一定要检查一下数据是否完整，确认无误后再进行下一步操作。</strong>-a：归档模式，保留权限、时间戳等。-X：保留扩展属性。-S：处理稀疏文件。</li></ul><h4 id="更新挂载点">更新挂载点</h4><ul><li>卸载原有 /home 分区：<code>sudo umount /home</code><ul><li>此时可能遇到/home忙碌的问题，可以使用<code>lsof /home</code>查看一下/home目录下的进程，但是如果有系统进程的话就无法卸载了，所以还是Live环境或者单用户模式比较方便。</li></ul></li><li>如果之前临时挂载点设置了永久挂载，需要卸载临时的new_home：<code>sudo umount /mnt/new_home</code>，这个如果不卸载的话会可能导致/home和new_home都在同一个分区下，非常不安全。</li><li>挂载新分区到/home：<code>sudo mount /dev/nvme1n1p1 /home</code></li><li>获取新分区 UUID：<code>sudo blkid /dev/nvme1n1p1</code></li><li>持久化挂载：编辑 /etc/fstab，添加新分区的 UUID 信息。 <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">UUID=xxxx-xxxx-xxxx-xxxx /home ext4 defaults 0 2</code></pre>#### 验证</li><li>/home 分区是否正常挂载：<code>df -h /home</code> <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sudo mount -adf -h /home</code></pre></li><li>旧的临时目录下内容和新的/home目录下内容是否是在一个分区下：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">df /mnt/new_home</code></pre> 检查清楚后可以删除/mnt/new_home目录下的内容。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 磁盘 </tag>
            
            <tag> 扩容 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python-模块/包的导入</title>
      <link href="/2024/12/27/python-mo-kuai-bao-de-dao-ru/"/>
      <url>/2024/12/27/python-mo-kuai-bao-de-dao-ru/</url>
      
        <content type="html"><![CDATA[<h2 id="模块module和包package">模块Module和包Package</h2><p>在Python中，一个 <strong>.py文件</strong>就称之为一个模块（Module）。Module的目的是代码的划分、管理以及代码重用，在一个Module中可以存在多个类、函数甚至是需要预执行的脚本。模块一共三种：python标准库、第三方模块、应用程序自定义模块。相同名字的函数和变量完全可以分别存在不同的模块中，不必考虑名字会与其他模块冲突。但是<strong>不要与内置函数名字冲突</strong>。</p><p>相关概念： -<code>_pycache__</code>：Python3.2版本引入的，用于存放模块的编译后的字节码文件，以提高模块的加载速度。-<code>__init__.py</code>：模块的初始化文件，可以为空，也可以有代码。**当一个文件夹下有__init__.py文件时，这个文件夹就会被当作一个包来处理。**- <code>dir()</code>：查看模块的所有属性和方法。比如： </p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import mathprint(dir(math))</code></pre> -<code>__name__</code>：模块的内置属性，用于判断模块是被导入还是直接当作脚本执行。比如：<pre class="line-numbers language-python" data-language="python"><code class="language-python">if __name__ == '__main__':    print('This is a module')</code></pre><p></p><h3 id="模块的查找路径">模块的查找路径</h3><p>在 Python 中，当运行一个项目时，模块的查找路径遵循特定的顺序。这是由<code>sys.path</code> 列表定义的，该列表按顺序列出了 Python在查找模块时会搜索的路径。</p><p>模块查找路径的顺序 -当前目录：运行脚本所在的目录。如果你执行了一个脚本，那么 Python会首先尝试从该脚本所在的目录加载模块。 - 环境变量 PYTHONPATH指定的目录：如果设置了 PYTHONPATH环境变量，其值中列出的目录将被加入查找路径。 - 标准库目录：Python自带的标准库路径（例如 lib 或 Lib 目录）。 - 第三方包路径：通常是安装在site-packages 目录下的路径。 - 内置模块：如果以上路径都找不到，Python会尝试加载内置模块（如 sys、os）。</p><p>示例及详细解释: 比如一个项目的目录结构如下： </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">project/│├── script.py          # 主运行脚本├── module1.py         # 自定义模块└── subdir/    ├── module2.py     # 自定义模块    └── __init__.py    # 将 subdir 标记为包</code></pre><p></p><p>其中 <code>script.py</code> 代码如下： </p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import sysprint("Module search paths:")for path in sys.path:    print(path)import module1         # 自定义模块，位于当前目录from subdir import module2  # 从子目录中导入模块</code></pre><p></p><p>模块查找过程如下： 1. 当前目录： - 运行 python script.py 时，Python首先查看当前目录（project/）。 - 找到 module1.py，并成功导入。 2.子目录导入： - 对于 from subdir import module2，Python 进入project/subdir/ 并找到 module2.py。 - 因为 subdir 包含<strong>init</strong>.py，它被认为是一个包。 3. 标准库： -如果你尝试导入如 os、sys 等模块，Python 会从标准库路径中加载。 4.第三方包： - 如果尝试导入第三方库（如 numpy），Python 会在 site-packages目录中查找。</p><h3 id="修改模块查找路径">修改模块查找路径</h3><p>可以动态修改 sys.path 来影响模块的查找路径。 </p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import sysimport os# 查看当前路径print("Before modification:", sys.path)# 添加新路径new_path = os.path.join(os.getcwd(), 'subdir')sys.path.insert(0, new_path)print("After modification:", sys.path)# 导入模块import module2</code></pre><p></p><h3 id="与包package的关系">与包package的关系</h3><p>包所在的目录需要包含一个空文件__init__.py来表明这个是一个包，所以说<strong>包是一个包含了多个模块的目录</strong>。包的目的是为了组织模块，以便更好地管理和维护代码。</p><p>在<code>__init__.py</code>文件中可以声明一些描述性的代码来变更package的特性。比如针对import*的__all__，如果某个package下有你明确不希望被引用的py文件，可以通过__all__明确说明哪些是希望引入的，这样在python处理import*时会忽略掉不在__all__列表的内容。比如： </p><pre class="line-numbers language-python" data-language="python"><code class="language-python">__all__ = ['module1', 'module2'] # 只允许导入module1和module2</code></pre> ### 模块的导入#### 同级目录下的模块导入如果你要使用的模块（py文件）和当前模块在同一目录，只要import相应的文件名就好，比如在当前目录下有一个module1.py文件，那么在另一个文件中可以直接使用importmodule1来导入。 -<strong>一个模块可以在当前位置import多次，但是只有第一次导入会执行内容，其他的都为引用内存</strong>- <strong>更改调用名称：<code>import module1 as m1</code></strong> -<strong>只导入模块中的部分内容：<code>from module1 import func1</code></strong><p></p><h4 id="不同目录下的模块导入">不同目录下的模块导入</h4><h5 id="子目录下的模块导入">子目录下的模块导入</h5><p>假设目录结构如下： </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">project/├── main.py├── subdir/│   ├── module2.py│   └── __init__.py</code></pre> 方法1：直接导入（如果子目录是一个包） 如果子目录包含<code>__init__.py</code> 文件（即被视为包），可以使用以下方式导入：<pre class="line-numbers language-python" data-language="python"><code class="language-python"># main.pyfrom subdir import module2  # 导入子目录中的模块module2.some_function()    # 调用 module2 中的函数</code></pre><p></p><p>方法 2：修改 sys.path 如果子目录中没有<code>__init__.py</code>，或者希望在没有包的情况下导入，可以通过添加子目录到sys.path： </p><pre class="line-numbers language-python" data-language="python"><code class="language-python"># main.pyimport sysimport os# 将子目录路径添加到 sys.pathsys.path.append(os.path.join(os.getcwd(), 'subdir'))import module2  # 现在可以直接导入module2.some_function()</code></pre><p></p><h5 id="父目录下的模块导入">父目录下的模块导入</h5><p>假设目录结构如下： </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">project/├── parent_module.py└── subdir/    ├── main.py    ├── module3.py    └── __init__.py</code></pre> 方法 1：使用 sys.path<pre class="line-numbers language-python" data-language="python"><code class="language-python"># main.pyimport sysimport os# 将父目录路径添加到 sys.pathsys.path.append(os.path.dirname(os.getcwd()))import parent_module  # 现在可以导入父目录的模块parent_module.some_function()</code></pre><p></p><p>方法 2：相对导入（仅适用于包结构）</p><p>如果 subdir 是一个包（包含<strong>init</strong>.py），可以使用相对导入： </p><pre class="line-numbers language-python" data-language="python"><code class="language-python"># main.pyfrom .. import parent_module  # 相对导入父目录的模块parent_module.some_function()</code></pre><strong>注意：相对导入仅在包结构中有效，且不能直接运行该脚本（需要通过python -m 的方式运行）。</strong><p></p><h2 id="其他">其他</h2><ol type="1"><li><strong>当使用 <code>from module import func_b</code>时，整个模块会被加载和执行一次(包括下划线开头的私有对象)，但只有你导入的部分会被绑定到当前的命名空间</strong>例如： <pre class="line-numbers language-python" data-language="python"><code class="language-python"># module.pyvalue = 1def func_a():    print("Function func_a is being executed")    return 0def func_b():    print("Function func_b is being executed")    b = func_a()    return b</code></pre></li></ol><pre class="line-numbers language-python" data-language="python"><code class="language-python"># main.pyfrom module import func_bprint("Calling func_b() in main.py")result = func_b()print(f"Result: {result}")</code></pre><p>执行过程： 1.<br>from module import func_b</p><p>Python 会先找到 module.py 文件并加载整个模块： - Python 会解释并执行module.py 文件的顶层代码。 - value = 1 会被执行并存储到 module的命名空间中。 - 函数 func_a 和 func_b 的定义会被加载，并绑定到 module的命名空间。 - 然后，func_b会被绑定到当前模块（main.py）的命名空间。</p><p><strong>注意：</strong> - 虽然整个模块被加载，但只有 func_b 被绑定到main.py 的命名空间中。 - 在加载过程中，module的顶层代码（如赋值语句、类定义等）都会被执行一次。</p><ol start="2" type="1"><li>调用 func_b()</li></ol><ul><li>Python 跳转到 module.py 中 func_b的定义，并开始执行该函数的内容。</li><li>在 func_b 中，func_a 被调用，导致 func_a 的代码被执行。 完整输出：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">Calling func_b() in main.pyFunction func_b is being executedFunction func_a is being executedResult: 0</code></pre></li></ul><ol start="2" type="1"><li><strong><strong>init</strong>.py文件的的规范</strong>加入存在以下目录结构： <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">project/├── my_package/│   ├── module1.py│   ├── module2.py│   └── __init__.py└── main.py</code></pre> <code>__init__.py</code>文件的一些写法的解释：</li></ol><ul><li>空：仅标记为包，导入需要显式地指明模块和内容。所以使用时也是<code>import my_package.module1</code>，本质上是模块，包中的定义的__all__列表不会生效。</li><li><code>from . import module2</code>:导入完整模块到包命名空间，使用时需通过模块名访问，本质上是模块，包中的定义的__all__列表不会生效。</li><li><code>from my_package import *</code>:导入完整模块到包命名空间，使用时无需模块名访问，</li><li><code>from .module2 import *</code>:将整个模块内容导入到包命名空间，使用时无需模块名。</li><li><code>from my_package.module2 import *</code>:将整个模块内容导入到包命名空间，使用时无需模块名。</li><li><code>from module2 import *</code>:这是种错误写法，无法找到模块。</li></ul><ol start="3" type="1"><li><code>__all__</code>在<code>__init__.py</code>中的作用：</li></ol><ul><li>当你使用 from my_package import * 时，<code>__all__</code>定义了哪些名称会被导入到当前命名空间。</li><li>如果__all__没有定义，那么默认会导入所有非以下划线 _开头的对象。</li></ul><p>调用时的区别： - 如果在 <code>main.py</code> 中需要写模块名（例如my_package.module2.func2()），那么<code>__all__</code>不会限制模块中的内容。 - <code>__all__</code> 只在from my_package import * 时生效，而不会影响显式导入 例如： 目录结构：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">my_package/├── __init__.py├── module1.py├── module2.py # 包含 func2 函数</code></pre> <pre class="line-numbers language-python" data-language="python"><code class="language-python">#__init__.py：from .module2 import *__all__ = []  # 不对外暴露任何内容</code></pre> <pre class="line-numbers language-python" data-language="python"><code class="language-python"># main.pyfrom my_package import *func2()  # 报错，因为 __all__=[] 限制了导出的内容from my_package.module2 import func2 func2()  # 正常运行from my_package import func2func2()  # 正常运行， 比较奇怪哈，但是测试了这样子是可以的</code></pre><p></p><ol start="4" type="1"><li><strong>通过<code>-m</code>运行一个模块</strong>对于一些项目，我们可能需要通过命令行来运行一个模块，这时候可以使用<code>-m</code>参数，以支持模块的相对导入。<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">project/├── my_package/│   ├── module1.py│   ├── module2.py│   └── __init__.py└── subdir/    ├── module3.py    └── __init__.py</code></pre> 对于moulde3.py中的内容，如果写成： <pre class="line-numbers language-python" data-language="python"><code class="language-python">from my_package import func1 # module1.py中的函数 func1()</code></pre>需要通过<code>-m</code>参数来运行：<code>python -m subdir.module3</code>，这样才能正确导入my_package中的内容。</li></ol><p>参考： &gt;https://blog.csdn.net/Uncle_GUO/article/details/80867086</p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> ImportError </tag>
            
            <tag> python模块 </tag>
            
            <tag> python包 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>好用的大模型prompt分享</title>
      <link href="/2024/12/21/hao-yong-de-da-mo-xing-prompt-fen-xiang/"/>
      <url>/2024/12/21/hao-yong-de-da-mo-xing-prompt-fen-xiang/</url>
      
        <content type="html"><![CDATA[<h1 id="中文写作">中文写作</h1><h2 id="润色">润色</h2><pre class="line-numbers language-none"><code class="language-none">作为一名中文学术论文写作改进助理，请用学术化的语言重写以下句子，请分条阐述，你的任务是改进所提供文本的拼写、语法、清晰、简洁和整体可读性，同时分解长句，减少重复，并提供改进建议。请只提供文本的更正版本，避免包括解释。且要求重新组织段落中的句子，使其具有逻辑性，并分条阐述。请编辑以下文本：</code></pre><h2 id="校对与完善">校对与完善</h2><pre class="line-numbers language-none"><code class="language-none">假设你是一位专注于学术写作的校审专家。你的主要工作是检查和修改一篇题为“[PAPER TITLE]”的论文段落，目标是让论文的表达更加清晰、条理清楚。找出所有不明确、复杂或冗余的句子或短语，并提供更加精确和简洁的选择方案。特别注意文章的逻辑流动，确保每个句子都能有效支持主旨或叙述。全文要保持术语、风格和语调的统一。对于可能需要对预期读者进行说明的行业术语或专业词汇，要给出清晰的解释。完成校审后，确保该段落符合学术规范，使得论文整体上更加易读，从而提高其影响力。接下来我会逐个给你展示这篇论文的段落，你需要根据上述要求对每个段落进行校审，注意你要保留原有的引用。请在每个段落的下方写下你的修改建议。</code></pre><h2 id="公式分析生成">公式分析生成</h2><pre class="line-numbers language-none"><code class="language-none">假设你是一名学术研究者，请使用学术化的用语，尽量使用数学公式表达，请帮我分析以下公式具有什么性质，有什么深入推导。请一步一步阐述：</code></pre><h2 id="改写查重后用">改写（查重后用）</h2><ul><li>示例一： <pre class="line-numbers language-none"><code class="language-none">1.假设你是一个论文润色写作员，具有学术研究相关知识。请给改写以下文段，要求给出的文段与原文段重复率低于10%，要求新文段的任意连续40个字与原文段中任意连续40个字中重复的字小于5个。要求尽可能替换文段中词汇使其更符合学术论文表达，要求尽可能更换说法避免与原文段的重复但是要保留原文段的含义，允许重新组织句子顺序、词语顺序、段落顺序，允许改写时对句子扩写或缩减。请给出改完后的文段、给出与原文段的对比，请一步一步阐述。文段如下：</code></pre></li><li>示例二： <pre class="line-numbers language-none"><code class="language-none">假设你是一个学术研究者，正在撰写毕业论文，具有学术研究相关知识。请简要总结以下文段的主要内容观点，并按条目输出，再根据每条总结扩写句子，最后组合成新文段，要求给出的文段与原文段重复率低于5%，要求输出的新文段中任意抽取连续40个字与原文段中任意连续40个字比较，重复的字小于5个。允许重新组织句子顺序、词语顺序、段落顺序，允许改写时对句子扩写或缩减。请给出改完后的文段、给出与原文段的对比，请一步一步阐述。文段如下：</code></pre> ## 综述协助</li><li>全职版 <pre class="line-numbers language-none"><code class="language-none">假设你是正在对某一学术领域的[RESEARCH TOPIC]进行深入研究的研究者。你的任务是从广泛的文献中识别、分析和综合关键的发现、理论和研究方法。首先，确定一个简单的研究问题或论点，以此为基础指导你的文献回顾。寻找相关的学术期刊、会议论文、书籍以及可靠的网络资源。对每项资源的可信度、相关性和对领域贡献进行评估。摘要每个资源的主要论点、证据和结论，注意观察文献中的一致性、差异或空缺。批判性地审视所研究的方法、其局限性及其发现的意义。根据主题或时间顺序安排文献综述，确保逻辑连贯，围绕研究主题构建出一篇有条理的综述文章。最后，强调你的研究是如何填补已识别的空白或对该学科领域提供新见解的。确保你的文献综述遵循所在机构要求的引用风格和学术写作标准。</code></pre></li><li>润色版 <pre class="line-numbers language-none"><code class="language-none">假设你是正在对某一学术领域的[RESEARCH TOPIC]进行深入研究的研究者。你的任务是将我给你的相关段落进行完善与展开。你需要批判性地审视所研究的方法、其局限性及其发现的意义，确保逻辑连贯，保持术语、风格和语调的统一。对于可能需要对预期读者进行说明的行业术语或专业词汇，要给出清晰的解释。</code></pre></li></ul><h1 id="英文写作">英文写作</h1><h2 id="中翻英">中翻英</h2><pre class="line-numbers language-none"><code class="language-none">假设你是一名美式英语为母语的翻译员，同时你还是一名理学的学术研究者，你的任务是将以下中文学术论文段落翻译为英文。要求严格保持语法正确，要求英文词语拼写正确，要求单复数使用正确，要求采用学术的表达，不使用口语化表达，参考其他类似文献中的句子和段落结构，参考其他类似文献中的专有名词和习惯用词，可以根据句子内容扩展添加词汇使句子结构完整，可以根据句子内容调整句子的先后位置。最后提供最后翻译完的英文文段以及调整后的中文文段，同时展示调整后的中文文段与原文段对比在哪些位置做了修改。请一步一步阐述。请翻译以下文段：</code></pre><blockquote><p>https://domyweb.org/chatgpt/#act-as-an-essay-writer</p></blockquote><blockquote><p>https://zhuanlan.zhihu.com/p/654520254</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Prompt </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLMs </tag>
            
            <tag> Prompt </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>在宇宙最强编辑器上使用latex</title>
      <link href="/2024/12/18/zai-yu-zhou-zui-qiang-bian-ji-qi-shang-shi-yong-latex/"/>
      <url>/2024/12/18/zai-yu-zhou-zui-qiang-bian-ji-qi-shang-shi-yong-latex/</url>
      
        <content type="html"><![CDATA[<h2 id="安装准备">安装准备</h2><ul><li>TeXLive / MacTeX / MiKTeX等LaTeX发行版</li><li>VSCode</li></ul><h2 id="vscode-配置">Vscode 配置</h2><h3 id="安装插件">安装插件：</h3><ul><li><p>LaTeX Workshop （必装）</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241218201018.png">LaTeX Workshop 支持 LaTeX 的编译、预览、语法检查等功能。</p></li><li><p>English Word Hint</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241218201116.png">English Word Hint是一个英语单词提示插件，可以在编写英语文档时，自动提示相关英语单词，并显示对应的中文翻译，提高英文文档编写效率。</p></li><li><p>Path Auto Complete</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241218201156.png">Path Auto Complete 可以自动补全路径，方便快速插入图片。</p></li><li><p>indent rainbow</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241218201245.png">indent rainbow 可以为不同层级的缩进添加不同的颜色，方便阅读。</p></li><li><p>Word Count CJK</p><p>Word Count CJK 可以统计中文文档的字数。</p></li><li><p>Material Icon Theme</p><p>Material Icon Theme可以为不同类型的文件添加不同的图标，方便区分。</p></li></ul><h3 id="latex-workshop-配置">LaTeX Workshop 配置</h3><h4 id="基本配置">基本配置</h4><pre class="line-numbers language-json" data-language="json"><code class="language-json">// 鼠标悬停，预览公式时，支持 boldsymbol 宏"latex-workshop.hover.preview.mathjax.extensions": [    "boldsymbol"],// 是否启用 IntelliSense，自动补全引用的包中的环境和命令"latex-workshop.intellisense.package.enabled": true,// 编译后的文件输出目录"latex-workshop.latex.outDir": "./tmp",// 默认编译引擎为上次使用的"latex-workshop.latex.recipe.default": "lastUsed",// 预览复杂公式，使用时需要通过 command palette (命令面板) 打开"latex-workshop.mathpreviewpanel.cursor.enabled": true,// 不允许弹窗显示错误信息"latex-workshop.message.error.show": false,// 不允许弹窗显示警告信息"latex-workshop.message.warning.show": false,// 预览 PDF 时，反转颜色"latex-workshop.view.pdf.invert": 1,// 预览 PDF 时，自动检测是否需要反转颜色"latex-workshop.view.pdf.invertMode.enabled": "auto",</code></pre><h4 id="编译工具链配置">编译工具链配置</h4><p>推荐使用 latexmk 进行编译，latexmk可以自动检测文档中的变化，自动进行编译，并且同时支持多种编译引擎，包括XeLaTeX、PdfLaTeX。</p><p>在 settings.json 文件中找到 latex-workshop.latex.tools 和latex-workshop.latex.recipes 配置项，将其全部删除，并修改为如下配置</p><pre class="line-numbers language-json" data-language="json"><code class="language-json">"latex-workshop.latex.recipes": [    {        "name": "XeLaTeX",        "tools": [            "xelatexmk"        ]    },    {        "name": "PdfLaTeX",        "tools": [            "pdflatexmk"        ]    }],"latex-workshop.latex.tools": [    {        "args": [            "-synctex=1",            "-pdfxe",            "-interaction=nonstopmode",            "-file-line-error",            "-outdir=%OUTDIR%",            "%DOC%"        ],        "command": "latexmk",        "env": {},        "name": "xelatexmk"    },    {        "args": [            "-synctex=1",            "-pdf",            "-interaction=nonstopmode",            "-file-line-error",            "-outdir=%OUTDIR%",            "%DOC%"        ],        "command": "latexmk",        "env": {},        "name": "pdflatexmk"    }],</code></pre><h4 id="预览与同步tex">预览与同步TeX</h4><p>https://github.com/James-Yu/LaTeX-Workshop/wiki/View#internal-pdf-viewer，官网上有详细配置说明，这里简单介绍通过内置的pdf预览器的前向与反向搜索的快捷键：- Forward/Direct search:在编辑器中点击某个位置，window通过<code>ctrl+alt+j</code>跳转，mac通过<code>cmd+option+j</code> 跳转。</p><ul><li>Backward/Inverse search: 使用内部查看器时，指向 pdf预览中的元素的默认键绑定是 <code>ctrl+click</code>。可以使用设置<code>latex-workshop.view.pdf.internal.synctex.keybinding</code>将其更改为<code>double-click</code> 。 <pre class="line-numbers language-json" data-language="json"><code class="language-json">"latex-workshop.view.pdf.internal.synctex.keybinding": "double-click"</code></pre></li></ul><p>参考：</p><blockquote><p>在 VSCode 中配置 LaTeX 环境https://github.com/shinyypig/latex-vscode-config</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Latex </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Latex </tag>
            
            <tag> VSCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>日本-消失的三十年-启示与借鉴</title>
      <link href="/2024/12/10/ri-ben-xiao-shi-de-san-shi-nian-qi-shi-yu-jie-jian/"/>
      <url>/2024/12/10/ri-ben-xiao-shi-de-san-shi-nian-qi-shi-yu-jie-jian/</url>
      
        <content type="html"><![CDATA[<h2 id="二战后日本经济奇迹">二战后日本经济奇迹</h2><p>二战后，世界迎来了冷战时期，日本作为美国抵御苏联影响的桥头堡，因此受到了美国极大的经济援助。</p><p>美国认为，经济发展不仅可以防止日本再次出现军国主义，并且可以防止共产主义。</p><p>1950年，由于朝鲜战争的爆发，日本成为美军军需生产和维修的基地，美国政府支付巨额的特殊采购，这些特殊采购占当年日本出口贸易的27%。1950年代后期，日本的经济完全恢复。</p><p>日本政府的一系列经济政策，加上没有养军队的束缚（《日本国宪法》第九条）也为日本经济的快速发展奠定了基础。</p><p>1956年，日本政府制定“电力五年计划”，建立电力工业和用石油取代煤炭，由此带来的良好影响带动了耐用品消费，出现战后第一次经济发展高潮，时称<strong>神武景气</strong>。</p><p>1958年后，日本政府开始引导企业生产如汽车、电视等家用电器和钢铁，出现了第二次经济发展高潮，称为<strong>岩户景气</strong>。</p><p>1960年，1960年代日本首相池田勇人提出了国民收入倍增计划，10年的时间让日本的国民生产总值增加到26兆日圆，但实际的经济成长远超过预期：仅六年的时间，国民平均收入就达到了倍增的计划。</p><p>1964年，日本成功举办东京奥运会，日本为了东京奥运的直接场馆投资为295亿日元，间接投资（公路、地下铁等交通建设，上下水道铺建等）则达9,600亿日元。房地产市场迅速发展，再一次拉动了经济，时称<strong>奥运会景气</strong>。</p><p>但奥运会结束后日本经济出现了放缓，于是日本政府决定发行战后的第一次建设国债刺激经济。在这期间，有不少大企业合并，同时私家车和彩色电视快速普及，国民的收入水平快速提高，时称<strong>伊奘诺景气</strong>。</p><h2 id="美国贸易逆差与广场协议">美国贸易逆差与广场协议</h2><p>在美国扶持下，日本制造业迎来飞速发展，很多产品出口到美国，抢占其本土产品市场。</p><p>1、1957-1972年：日本 纺织品大量进入美国，美国密集出台限制日本纺织品的法案；</p><p>2、1968-1978年：日本 钢铁行业接棒纺织业，成为对美出口的主力，随后遭到美国钢铁行业工会的阻击；</p><p>3、1970-1980年：日本 家电行业崛起，特别是彩电，巅峰期美国市场三成的彩电都是日本产品；</p><p>4、1979-1987年：日本 汽车行业崛起，成为日本赚取美元的主力，导致全美范围内的抗议潮。</p><p>为阻止贸易逆差扩大，美国对日出台一系列限制政策，可惜效果一直不大，直到1985年的广场协议。</p><p>美国、日本、英国、法国及德国5个工业发达国家代表人在纽约的广场饭店会晤后，于1985年9月签署该协议，协议要求干预外汇市场，使美元对日元、德国马克等主要货币贬值。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241210195705.png"></p><p>签完协议头一年，日元仅相对美元升值20%，还在可控范围，日本制造企业省省钱，裁裁员，还撑得下去，到了1987年直接失控，日元相对美元升值到100%，从1美元兑240日元左右上升到一年后的1美元兑120日元，由于汇率的剧烈变动，由美国国债组成的资产发生账面亏损，因此大量资金为了躲避汇率风险而进入日本国内市场。但是日本出口企业受到了重创。</p><h2 id="泡沫经济与破灭">泡沫经济与破灭</h2><h3 id="虚假的繁荣">虚假的繁荣</h3><p>为了缓解日元升值的速度，日本政府采取的办法是<strong>降息</strong>，日本政府认为只要存款利息减少，大家的钱存银行不划算，市场上流动的日元会大大增多，刺激国民消费，扩大内需，实现一种出口转内销的效果，同时缓解日元升值的速度。</p><p>于是1986年宣布将银行利率从5%降到4.5%，到1987年直接降到2.5%。 <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241210200859.png"></p><p>汇率是稳下来，从汇率表可以看到，日元升值到1987年就停下来。 <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241210201032.png"></p><p>但没想到的是，降低利率对国内基本消费需求没有刺激作用，但对投机、特别是投资房地产行为却能起很大的促进的作用，市场上多出来的日元没有进入制造业，而是<strong>投到股市和房市上</strong>，当时采取这样的政策，本身就是以虚假的经济泡沫去粉饰经济增长的数据，泡沫撑起日本GDP自1987年后的高速增长。日经平均股价从85年的13000，一路飙涨到90年的38000点左右，接近3倍。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241210201556.png"></p><p>房价从83年的37万日元一平米，涨到91年的278万日元一平米，如果按目前1:0.05 的人民币日元汇率算，相当于 14万人民币一平米。 <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241210201624.png"></p><p>当时东京23个区的地价总和甚至达到了可以购买美国全部国土的水平，而银行则以不断升值的土地作为担保，向债务人大量放款。此外地价上升也使得土地所有者的账面财产增加，刺激了消费欲望，从而导致了国内消费需求增长。由于日元急速升值导致国外资产变得便宜，当时很多人卖奢侈品和买菜一样平常。当时日本媒体为了给这种经济繁荣状况命名，还希望募集像岩户景气、神武景气类似的名称。</p><h3 id="泡沫经济的破灭">泡沫经济的破灭</h3><p>随着越来越多的资本流向股市和房市，而不是实体产业，加上由于美国发动了伊拉克战争，石油价格飙升，日本国内制造业成本激增，于是大量制造业倒闭。</p><p>1989年，日本央行开始加息，希望通过加息来抑制股市和房市的投机热情，但是这一举措却引起了外资的抛售，股市在1989年12月达到38957.44点后开始下跌，泡沫经济开始正式破裂。</p><p>1991年一月，日本政府发现事情不对，又开始大幅降息，想以此稳住股市，但依旧无法挽救股市，外资持续出逃。</p><p>比股市更惨的是楼市，1991年9月，土地开始暴跌，1992年日本出台“地价税”政策，要求所有持有土地的人交纳地价税，导致大量人开始抛售房产，陷入了供大于求的局面。随着房价暴跌，越来越多的人发现自己持有的房子的价值比自己的贷款还低，于是大量人违约，日本银行出现大量不良资产，银行开始倒闭，许多日本居民的存款一夜归零。</p><p>到这里，大家都没钱了，整体经济断崖式下跌，买了房的人，失业后还要还房贷，没买房的人，也失去了买房拼搏的动力。年轻人逐渐开始走向佛系，选择低欲望生活，他们不上班，在家啃老，沉迷二次元和网络游戏，被称为“平成废物”。</p><h2 id="启示与借鉴">启示与借鉴</h2><h3 id="当前中国经济发展与日本战后经济发展的比较">当前中国经济发展与日本战后经济发展的比较</h3><ol type="1"><li>快速工业化</li></ol><ul><li>日本战后通过政府规划和政策支持实现了高速工业化，中国自改革开放以来也经历了类似的工业化进程，逐步从农业经济转向制造业和服务业为主。</li><li>两国都通过基础设施建设和出口导向政策，拉动经济增长。</li></ul><ol start="2" type="1"><li>高储蓄率与高投资率</li></ol><ul><li>日本战后储蓄率高，资金主要流向工业和基础设施投资；中国近年来也保持着高储蓄率和投资率，大量资金用于基建、房地产和科技产业。</li><li>在高投资驱动下，经济增长迅速，但也存在投资过剩的风险。</li></ul><ol start="3" type="1"><li>政府主导经济</li></ol><ul><li>日本战后经济受到政府强烈干预，实施产业政策支持关键领域，中国同样通过五年规划和政策导向调控经济发展。</li><li>政府对市场的干预在某种程度上加剧了市场的失衡，带来潜在风险。</li></ul><ol start="4" type="1"><li>快速城市化和房地产市场繁荣</li></ol><ul><li>日本战后城市化进程迅速，房地产市场经历了高速增长；中国也在近年来快速城市化，房地产行业成为经济增长的重要引擎。</li><li>房地产泡沫在日本曾造成严重的经济问题，中国当前也面临类似的泡沫风险。</li></ul><ol start="5" type="1"><li>债务问题</li></ol><ul><li>日本战后经济高速增长时期，企业债务迅速膨胀，政府债务也不断攀升。中国目前地方政府债务和企业债务同样增长迅速，潜在风险值得关注。</li></ul><h3 id="泡沫经济的潜在危机中国是否会重蹈日本覆辙">泡沫经济的潜在危机：中国是否会重蹈日本覆辙？</h3><ol type="1"><li>房地产市场风险</li></ol><ul><li>日本经验：1980年代日本房地产价格暴涨，但随着政府收紧货币政策，泡沫破裂，导致金融系统崩溃。</li><li>中国现状：房地产行业在中国经济中占据重要地位，但存在过度依赖的问题。目前房地产市场面临高杠杆和需求疲软的双重压力，部分城市已经显现泡沫破裂迹象。</li></ul><ol start="2" type="1"><li>债务积累</li></ol><ul><li>日本经验：泡沫时期企业过度借贷，最终因资产价格下跌和经济衰退导致大规模债务违约。</li><li>中国现状：地方政府隐性债务和企业债务风险持续上升，部分债务已经难以持续偿还，一旦经济下行可能触发债务危机。</li></ul><ol start="3" type="1"><li>金融系统稳定性</li></ol><ul><li>日本经验：泡沫破裂后，银行因不良贷款增加而陷入困境，金融系统崩溃成为长期经济低迷的主要原因。</li><li>中国现状：中国金融系统中存在不良贷款率上升的问题，特别是在房地产和地方政府融资平台领域，可能对银行体系构成威胁。</li></ul><ol start="4" type="1"><li>人口与消费问题</li></ol><ul><li>日本经验：泡沫经济破裂后，人口老龄化和消费疲软导致经济难以复苏。</li><li>中国现状：人口增速放缓、老龄化问题显现，内需不足可能进一步制约经济增长，形成长期性结构问题。</li></ul><p>历史证明，<strong>当经济以超出潜在经济增长率的速度奔跑时，发生资产泡沫乃至成为泡沫经济几乎是不可避免的事情</strong>。</p><h3 id="中国会不会重蹈复辙">中国会不会重蹈复辙？</h3><p>中国经济发展与日本战后有诸多相似之处，但中国的经济规模更大、政策弹性更强，并且具备应对危机的经验。然而，中国当前面临的房地产泡沫、债务问题和人口老龄化等挑战，确实可能引发类似于日本的经济泡沫危机。如果不能妥善处理，这些问题可能成为经济发展的重大障碍。因此，继续推进经济结构改革和风险防控是避免陷入泡沫经济困境的关键。</p><h3 id="逆势上涨的行业机会">逆势上涨的行业机会</h3><p>经济越不好，什么行业反而越好呢？其实日本“消失的三十年”早就写好了答案。当时的年轻人也是想着去考公考编，去银行工作，结果日本的公务员年年缩编，153家银行直接倒闭了。消费降级，年轻人躺平，跟我们当下如出一辙。那当时的日本什么行业反而好呢？主要有三个方向。一、多巴胺经济，就是那种花点小钱能买到大快乐的。三得利当时就是靠着卖茶和咖啡一波崛起的。二、平价经济。日本连续七年的首富就是优衣库。那咱们的蜜雪冰城、拼多多、名创优品是不是也很类似呢？最后就是造梦经济。通俗点说，就是能让人短暂地忘记痛苦，逃避现实的行业。比如20年前日本的索尼和任天堂，以及国内爆火的《黑神话：悟空》，都是因为消费降级了，大家不爱出门，宁愿花点小钱在家玩游戏，主打的就是忘记痛苦，过得快乐。</p>]]></content>
      
      
      <categories>
          
          <category> 杂谈 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 经济发展 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型优化-FlashAttention-v2/v3</title>
      <link href="/2024/12/10/da-mo-xing-you-hua-flashattention-v2-v3/"/>
      <url>/2024/12/10/da-mo-xing-you-hua-flashattention-v2-v3/</url>
      
        <content type="html"><![CDATA[<h2 id="flashattention-v1回顾">FlashAttention-v1回顾</h2><p>我们先快速回顾一下V1的运作流程：以K，V为外循环，Q为内循环。 <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241210021847.png"></p><h2 id="flashattention-v2">FlashAttention-v2</h2><p>FlashAttention V2 出自论文(《FlashAttention-2: Faster Attention withBetter Parallelism and WorkPartitioning》)[https://arxiv.org/pdf/2307.08691]， 主要改进包括：</p><ul><li>优化计算次序，减少非矩阵计算量。</li><li>增加 seq_len 维度的并行计算，提升 SM 利用率。</li><li>优化 warp 级工作模式，减少内部通信和 shared memory 访问。</li></ul><h2 id="flashattention-v3">FlashAttention-v3</h2><p>Flash Attention V3 出自论文(《FlashAttention-3: Fast and AccurateAttention with Asynchrony andLow-precision》)[https://arxiv.org/pdf/2407.08608]，主要改进如下：</p><p>引入生产者-消费者异步机制，提升并行度。 优化 GEMM 和 Softmax操作的重叠计算。 支持 FP8 低精度硬件加速，提升吞吐量并减少精度损失。</p><p>参考： &gt; 图解大模型计算加速系列：Flash AttentionV2，从原理到并行计算: https://zhuanlan.zhihu.com/p/691067658</p>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLMs </tag>
            
            <tag> FlashAttention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo正确渲染md/latex公式</title>
      <link href="/2024/12/07/hexo-zheng-que-xuan-ran-md-latex-gong-shi/"/>
      <url>/2024/12/07/hexo-zheng-que-xuan-ran-md-latex-gong-shi/</url>
      
        <content type="html"><![CDATA[<p>我们平时使用markdown写文档的时候，免不了会碰到数学公式，好在有强大的Mathjax，可以解析网页上的数学公式，大部分情况下都是可以的，但是Markdwon本身的特殊符号与Latex中的符号会出现冲突的时候:</p><ul><li><code>_</code>的转义，在markdown中，<code>_斜体_</code>是<em>斜体</em>，但是在latex中，却有下标的意思，就会出现问题。</li><li><code>\\</code>的换行，在markdown中，<code>\\</code>会被转义为<code>\</code>这样也会影响影响mathjax对公式中的<code>\\</code>进行渲染</li></ul><p>我从网上找到的解决办法大多都是使用pandoc提换markdown的渲染引擎：</p><h2 id="hexo-render-pandoc">hexo-render-pandoc</h2><p>使用 hexo-render-pandoc, 可以避免上述提到的很多歧义问题步骤如下：</p><ol type="1"><li>安装配置hexo-render-pandoc</li></ol><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">npm un hexo-renderer-marked --save#or npm un hexo-renderer-kramed --save (如果安装的是kramed的话)# 在安装之前，建议先删除node_modules文件夹下所有文件# 并运行命令 npm installnpm i hexo-renderer-pandoc --save</code></pre><p>前去官网安装pandoc，并保证能在命令行中运行 pandoc -v.</p><ol start="2" type="1"><li>主题配置打开mathjax开关</li></ol><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"># MathJax Supportmathjax:  enable: true</code></pre><ol start="3" type="1"><li>文章的Front-matter里打开mathjax开关(一定要打开，不然不会渲染)<pre class="line-numbers language-markdown" data-language="markdown"><code class="language-markdown">---title: index.htmldate: 2016-12-28 21:01:30tags:mathjax: true---</code></pre></li></ol><p>但是pandoc比较笨重，需要首先安装Pandoc,不过的确可以完美解决上述的不兼容问题，另外它的语法与markdown有些微的差异，需要注意。</p><h2 id="分离markdown和latex">分离markdown和latex</h2><h3 id="使用div或者span标签">使用<code>&lt;div&gt;</code>或者<code>&lt;span&gt;</code>标签</h3><p>针对有可能被转义的公式（如带换行符的），用<code>&lt;span&gt;</code>或者<code>&lt;div&gt;</code>标签将LaTeX公式包裹起来，这样公式内容就不会被markdown渲染器识别为转义字符。</p><p>比如： </p><pre class="line-numbers language-latex" data-language="latex"><code class="language-latex">$$\begin{pmatrix}    a_{11} &amp; a_{12} &amp; a_{13} \\    a_{21} &amp; a_{22} &amp; a_{23} \\    a_{31} &amp; a_{32} &amp; a_{33}  \end{pmatrix} $$</code></pre><code>\\</code>会无法识别导致不能换行，可以写成： <pre class="line-numbers language-latex" data-language="latex"><code class="language-latex">&lt;span&gt;$$\begin{pmatrix}    a_{11} &amp; a_{12} &amp; a_{13} \\    a_{21} &amp; a_{22} &amp; a_{23} \\    a_{31} &amp; a_{32} &amp; a_{33}  \end{pmatrix}$$&lt;/span&gt;</code></pre>渲染结果就正常了。<p></p><h3 id="使用hexo自带的分离标记推荐简单">使用hexo自带的分离标记（推荐，简单）</h3><p>在官方文档中提到了可以为hexo提供标记，阻止其按照自己的规则解释我们的字符串，显示其原本的含义</p><pre class="line-numbers language-latex" data-language="latex"><code class="language-latex">{% raw %}...{% endraw %}</code></pre><p>这样就可以避免markdown解释器对我们的字符串进行转义，而是直接输出原本的字符串。</p><p>比如： </p><pre class="line-numbers language-latex" data-language="latex"><code class="language-latex">{% raw %}$$\begin{align*}a &amp;= b + c \\    &amp;= d + e \\    &amp;= f + g\end{align*}$${% endraw %}</code></pre><p></p><p>我测试使用<code>&lt;div&gt;</code>或者<code>&lt;span&gt;</code>在多行公式渲染时依旧会出现异常，而使用<code></code>则可以正常渲染。</p>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> hexo公式渲染 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型优化-FlashAttention-v1</title>
      <link href="/2024/12/07/da-mo-xing-you-hua-flashattention-v1/"/>
      <url>/2024/12/07/da-mo-xing-you-hua-flashattention-v1/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>在传统的自注意力机制中，注意力矩阵的<strong>计算复杂度为O(N²)</strong>，其中 N是序列的长度。对于长序列的输入（如文本或图像中的像素点），这种计算代价极高，特别是在训练大型语言模型或视觉模型时，内存占用和计算开销随着序列长度的增加而急剧上升。此外，<strong>注意力矩阵的大小为N×N</strong>，这也对 GPU内存消耗极大。自注意力机制不仅在计算时消耗大量内存，还需要存储所有中间变量（如Q、K、V 矩阵及注意力权重），以支持后续的反向传播。</p><p>因此，找到有效降低 Transformer 模型 O(N²)复杂度的方案至关重要。理想情况下，若能将复杂度降至O(N)，将大大提升模型效率。即使无法完全实现O(N)，逼近这一复杂度也是十分有价值的。在这一背景下，Flash Attention应运而生，成为解决该问题的有效方案。</p><p>从 Flash Attention（Fast and Memory Efficient Exact Attention withIO-Awareness）的命名可见其优势：</p><ul><li>Fast：在FlashAttention之前，也出现过一些加速Transformer计算的方法，这些方法的着眼点是“减少计算量FLOPs”，例如用一个稀疏attention做近似计算。而FlashAttention发现：<strong>计算慢的卡点不在运算能力，而是在读写速度上。</strong>所以它通过降低对显存（HBM）的访问次数来加快整体运算速度，这种方法又被称为<strong>O-Awareness</strong>。</li><li>Memory Efficient：在 Flash Attention 中，内存使用压力从 O(N²) 降至O(N)，显著节省内存。</li><li>Exact Attention：与稀疏 Attention 不同，Flash Attention完全等效于标准 Attention。</li></ul><h2 id="背景知识">背景知识</h2><h3 id="计算限制与内存限制">计算限制与内存限制</h3><h4 id="受限原因分析">受限原因分析</h4><p>首先介绍几个关键概念：</p><ul><li><span class="math inline">\(\pi\)</span>：<strong>硬件算力上限</strong>，表示一个计算平台在全负荷情况下每秒能够执行的浮点运算次数，单位为FLOPS（浮点运算次数每秒）。</li><li><span class="math inline">\(\beta\)</span>：<strong>硬件带宽上限</strong>，表示一个计算平台在全负荷情况下每秒能够完成的数据交换量，单位为Byte/s。</li><li><span class="math inline">\(\pi_t\)</span>：<strong>某算法所需的总运算量</strong>，单位为FLOPs。（t 表示 total）</li><li><span class="math inline">\(\beta_t\)</span>：<strong>某算法所需的总数据读取和存储量</strong>，单位为Byte。</li></ul><blockquote><p>这里强调一下对FLOPS和FLOPs的解释：</p></blockquote><p>FLOPS：等同于FLOP/s，表示Floating Point Operations PerSecond，即每秒执行的浮点数操作次数，用于衡量硬件计算性能。</p><p>FLOPs：表示Floating PointOperations，表示某个算法的总计算量（即总浮点运算次数），用于衡量一个算法的复杂度。</p><p>在实际执行过程中，时间不仅消耗在计算上，也消耗在数据读取和存储上。因此，我们定义：</p><ul><li><span class="math inline">\(T_{cal}\)</span>：算法执行所需的计算时间，其公式为<span class="math inline">\(\pi_t / \pi\)</span>。</li><li><span class="math inline">\(T_{load}\)</span>：算法执行所需的数据读取与存储时间，公式为<span class="math inline">\(\beta_t / \beta\)</span>。</li></ul><p>由于计算和数据传输可以同时进行，我们定义算法的总执行时间：<span class="math inline">\(T = max(T_{cal}, T_{load})\)</span></p><ul><li>当 <span class="math inline">\(T_{cal}&gt;T_{load}\)</span>时，算法的瓶颈在计算部分，称为<strong>计算限制（math-bound）。</strong>此时，<span class="math inline">\(\pi_t / \pi &gt; \beta_t / \beta\)</span>，即<span class="math inline">\(\pi_t/\beta_t &gt; \pi/\beta\)</span>。</li><li>当 <span class="math inline">\(T_{cal}&lt;T_{load}\)</span>时，瓶颈在数据读取部分，称为<strong>内存限制</strong>（memory-bound）。此时，<span class="math inline">\(\pi_t / \pi &lt; \beta_t / \beta\)</span>，即<span class="math inline">\(\pi_t/\beta_t &lt; \pi/\beta\)</span>。</li></ul><p>算法的<strong>计算强度</strong>（Operational Intensity）定义为 <span class="math inline">\(\pi_t/\beta_t\)</span>，表示每个数据读取操作对应的计算量。当算法的计算强度越高，说明计算部分的工作量越大，反之则说明数据读取部分的工作量越大。</p><p>假设我们现在采用的硬件为A100-40GBSXM，同时采用混合精度训练（可理解为训练过程中的计算和存储都是fp16形式的，一个元素占用2byte），则：<span class="math display">\[ \pi/\beta = 312*10^{12} / 1555*10^9 = 201FLOPs/Bytes \]</span></p><p>对于一个模型，<span class="math inline">\(Q, K \in \mathbb{R}^{n\times d}\)</span>，其中N为序列长度，d为embedding dim。现在计算<span class="math inline">\(S = QK^T\)</span>，则有： <span class="math display">\[\frac{\pi_t}{\beta_t} = \frac{2N^2d}{2Nd + 2Nd + 2N^2} = \frac{N^2d}{2Nd+ N^2}\]</span></p><p>下表记录了不同的N，d下的受限类型： <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241207024735.png"></p><p>根据这个表格，我们可以来做下总结： -计算限制（math-bound）：大矩阵乘法（N和d都非常大）、通道数很大的卷积运算。相对而言，读得快，算得慢。-内存限制（memory-bound）：逐点运算操作。例如：激活函数、dropout、mask、softmax、BN和LN。相对而言，算得快，读得慢。</p><p>所以，“Transformer计算受限于数据读取”也不是绝对的，要综合硬件本身和模型大小来综合判断。但从表中的结果我们可知，memory-bound的情况还是普遍存在的，所以Flashattention的改进思想在很多场景下依然适用。</p><p>在Flashattention中，<strong>计算注意力矩阵时的softmax计算就受到了内存限制，这也是flashattention的重点优化对象</strong>，我们会在下文来详细看这一点。</p><h4 id="roof-line-模型">roof-line 模型</h4><p>一个算法运行的效率是离不开硬件本身的。我们往往想知道：对于一个运算量为<span class="math inline">\(\pi_t\)</span>，数据读取存储量为 <span class="math inline">\(\beta_t\)</span> 的算法，它在算力上限为 <span class="math inline">\(\pi\)</span>，带宽上限为 <span class="math inline">\(\beta\)</span> 的硬件上，能达到的最大性能 <span class="math inline">\(P\)</span>（Attanable Performance）是多少？</p><p>这里最大性能 <span class="math inline">\(P\)</span>指的是当前算法实际运行在硬件上时，每秒最多能达到的计算次数，单位是FLOP/s。</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241207025413.png"></p><p>从图中可以直观的看出，当计算强度达到了硬件的上限时，算法的性能达到最大值。而在之前的计算强度范围内，都属于内存限制。</p><h3 id="gpu存储与计算">GPU存储与计算</h3><h4 id="gpu存储分类">GPU存储分类</h4><p>通常，GPU 存储分为片上内存（on-chip memory）和片下内存（off-chipmemory），这主要取决于存储单元是否位于芯片内部。 <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241207030019.png"></p><ul><li><strong>片上内存</strong>：用于缓存等，容量小但带宽极高。如上图中的<strong>SRAM</strong>，容量仅 20MB，带宽却达 19TB/s。</li><li><strong>片下内存</strong>：用于全局存储（即显存），容量大但带宽相对较小。如HBM，容量可达 40GB，带宽为 1.5TB/s。</li></ul><h4 id="gpu的计算">GPU的计算</h4><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md/20240826055530.png">如图，负责GPU计算的一个核心组件叫SM（StreamingMultiprocessors，流式多处理器），可以将其理解成GPU的计算单元，一个SM又可以由若干个SMP（SMPartition）组成，例如图中就由4个SMP组成。SM就好比CPU中的一个核，但不同的是一个CPU核一般运行一个线程，但是一个SM却可以运行多个轻量级线程（由WarpScheduler控制，一个Warp Scheduler会抓一束线程（32个）放入cudacore（图中绿色小块）中进行计算）。</p><p>现在，我们将GPU的计算核心SM及不同层级GPU存储结构综合起来，绘制一张简化图：<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241207030737.png">- HBM2：即是我们的显存。 - L1缓存/sharedmemory：每个SM都有自己的L1缓存，用于存储SM内的数据，被SM内所有的cudacores共享。SM间不能互相访问彼此的L1。NV Volta架构后，L1和sharedmemory合并（Volta架构前只有Kepler做过合并），目的是为了进一步降低延迟。合并过后，用户能写代码直接控制的依然是sharedmemory，同时可控制从L1中分配多少存储给shared memory。<strong>Flashattention中SRAM指的就是L1 cache/shared memory</strong>。 -L2缓存：所有SM共享L2缓存。L2缓存不直接由用户代码控制。L1/L2缓存的带宽都要比显存的带宽要大，也就是读写速度更快，但是它们的存储量更小。</p><p>GPU 的计算流程可以理解为：数据从显存（HBM）加载到片上内存（SRAM），由SM（Streaming Multiprocessor）读取并进行计算，计算结果再通过 SRAM返回显存。具体可参考：<a href="https://www.yidoo.xyz/nvidia-gpu-principles">NVIDIA GPU原理详解</a>。</p><p>显存带宽远低于SRAM，因此从显存读取数据往往较耗时。为了优化读取效率，我们会<strong>尽量将数据填满SRAM，从而减少频繁读取</strong>。</p><h4 id="kernel-fusion">kernel fusion</h4><p>为减少显存读取次数，若 SRAM容量允许，多个计算步骤可合并在一次数据加载中完成。这被称为kernel融合。</p><p>举例来说，我现在要做计算A和计算B。在老方法里，我做完A后得到一个中间结果，写回显存，然后再从显存中把这个结果加载到SRAM，做计算B。但是现在我发现SRAM完全有能力存下我的中间结果，那我就可以把A和B放在一起做了，这样就能节省很多读取时间，我们管这样的操作叫kernel融合。</p><p>对于kernel可以粗犷地理解成是“函数”，它包含对线程结构（grid-block-thread）的定义，以及结构中具体计算逻辑的定义。理解到这一层已不妨碍我们对flashattention的解读了，想要更近一步了解的朋友，推荐阅读这篇<a href="https://zhuanlan.zhihu.com/p/34587739">小小将：CUDA编程入门极简教程</a>。</p><h3 id="标准attention计算">标准Attention计算</h3><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241208192423.png">其中， <span class="math inline">\(S=QK^T,P=softmax(S)\)</span>。在GPT类的模型中，还需要对<span class="math inline">\(P\)</span>做mask处理。为了表达方便，诸如mask、dropout之类的操作，我们都忽略掉，下文也是同理。</p><h3 id="标准safe-softmax">标准safe softmax</h3><p>在<span class="math inline">\(softmax(x_i) =\frac{e^{x_i}}{\sum_{j=1}^d e^{x_j}}\)</span>的计算中，如果<span class="math inline">\(x_i\)</span>的值很大，那么<span class="math inline">\(e^{x_i}\)</span>会变得非常大，这样就会导致数值溢出。为了解决这个问题，我们可以对<span class="math inline">\(x_i\)</span>做一个平移，即<span class="math inline">\(x_i - max(x)\)</span>，这样就能保证<span class="math inline">\(e^{x_i}\)</span>不会溢出。</p><p>下图展示了safe softmax的过程，这里 <span class="math inline">\(\tilde{P}, P\)</span>分别表示平移前后的softmax结果。 <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241208194350.png"></p><h2 id="flash-attention">Flash Attention</h2><h3 id="flash-attention的核心思想">Flash Attention的核心思想</h3><ul><li><strong>分块计算</strong>：将输入矩阵划分为小块，并逐块在 SRAM上计算注意力，避免将整个 N×N 矩阵存储于显存。</li><li><strong>重计算</strong>：通过前向传播时保存归一化因子，避免在反向传播中存储中间结果，而是通过重计算得出注意力矩阵。这虽然增加了浮点运算次数，但通过减少HBM 访问，提升了整体效率。</li></ul><h3 id="前向计算">前向计算</h3><h4 id="分块计算tiling">分块计算tiling</h4><p>我们先来了解分块计算的整体流程（帮助大家理解数据块是怎么流转的），然后我们再针对其中的细节做一一讲解。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241208195502.png"></p><p>在计算这些分块时，GPU是可以做并行计算的，这也提升了计算效率。</p><p>好！现在你已经知道了单块的计算方式，现在让我们把整个流程流转起来把。在上图中，我们注明了j 是外循环， i是内循环，在论文里，又称为K，V是外循环，Q是内循环。写成代码就是:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"># ---------------------# Tc: K和V的分块数# Tr: Q的分块数量# ---------------------for 1 &lt;= j &lt;= Tc:    for 1 &lt;= i &lt;= Tr:        do....</code></pre><p></p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241208201454.png"><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241208201509.png"></p><p>这里的 <span class="math inline">\(O\)</span>还需要经过一定的处理，才能和不分块场景下的 <span class="math inline">\(O\)</span>完全等价。这里我们将每一块的 <span class="math inline">\(O\)</span>单独画出，是为了帮助大家更好理解分块计算的整体流程，不代表它是最终的输出结果。</p><h4 id="tiliing中的safe-softmax">tiliing中的safe softmax</h4><p>回顾之前绘制的标准safe softmax流程图，我们知道m、l都是针对完整的一行做rowmax、rowsum后的结果，那么在分块场景下，会变成什么样呢？<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241208201852.png">以上图红圈内的数据为例，在标准场景下，我们是对红圈内的每一行做rowmax、rowsum后得到<span class="math inline">\(\tilde{P}\)</span>的。而分块后这部分数据会被分到不同的块中。</p><p>所以Flash Attention中采用如下方式实现safe softmax：</p><ol type="1"><li><p>我们假设标准场景下，<span class="math inline">\(S\)</span>矩阵某一行的向量为 <span class="math inline">\(x = [x_1, x_2, \dots,x_d]\)</span>，因为分块的原因， 它被我们切成了两部分 <span class="math inline">\(x = \begin{bmatrix} x^{(1)}, x^{(2)}\end{bmatrix}\)</span>。</p></li><li><p>我们定义：</p></li></ol><ul><li><span class="math inline">\(m(x)\)</span>：标准场景下，该行的全局最大值</li><li><span class="math inline">\(m(x^{(1)})\)</span>：分块1的全局最大值</li><li><span class="math inline">\(m(x^{(2)})\)</span>：分块2的全局最大值</li></ul><p>那么易知： <span class="math inline">\(m(x) = m\left( \begin{bmatrix}x^{(1)}, x^{(2)} \end{bmatrix} \right) = \max \left( m(x^{(1)}),m(x^{(2)}) \right)\)</span></p><ol start="3" type="1"><li>我们定义：</li></ol><ul><li><span class="math inline">\(f(x)\)</span>：标准场景下，<span class="math inline">\(\exp(x - m(x))\)</span> 的结果</li><li><span class="math inline">\(f(x^{(1)})\)</span>：分块场景下，<span class="math inline">\(\exp(x^{(1)} - m(x^{(1)}))\)</span> 的结果</li><li><span class="math inline">\(f(x^{(2)})\)</span>：分块场景下，<span class="math inline">\(\exp(x^{(2)} - m(x^{(2)}))\)</span> 的结果</li></ul><p>那么易知：<span class="math inline">\(f(x) = \left[ e^{m(x^{(1)}) -m(x)} f(x^{(1)}), e^{m(x^{(2)}) - m(x)} f(x^{(2)}) \right]\)</span></p><p>这个很好理解，详细的证明过程就不写了。</p><ol start="4" type="1"><li>我们定义：</li></ol><ul><li><span class="math inline">\(l(x)\)</span>：标准场景下，<span class="math inline">\(\text{rowsum}[f(x)]\)</span> 的结果</li><li><span class="math inline">\(l(x^{(1)})\)</span>：分块场景下，<span class="math inline">\(\text{rowsum}[f(x^{(1)})]\)</span> 的结果</li><li><span class="math inline">\(l(x^{(2)})\)</span>：分块场景下，<span class="math inline">\(\text{rowsum}[f(x^{(2)})]\)</span> 的结果</li></ul><p>那么由（3）易知：<span class="math inline">\(l(x) = e^{m(x^{(1)}) -m(x)} l(x^{(1)}) + e^{m(x^{(2)}) - m(x)} l(x^{(2)})\)</span></p><ol start="5" type="1"><li>现在，我们就可以用分块计算的结果，来表示标准场景下 safe softmax的结果了： <span class="math display">\[softmax(x) = \frac{f(x)}{l(x)} =\frac{\left[e^{m(x^{(1)}) - m(x)} f(x^{(1)}), e^{m(x^{(2)}) - m(x)}f(x^{(2)})\right]}{e^{m(x^{(1)}) - m(x)} l(x^{(1)}) + e^{m(x^{(2)}) -m(x)} l(x^{(2)})}\]</span></li></ol><p>我们配合上面的图例和flashattention论文中的伪代码，再来进一步理解一下分块计算safesoftmax的（1）～（5）步骤。</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241209021146.png"></p><p>我们用 <span class="math inline">\(S_{00}\)</span>（图中浅绿色方块）替换掉（1）<span class="math inline">\(\sim\)</span>（5）步骤中的 <span class="math inline">\(x^{(1)}\)</span>，用 <span class="math inline">\(S_{01}\)</span>（图中深绿色方块）替换掉 <span class="math inline">\(x^{(2)}\)</span>。我们关注点在伪代码部分的 6 <span class="math inline">\(\sim\)</span> 11 行。</p><p>由于伪代码中的表达符号较多，容易阻碍大家的理解，因此我们先明确各个数学符号表达的含义：</p><ul><li><span class="math inline">\(S_{ij}\)</span>：对应在我们的例子里，就是 <span class="math inline">\(S_{00}\)</span> 和 <span class="math inline">\(S_{01}\)</span>，即 <span class="math inline">\(Q_i K_j^\top\)</span> 的结果。</li><li><span class="math inline">\(m_{ij}\)</span>：对于当前分块 <span class="math inline">\(S_{ij}\)</span>来说，每行的局部最大值。相当于前面步骤（2）中对 <span class="math inline">\(m(x^{(1)})\)</span>, <span class="math inline">\(m(x^{(2)})\)</span> 的定义。</li><li><span class="math inline">\(\tilde{P}_{ij}\)</span>：分块场景下，各块的 <span class="math inline">\(p\)</span>矩阵（归一化前）结果。相当于步骤（3）中对 <span class="math inline">\(f(x^{(1)})\)</span>, <span class="math inline">\(f(x^{(2)})\)</span> 的定义。</li><li><span class="math inline">\(l_{ij}\)</span>：分块场景下，<span class="math inline">\(\text{rowsum}\)</span> 的结果。相当于步骤（4）中对<span class="math inline">\(l(x^{(1)})\)</span>, <span class="math inline">\(l(x^{(2)})\)</span> 的定义。</li><li><span class="math inline">\(m\)</span>：标准场景下，对 <span class="math inline">\(S\)</span>矩阵而言，每行的最大值，这里是全局最大值（<span class="math inline">\(m\)</span> 首次定义在伪代码第 2行），相当于前面步骤（2）中对 <span class="math inline">\(m(x)\)</span>的定义。</li><li><span class="math inline">\(l\)</span>：标准场景下，全局 <span class="math inline">\(\text{rowsum}\)</span> 的结果（<span class="math inline">\(l\)</span> 首次定义在伪代码第 2行），相当于前面步骤（4）中对 <span class="math inline">\(l(x)\)</span>的定义。</li><li><span class="math inline">\(m_i\)</span>：表示 <span class="math inline">\(\max(m_{i0}, m_{i1}, \dots,m_{i(j-1)})\)</span>。如果当前分块是 <span class="math inline">\(S_{ij}\)</span>，则 <span class="math inline">\(m_i\)</span> 表示固定 <span class="math inline">\(i\)</span> 时，前 <span class="math inline">\(j -1\)</span> 个分块中的局部最大值。容易推出，当固定 <span class="math inline">\(i\)</span>，遍历完成 <span class="math inline">\(S_{00}, S_{01}\)</span> 后，<span class="math inline">\(m_i\)</span> 的结果就是全局最大值 <span class="math inline">\(m_0\)</span>。</li><li><span class="math inline">\(m_i^{\text{new}}\)</span>：表示 <span class="math inline">\(\max(m_{i0}, m_{i1}, \dots, m_{i(j-1)},m_{ij})\)</span>。如果当前分块为 <span class="math inline">\(S_{ij}\)</span>，则 <span class="math inline">\(m_i^{\text{new}}\)</span> 表示固定 <span class="math inline">\(i\)</span>时，截止到当前分块为止的局部最大值。</li><li><span class="math inline">\(l_i\)</span>：和 <span class="math inline">\(m_i^{\text{new}}\)</span>对应，相当于步骤（4）中用分块更新 <span class="math inline">\(l(x)\)</span> 的步骤。</li><li><span class="math inline">\(l_i\)</span>：和 <span class="math inline">\(m_i\)</span> 同理，即当我们将 <span class="math inline">\(j\)</span> 遍历完后，我们就能得到针对 <span class="math inline">\(i\)</span> 的全局 rowmax 和全局 rowsum。</li></ul><p>而根据前面的定义，<span class="math inline">\(m_i^{\text{new}}\)</span> 和 <span class="math inline">\(l_i^{\text{new}}\)</span> 是遍历完成最新的 <span class="math inline">\(S_{ij}\)</span> 后得到的 rowmax 和 rowsum 结果，所以每遍历完一块 <span class="math inline">\(S_{ij}\)</span>，我们就执行伪代码的第 13行，做一次更新。</p><p>从伪代码 5-13 行中，你会发现，在整个计算过程中，只有 <span class="math inline">\(m_i, l_i, O_i\)</span> 被从 on-chip 的 SRAM中写回到显存（HBM）中。 把 <span class="math inline">\(i\)</span>都遍历完成后，读写量也不过是 <span class="math inline">\(m, l,O\)</span>。相比于标准场景下，我们要读写的是 <span class="math inline">\(S, P,O\)</span>，读写量是不是一下就少很多，这不就能解决 memory-bound的问题了吗。</p><p>所以，<strong>分块计算 safe softmax 的意义，就是抹去对 <span class="math inline">\(S, P\)</span> 的读写</strong>。</p><h4 id="分块计算中的o">分块计算中的<span class="math inline">\(O\)</span></h4><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241209024821.png">上图中画的6个<span class="math inline">\(O\)</span>并不是我们最终想要的结果。我们期望维护并更新<span class="math inline">\(O_i\)</span>，当该 <span class="math inline">\(i\)</span>下的所有 <span class="math inline">\(j\)</span> 遍历完毕后，我们的 <span class="math inline">\(O_i\)</span>就应该和标准场景下的 <span class="math inline">\(O_i\)</span>完全相等。</p><p>在图中，<span class="math inline">\(O_i\)</span>应该是红圈部分的乘积，但是我们只存了<span class="math inline">\(m_i, l_i, O\)</span>，但是没有存<span class="math inline">\(S,P\)</span>，所以到了最后一块我们也无法计算出<span class="math inline">\(O_i\)</span>。</p><p>所以这里我们换个思路： <span class="math inline">\(O_i\)</span>不是每遍历一块就更新一次吗？那有没有一种办法，<strong>不断用当前最新的rowmax和rowsum去更新<span class="math inline">\(O_i\)</span>，直到遍历完最后一块，这时的<span class="math inline">\(O_i\)</span>不就和标准场景下的结果完全一致了吗？也就是我们想构造形如下面这样的更新等式：</strong><span class="math display">\[O_i = O_i + 当前最新结果\]</span></p><p>因此我们有了伪代码中12行的推导： $$\begin{aligned}O_i^{(j+1)} &amp;= P_{i, j+1} V_{:j+1} \\            &amp;= \text{softmax}(S_{i, :j+1}) V_{:j+1} \\            &amp;= \text{diag}(l^{(j+1)})^{-1} \left[ \exp\left([S_{i, :j}, S_{i(j+1)}] - m^{(j+1)}\right) \right]             \begin{bmatrix}                 V_{:j} \\                 V_{j+1}            \end{bmatrix} \\            &amp;= \text{diag}(l^{(j+1)})^{-1} \left[\exp(S_{i, :j} - m^{(j+1)})V_{:j} + \exp(S_{i(j+1)} - m^{(j+1)})V_{j+1}\right] \\            &amp;= \text{diag}(l^{(j+1)})^{-1} \left[\exp(S_{i, :j} - m^{(j)})V_{:j} + e^{-m^{(j+1)}}\exp(S_{i(j+1)})V_{j+1}\right] \\             &amp;= \text{diag}(l^{(j+1)})^{-1} \left[\text{diag}(l^{(j)})e^{m^{(j)} - m^{(j+1)}}O_i^{(j)} + e^{-m^{(j+1)}}\exp(S_{i(j+1)})V_{j+1}\right] \\             &amp;= \text{diag}(l^{(j+1)})^{-1} \left[\text{diag}(l^{(j)})e^{m^{(j)} - m^{(j+1)}}O_i^{(j)} + e^{m^{(j)} - m^{(j+1)}} \tilde{P}_{i(j+1)} V_{j+1}\right]\end{aligned}$$</p><p>推导过程中的符号上下标的含义： - <span class="math inline">\(i\)</span>：这个大家应该很熟悉了。例如图例中，<span class="math inline">\(i=0, 1, 2\)</span>分别对应着深浅绿、深浅蓝、深浅黄块。 - <span class="math inline">\((j+1)\)</span>：表示当前分块的相关结果。 - <span class="math inline">\(i,:j+1\)</span>：表示截止到当前分块（包含当前分块）的相关结果。<span class="math inline">\(i, :j\)</span>表示截止到前一分块（包含前一分块）的相关结果。</p><h3 id="后向计算">后向计算</h3><h4 id="softmax的求导">softmax的求导</h4><p>设 <span class="math display">\[\begin{cases}    y = \text{softmax}(z) \\    L = f(y)\end{cases}\]</span></p><p>其中，<span class="math inline">\(L\)</span> 表示 Loss，<span class="math inline">\(f(\cdot)\)</span> 表示 Loss 函数，<span class="math inline">\(y = [y_1 \; y_2 \; y_3]\)</span>，<span class="math inline">\(z = [z_1 \; z_2 \; z_3]\)</span>，若现在我们想求<span class="math inline">\(\frac{\partial L}{\partialz_j}\)</span>，要怎么计算呢？</p><p>根据链式法则，我们有： <span class="math display">\[\frac{\partial L}{\partial z_j} = \frac{\partial L}{\partial y}\frac{\partial y}{\partial z_j}\]</span> 所以我们分别来看这两项。</p><ol type="1"><li><p><span class="math inline">\(\frac{\partial L}{\partialy}\)</span> 我们现在不考虑具体的 Loss 函数，直接假设这一项的结果为 <span class="math inline">\([m_1 \; m_2 \; m_3]\)</span>。</p></li><li><p><span class="math inline">\(\frac{\partial y}{\partialz_j}\)</span> 我们知道，对于某个 <span class="math inline">\(z_j\)</span> 来说，在 <span class="math inline">\(\text{softmax}\)</span> 的操作下，它参与了 <span class="math inline">\(y_1, y_2, y_3\)</span> 三者的计算，因此它的偏导和这三者密切切相关，这里我们分成两种情况： <span class="math display">\[\begin{cases}\frac{\partial y_i}{\partial z_j} = y_i (1 - y_i), &amp; \text{当 } i =j \\\frac{\partial y_i}{\partial z_j} = -y_i y_j, &amp; \text{当 } i \neq j\end{cases}\]</span></p></li></ol><p>具体的推倒过程可以看这篇文章：<a href="https://www.cnblogs.com/wuliytTaotao/p/10787510.html">对 softmax和 cross-entropy 求导</a></p><p>有了这个理解，我们再来谈谈基于 <span class="math inline">\(y =softmax(z)\)</span> 的 Jacobian 矩阵 <span class="math inline">\(diag(y)- y^T y\)</span>：</p>$$\begin{aligned}diag(y) - y^T y &amp;=\begin{bmatrix}y_1 &amp; 0 &amp; 0 \\0 &amp; y_2 &amp; 0 \\0 &amp; 0 &amp; y_3\end{bmatrix}-\begin{bmatrix}y_1 \\ y_2 \\ y_3\end{bmatrix}*\begin{bmatrix}y_1 &amp; y_2 &amp; y_3\end{bmatrix} \\&amp;=\begin{bmatrix}y_1 - y_1^2 &amp; -y_1 y_2 &amp; -y_1 y_3 \\-y_2 y_1 &amp; y_2 - y_2^2 &amp; -y_2 y_3 \\-y_3 y_1 &amp; -y_3 y_2 &amp; y_3 - y_3^2\end{bmatrix}\end{aligned}$$<p>很容易发现只要把每行/每列相加，就能得到对应 <span class="math inline">\(z\)</span>的偏导。别着急求和，我们继续往下看。</p><ol start="3" type="1"><li>$ = $</li></ol><p>有了 (1) (2) 的结果，现在就可以来推导 <span class="math inline">\(\frac{\partial L}{\partial z_j}\)</span>，我们有：<span class="math display">\[\frac{\partial L}{\partial z_j} = \frac{\partial L}{\partial y}\frac{\partial y}{\partial z_j}= \sum_{i=1}^l \frac{\partial L}{\partial y_i} \frac{\partialy_i}{\partial z_j} = y_j (d y_j - \sum_{j=1}^l y_j d y_j)\]</span></p><p>举个例子，若我们现在想求 <span class="math inline">\(\frac{\partialL}{\partial z_1}\)</span>，我们将 <span class="math inline">\(\frac{\partial L}{\partial y} = [m_1 \; m_2 \;m_3]\)</span> 代入上面公式，则有：</p><p><span class="math display">\[\frac{\partial L}{\partial z_1} = m_1 (y_1 - y_1^2) - m_2 y_1 y_2 - m_3y_1 y_3\]</span></p><p>现在，针对所有的 <span class="math inline">\(z\)</span>，我们将 <span class="math inline">\(\frac{\partial L}{\partial z}\)</span>写成矩阵表达式有：</p>$$\begin{aligned}\frac{\partial L}{\partial z} &amp;= \frac{\partial L}{\partial y} \frac{\partial y}{\partial z} = dy(diag(y) - y^T y) \\&amp;= [m_1 \; m_2 \; m_3]\begin{bmatrix}y_1 &amp; 0 &amp; 0 \\0 &amp; y_2 &amp; 0 \\0 &amp; 0 &amp; y_3\end{bmatrix}-\begin{bmatrix}y_1 \\ y_2 \\ y_3\end{bmatrix}\begin{bmatrix}y_1 &amp; y_2 &amp; y_3\end{bmatrix} \\ &amp;= [m_1 \; m_2 \; m_3]\begin{bmatrix}y_1 - y_1^2 &amp; -y_1 y_2 &amp; -y_1 y_3 \\-y_2 y_1 &amp; y_2 - y_2^2 &amp; -y_2 y_3 \\-y_3 y_1 &amp; -y_3 y_2 &amp; y_3 - y_3^2\end{bmatrix}\end{aligned}$$<p><strong>至此，大家记住这两个重要的结论：</strong></p><p><span class="math display">\[\frac{\partial L}{\partial z} = \frac{\partial L}{\partial y}\frac{\partial y}{\partial z} = dy(diag(y) - y^T y)\]</span></p><p><span class="math display">\[\frac{\partial L}{\partial z_j} = y_j \left( dy_j - \sum_{j=1}^l y_jdy_j \right)\]</span></p><h4 id="标准后向计算">标准后向计算</h4><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241209041228.png">首先我们先回顾一下标准前向过程： <span class="math display">\[S = QK^T\]</span></p><p><span class="math display">\[P = \text{softmax}(S)\]</span></p><p><span class="math display">\[O = PV\]</span></p><p><span class="math display">\[L = f(O)\]</span></p><p>对于标准backward来说，在计算开始时，显存（HBM）上已经存放有<span class="math inline">\(Q, K, V, O, S, P\)</span>这些数据。</p><h4 id="分块后向计算">分块后向计算</h4><p>首先回顾一下经过分块 Forward计算后，显存（HBM）上都存了哪些数据：</p><ul><li><strong><span class="math inline">\(m\)</span></strong>：全局rowmax</li><li><strong><span class="math inline">\(l\)</span></strong>：全局rowsum</li><li><strong><span class="math inline">\(Q, K,V\)</span></strong>：等同于标准 attention 场景下的结果</li><li><strong><span class="math inline">\(O\)</span></strong>：等同于标准attention 场景下的输出结果 <span class="math inline">\(O\)</span></li><li><strong><span class="math inline">\(dO\)</span></strong>：有了完整的<span class="math inline">\(O\)</span>，我们就可以按正常的 backward步骤先求出它的梯度，也存放在显存上。然后我们就能按照链式法则，分块地去求列的矩阵的梯度了。</li></ul><p>既然有了全局的 <span class="math inline">\(m,l\)</span>，那么现在对任意一块 <span class="math inline">\(S_{ij}\)</span>，我们就能基于 <span class="math inline">\(m, l\)</span> 算出和标准场景下完全一致的 <span class="math inline">\(P_{ij}\)</span> 了。 <strong>因此，在 backward的过程中，flash attention 将采用重计算的方式，重新算出 <span class="math inline">\(S_{ij}, P_{ij}\)</span>， 并将它们运用到 backward的计算中去</strong>，所以在接下来的讲解中，大家就可以把 <span class="math inline">\(S, P\)</span> 理解成完全等同于标准场景下的结果，而不是像分块计算 forward 中那样的 <span class="math inline">\(S,P\)</span>。</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241209042346.png"></p><ol type="1"><li>求 <span class="math inline">\(V_j\)</span> 梯度</li></ol><p>由 Forward 过程我们知：<span class="math inline">\(O =PV\)</span>，因此有了 <span class="math inline">\(dO\)</span>后，我们就可以先来求 <span class="math inline">\(dP\)</span> 和 <span class="math inline">\(dV\)</span> 了。观察下方的图，我们会发现此时所有的 <span class="math inline">\(P\)</span>都是不带波浪号的，再强调一下，这是因为经过了重计算， 此处 <span class="math inline">\(S, P\)</span>的结果都等同于标准场景下的结果，而不是 forward 中所代表的含义。</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241209042743.png"></p><p>假设现在 <span class="math inline">\(j = 0\)</span>，那我们要怎么求<span class="math inline">\(dV_0\)</span> 呢？</p><p>我们先来看 <span class="math inline">\(V_0\)</span> 都参与了 <span class="math inline">\(O\)</span>哪些部分的计算，以及是怎么参与的：由图可知，<span class="math inline">\(P_{00}\)</span> 和 <span class="math inline">\(V_0\)</span> 参与了 <span class="math inline">\(O_0\)</span> 的计算， <span class="math inline">\(P_{10}\)</span> 和 <span class="math inline">\(V_0\)</span> 参与了 <span class="math inline">\(O_1\)</span> 的计算，<span class="math inline">\(P_{20}\)</span> 和 <span class="math inline">\(V_0\)</span> 参与了 <span class="math inline">\(O_2\)</span> 的计算。所以我们有：</p><p><span class="math display">\[dV_0 = (P_{00})^T dO_0 + (P_{10})^T dO_1 + (P_{20})^T dO_2\]</span></p><p>进而推知： <span class="math display">\[dV_j = \sum_i (P_{ij})^T dO_i\]</span></p><p>在伪代码 11～15 行中，做的都是 <span class="math inline">\(S,P\)</span> 重计算的过程，伪代码的第 16 行，就是在按这个方法分块计算并累积 <span class="math inline">\(dV_j\)</span>。</p><ol start="2" type="1"><li>求 <span class="math inline">\(P_{ij}\)</span> 梯度</li></ol><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241209043105.png"></p><p>观察上图，可以发现 <span class="math inline">\(P_{ij}\)</span> 只与<span class="math inline">\(V_j, O_i\)</span> 相关，例如 <span class="math inline">\(P_{10}\)</span> 只与 <span class="math inline">\(V_0, O_1\)</span> 相关。因此我们有：</p><p><span class="math display">\[dP_{ij} = dO_i V_j^T\]</span></p><p>这就是伪代码第 17 行做的事情。</p><ol start="3" type="1"><li>求 <span class="math inline">\(S_{ij}\)</span> 梯度</li></ol><p>这一块是令许多人感到迷惑的，我们先来回顾下 “softmax 求导”部分让大家记住的一个重要结论：</p><p><span class="math display">\[\frac{\partial L}{\partial z} = \frac{\partial L}{\partial y}\frac{\partial y}{\partial z} = dy(diag(y) - y^T y)\]</span></p><p>我们假设 <span class="math inline">\(s_i, p_i, o_i\)</span>分别为矩阵 <span class="math inline">\(S, P, O\)</span>的某一行（注意这里 <span class="math inline">\(i\)</span> 不是表示第<span class="math inline">\(i\)</span> 块的意思，是表示第 <span class="math inline">\(i\)</span> 行，所以我们用小写的 <span class="math inline">\(s, p, o\)</span>表示），那么根据这个结论，我们有：</p><p><span class="math display">\[\begin{aligned}ds_i &amp;= dp_i \left( diag(p_i) - p_i^T p_i \right) \\&amp;= dp_i diag(p_i) - dp_i p_i^T p_i \\&amp;= dp_i diag(p_i) - dO_i V_j^T p_i \\&amp;= dp_i diag(p_i) - dO_i o_i^T p_i \\&amp;= p_i \circ \left[ dp_i - \text{rowsum}(dO_i \cdot o_i) \right]\end{aligned}\]</span> 你可能对这个推导的最后一步有疑惑：为什么要大费周章，将 <span class="math inline">\(ds_i\)</span>改写成这么复杂的形式呢？因为在最后一步之前，我们都是针对“某一行”来求导，而引入最后一步的目的，是为了延展至对“某一块（多行）”的求导，也就是说针对某一块<span class="math inline">\(dS_i\)</span>（注意这里是大写的 <span class="math inline">\(S\)</span>，<span class="math inline">\(i\)</span>的含义也回归至“第几块”），我们有：</p><p><span class="math display">\[dS_i = P_i \circ \left[dP_i - \text{rowsum}(dO_i \circ O_i)\right]\]</span></p><p>如果实在难以理解推导过程，建议大家可以带一些具体的值进去，就能理解我们为什么要写成这种形式了。进而，我们可以推知：</p><p><span class="math display">\[dS_{ij} = P_{ij} \circ \left[dP_{ij} - \text{rowsum}(dO_i \circO_i)\right]\]</span></p><p>这就是伪代码第 19～20 行做的事情。</p><ol start="4" type="1"><li>求 <span class="math inline">\(Q_i\)</span> 梯度</li></ol><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241209043616.png"></p><p>到目前为止，我们已经知道 <span class="math inline">\(dS_{ij}\)</span>，那么现在就可以根据链式法则继续求<span class="math inline">\(dQ_i\)</span> 了。</p><p>对照上图，我们把目光聚焦在 <span class="math inline">\(Q_0\)</span>身上，由 forward 过程可知：</p><p><span class="math display">\[S_{00} = Q_0 K_0^T\]</span></p><p><span class="math display">\[S_{01} = Q_0 K_1^T\]</span></p><p>因此，针对 <span class="math inline">\(Q_0\)</span>，我们有：</p><p><span class="math display">\[dQ_0 = dS_{00} K_0 + dS_{01} K_1\]</span></p><p>推广到任意 <span class="math inline">\(Q_i\)</span>，我们有：</p><p><span class="math display">\[dQ_i = \sum_j dS_{ij} K_j\]</span></p><p>这就是伪代码第 21 行做的事情。</p><ol start="5" type="1"><li>求 <span class="math inline">\(K_j\)</span> 梯度 <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241209043830.png"></li></ol><p>这一步就很简单啦，如果你被复杂的分块推导弄得晕了脑袋，那不妨再复习一下我们前面提过的trick： 对照上图，取出某一块 <span class="math inline">\(K_j\)</span>。由于我们是从 <span class="math inline">\(dS_{ij}\)</span> 链式推向 <span class="math inline">\(K_j\)</span>，所以这里只要搞明白这块 <span class="math inline">\(K_j\)</span> 和哪些 <span class="math inline">\(Q_i\)</span> 一起计算出了哪些 <span class="math inline">\(S_{ij}\)</span> 再把相关结果相加即可。</p><p>只要看了流程图，就不难得知：某块 <span class="math inline">\(K_j\)</span> 和对应的 <span class="math inline">\(Q_i\)</span> 共同计算出了对应的 <span class="math inline">\(S_{ij}\)</span>，因此有：</p><p><span class="math display">\[dK_j = \sum_i dS_{ij}^T Q_i\]</span></p><p>这就是伪代码第 22 行做的事情。</p><h2 id="计算量与显存占用">计算量与显存占用</h2><h3 id="矩阵相乘的计算量">矩阵相乘的计算量</h3><p>我们先来看一个前置知识：两个矩阵相乘，要怎么统计它们的计算量？</p><p>我们一般用<strong>FLOPs（floating pointoperations，浮点运算次数）</strong>来表示运算量的大小。对于“两矩阵相乘”这个操作而言，其<strong>运算量 = </strong>乘法运算的次数 + 加法运算的次数**。</p><p>来看一个具体例子：</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241209224248.png"></p><p>两矩阵相乘，为了获取图中深橘色部分的元素，我们一共需要进行<strong>n次乘法运算和n-1次加法运算</strong>。</p><p>对于示例矩阵，我们需要进行：<span class="math inline">\(mp \cdot (n +n - 1)\)</span> 次浮点计算。</p><p>再进一步，假设此时在蓝色和绿色的矩阵外，我们还有一个bias矩阵，意味着计算单个橘色方块时我们需要进行<em>n次乘法和n-1+1次加法运算</em>，那么此时总计算量为：<span class="math inline">\(mp \cdot (n+n) =2mnp\)</span>。当然，即使不加这个bias，我们也可以把-1项给忽略，得到相同的结果。</p><p>总结一下： - 假设有两个矩阵A和B，它们的维度分别为(m, n)和(n,p)，则这两矩阵相乘的运算量为<strong>2mnp</strong>。 -由于乘法运算的时间要高于加法运算的时间，因此有时在统计运算量时，我们只考虑乘法运算的次数，则此时两矩阵相乘的运算量可近似为<strong>mnp</strong>。</p><h3 id="flash-attention的计算量">Flash Attention的计算量</h3><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241209021146.png"></p><p>我们知道矩阵相乘运算占据了运算量的大头，因此我们把分析目光集中到所有的矩阵运算上来。</p><ol type="1"><li><p>在代码第 9 行，我们有 <span class="math inline">\(S_{ij} = Q_iK_j^T\)</span>，其中 <span class="math inline">\(Q_i \in \mathbb{R}^{B_r\times d}\)</span>，<span class="math inline">\(K_j^T \in \mathbb{R}^{d\times B_c}\)</span>。根据前置知识，求 <span class="math inline">\(S_{ij}\)</span> 的计算量为 <span class="math inline">\(O(B_r B_c d)\)</span>。</p></li><li><p>在代码第 12 行，我们有 <span class="math inline">\(\tilde{P}_{ij}V_j\)</span>，其中 <span class="math inline">\(\tilde{P}_{ij} \in\mathbb{R}^{B_r \times B_c}\)</span>，<span class="math inline">\(V_j\in \mathbb{R}^{B_c \times d}\)</span>。则这里的计算量同样为 <span class="math inline">\(O(B_r B_c d)\)</span>。</p></li><li><p>接下来我们看一共计算了多少次 (1) 和(2)，也就是执行了多少次内循环：</p></li></ol><p><span class="math display">\[T_c T_r = \frac{N}{B_c} \cdot \frac{N}{B_r}\]</span></p><ol start="4" type="1"><li>综合以上三点，<strong>flash attention 的 forward计算量为</strong>：</li></ol><p><span class="math display">\[O\left(\frac{N^2}{B_r B_c} B_r B_c d \right) = O(N^2 d)\]</span></p><p>注意，因为计算量是用大 O 阶表示的，所以这里我们把常数项都省略了。</p><p>同理大家可以自行推一下 backward 中的计算量，在论文里给出的结论是<span class="math inline">\(O(N^2)\)</span>，d 远小于 N，因此 d也可以略去不表述。</p><h3 id="flash-attention的显存占用">Flash Attention的显存占用</h3><p>和标准 attention 相比，如果不考虑 <span class="math inline">\(O\)</span> 的话，Flash Attention 只需要存储 <span class="math inline">\(m, l\)</span>，其显存需求为 <span class="math inline">\(O(N)\)</span>。</p><p>而标准 attention 需要存储 <span class="math inline">\(S,P\)</span>，其显存需求为 <span class="math inline">\(O(N^2)\)</span>。</p><p>FlashAttention 将显存需求降低到O(N)，通过分块处理和重计算，<strong>显著减少了显存使用</strong>。实验显示，其显存消耗可减少至标准Attention 的 1/20。</p><h2 id="io复杂度分析">IO复杂度分析</h2><p>flashattention相比于标准attention的最大优势，就是其<strong>减少了对显存（HBM）的访问次数</strong>，一定程度上解决了memorybound的问题。所以这一节我们就来具体分析这两者对显存的访问次数（同样都是以forward为例，backward部分论文中也有给出相关推导过程，大家可以类比forward自行阅读）。### 标准Attention的IO复杂度</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241209233647.png"></p><ol type="1"><li><strong>从 HBM 中读取 <span class="math inline">\(Q, K \in\mathbb{R}^{N \times d}\)</span>，计算 <span class="math inline">\(S = QK^T\)</span>，<span class="math inline">\(S \in \mathbb{R}^{N \timesN}\)</span> 并将 <span class="math inline">\(S\)</span> 写回HBM</strong>。<ul><li>一读一写的 IO 复杂度为：<span class="math inline">\(O(2Nd +N^2)\)</span>。</li></ul></li><li><strong>从 HBM 中读取 <span class="math inline">\(S \in\mathbb{R}^{N \times N}\)</span>，同时计算 <span class="math inline">\(P\in \mathbb{R}^{N \times N}\)</span> 并将其写回 HBM</strong>。<ul><li>一读一写的 IO 复杂度为：<span class="math inline">\(O(2N^2)\)</span>。</li></ul></li><li><strong>从 HBM 中读取 <span class="math inline">\(P \in\mathbb{R}^{N \times N}, V \in \mathbb{R}^{N \times d}\)</span>，计算<span class="math inline">\(O = P V, O \in \mathbb{R}^{N \timesd}\)</span> 并将 <span class="math inline">\(O\)</span> 写回HBM</strong>。<ul><li>两读一写的 IO 复杂度为：<span class="math inline">\(O((N^2 + Nd) +Nd)\)</span>。</li></ul></li></ol><p><strong>因此，总体来说标准 attention 的 IO 复杂度为：<span class="math inline">\(O(Nd + N^2)\)</span>。</strong></p><h3 id="flash-attention的io复杂度">Flash Attention的IO复杂度</h3><ol type="1"><li><p>我们来看伪代码的第 6 行，在每个外循环中，我们都会加载 <span class="math inline">\(K, V\)</span> 的block。所有外循环结束后，相当于我们加载了完整的 <span class="math inline">\(K, V \in \mathbb{R}^{N \timesd}\)</span>，因此这里的 IO 复杂度为：<span class="math inline">\(O(2Nd)\)</span>。</p></li><li><p>再看伪代码第 8 行，在每个内循环中，我们都加载部分 <span class="math inline">\(Q, O, m, l\)</span> block。由于 <span class="math inline">\(m, l\)</span> 本身比较小（IO 复杂度是 <span class="math inline">\(O(N)\)</span>），因此我们暂时忽略它们，只考虑<span class="math inline">\(Q, O\)</span>（原论文也是这么分析的）。固定某个小循环，对于所有内循环结束后，我们相当于完整遍历了 <span class="math inline">\(Q, O \in \mathbb{R}^{N \timesd}\)</span>。同时我们经历了 <span class="math inline">\(T_c\)</span>次外循环。因此这里最终的 IO 复杂度为：<span class="math inline">\(O(T_cNd)\)</span>。</p></li><li><p><strong>将 <span class="math inline">\(O, m, l\)</span> 写回HBM</strong>，这里近似后 IO 复杂度为：<span class="math inline">\(O(Nd)\)</span>。</p></li></ol><p>不过在原论文的分析中并没有考虑写回的复杂度，不过省略一些常数项不会影响我们最终的分析。</p><p>总体来说，<strong>flash attention 的 IO 复杂度为</strong>： <span class="math display">\[O(T_c Nd + Nd) = O\left(\frac{N}{B_c} Nd \right) = O\left(\frac{4Nd}{M}Nd \right) = O\left(N^2 d \frac{d}{M} \right)\]</span> 在文章中提到，一般的 <span class="math inline">\(d\)</span>取值在 64～128，<span class="math inline">\(M\)</span> 的取值在 100KB左右，因此有 <span class="math inline">\(\frac{d^2}{M} \ll1\)</span>。因此可以看出，<strong>Flash Attention 的 IO复杂度是显著小于标准 attention 的 IO 复杂度的</strong>。</p><h3 id="复杂度总结">复杂度总结</h3><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241210000354.png"></p><h2 id="实验">实验</h2><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241210000654.png"></p><p>Flash attention 的作者将 <span class="math inline">\(N =1024\)</span>, <span class="math inline">\(d = 64\)</span>, <span class="math inline">\(B = 64\)</span> 的 GPT2-medium 部署在 A100 GPU上，来观察采用 flash attention 前后的模型的计算性能。</p><p>我们先看最左侧图表，标准 attention 下，<strong>计算强度 <span class="math inline">\(I = \frac{66.6}{40.3} \approx 1.6 &lt;201\)</span></strong>，说明 GPT2 在 A100上的训练是受到内存限制的。而在采用 flash attention后得到明显改善，runtime 也呈现了显著下降。</p><p>我们再来看中间的图表，它表示在使用 flash attention 的前提下，以forward 过程为例，每个数据块的大小对 HBM读写次数（绿色）和耗时（蓝色）的影响。可以发现，数据块越大，读写次数越少，而随着读写次数的减少，runtime也整体下降了（复习一下，读写复杂度为 <span class="math inline">\(O(T_cNd)\)</span>，数据块越大意味着 <span class="math inline">\(T_c\)</span>越小）。<strong>但有趣的是，当数据块大小 <span class="math inline">\(&gt; 256\)</span> 后，runtime的下降不明显了，这是因为随着矩阵的变大，计算耗时也更大了，会抵平读写节省下来的时间</strong>。</p><p>参考资料 &gt; 图解大模型计算加速系列：FlashAttentionV1，从硬件到计算逻辑:https://zhuanlan.zhihu.com/p/669926191 &gt;图解大模型计算加速系列：Flash AttentionV2，从原理到并行计算:https://zhuanlan.zhihu.com/p/691067658</p>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLMs </tag>
            
            <tag> FlashAttention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>个人博客加入谷歌元标记与站点地图</title>
      <link href="/2024/12/04/ge-ren-bo-ke-jia-ru-gu-ge-yuan-biao-ji-yu-zhan-dian-di-tu/"/>
      <url>/2024/12/04/ge-ren-bo-ke-jia-ru-gu-ge-yuan-biao-ji-yu-zhan-dian-di-tu/</url>
      
        <content type="html"><![CDATA[<h2 id="背景">背景</h2><p>在建立好我们的博客或者个人网站后，需要再让自己的链接被搜索引擎所收录。这里有两种方法能够被搜索引擎添加自己网站的索引。一个是自己努力提高自己的网站知名度，让搜索引擎主动去添加索引。另外一种就是自己把自己的链接添加到搜索引擎的索引当中。</p><h2 id="查看是否收录">查看是否收录</h2><p>在google或者百度等搜索引擎中搜索<code>site:website addres</code>，查看是否已经被收录。</p><p>如果搜索不出来自己等网站，那么就需要自己添加到搜索引擎的索引当中。下面以google为例，介绍如何添加自己的网站到google的索引当中。</p><h2 id="添加谷歌元标记">添加谷歌元标记</h2><p>首先你需要一个谷歌账号，然后使用<a href="https://search.google.com/search-console/about">Google SearchConsole</a>服务，点击立即使用！</p><p>接下来你可以看到如下的验证界面: <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205035222.png"></p><p>这里选择前缀，然后输入自己网站地址。之后按照提示添加meta标签到自己的网站中。对于hexo博客，可以在<code>themes/next/layout/_partials/head.ejs</code>文件中添加。</p><p>在添加完元标记后，进行网址检查，如果显示网址未收录，点击右下角的请求编入索引，别着急，一般来说一到两天后Google就会收录。</p><h2 id="添加站点地图">添加站点地图</h2><h3 id="生成站点地图">生成站点地图</h3><ul><li>安装sitemap插件：<code>npm install hexo-generator-sitemap --save</code></li><li>生成站点地图：<code>hexo generate</code>，然后在<code>public</code>目录下会生成<code>sitemap.xml</code>文件</li><li>修改配置：<ul><li>在<code>_config.yml</code>文件中添加如下配置 <pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml">Plugins:    - hexo-generator-sitemapsitemap:    path: /sitemap.xml</code></pre></li><li>在主题配置文件中添加如下配置 <pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml">menu:...sitemap: /sitemap.xml || fa fa-sitemap...</code></pre> #### 提交站点地图还是Google SearchConsole，点击左侧边栏中的站点地图，添加新的站点地图，在主站地址后面填入sitemap.xml，即与前面生成的站点地图文件名称相同！</li></ul></li></ul><p>等待Googlec处理，一般来说一到两天后Google就会收录。</p><blockquote><p>https://blog.csdn.net/tzhuwb/article/details/125477001https://mizeri.github.io/2021/04/18/hexo-sitemap-google/</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> google search console </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> sitemap </tag>
            
            <tag> google search console </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型高效微调方法PEFT--LoRA/QLoRA</title>
      <link href="/2024/12/04/da-mo-xing-gao-xiao-wei-diao-fang-fa-peft-lora-qlora/"/>
      <url>/2024/12/04/da-mo-xing-gao-xiao-wei-diao-fang-fa-peft-lora-qlora/</url>
      
        <content type="html"><![CDATA[<h2 id="参数高效微调peft">参数高效微调PEFT</h2><h4 id="微调">微调</h4><p>微调（Fine-tuning）是一种迁移学习的技术，用于在一个已经预训练好的模型基础上，通过进一步训练来适应特定的任务或数据集。微调可以在具有相似特征的任务之间共享知识，从而加快训练速度并提高模型性能。</p><p>以下是一般的微调步骤：</p><ul><li><strong>选择择预训练模型</strong>：选择一个在大规模数据集上预训练好的模型，如ImageNet上的预训练的卷积神经网络（如ResNet、VGG等）。这些模型通常具有良好的特征提取能力。</li><li><strong>冻结底层权重</strong>：将预训练模型的底层权重（通常是卷积层）固定住，不进行训练。这是因为底层权重通常学习到了通用的特征，可以被用于许多不同的任务。</li><li><strong>替换顶层分类器</strong>：将预训练模型的顶层分类器（通常是全连接层）替换为适合特定任务的新的分类器。新的分类器的输出节点数量应该与任务的类别数相匹配。</li><li><strong>解冻部分权重（可选）</strong>：根据任务的复杂性和可用的训练数据量，可以选择解冻一些底层权重，以便更好地适应新的任务。这样可以允许底层权重进行微小的调整，以更好地适应新任务的特征。</li><li><strong>进行训练</strong>：使用特定任务的训练数据集对新的分类器进行训练。可以使用<strong>较小的学习率</strong>进行训练，以避免对预训练模型的权重进行过大的更新。</li><li><strong>评估和调整</strong>：在训练完成后，使用验证集或测试集评估模型的性能。根据评估结果，可以进行调整，如调整学习率、调整模型结构等。</li></ul><p>微调的关键是在预训练模型的基础上进行训练，从而将模型的知识迁移到特定任务上。通过这种方式，可以在较少的数据和计算资源下，快速构建和训练高性能的模型。</p><h4 id="peft">PEFT</h4><p>PEFT（Parameter-EfficientFine-Tuning，参数高效微调）是一种在大语言模型（Large LanguageModel，LLM）上进行微调的新技术，旨在降低微调大模型的计算和存储成本，使得微调过程更为高效。与传统的微调技术相比，PEFT可以显著减少训练所需的参数量，并保持与标准微调相似的性能。PEFT技术在优化资源效率、降低硬件需求、并提高开发灵活性方面有显著优势，特别是在需要频繁微调以适应不同任务或领域的场景中尤为有用。</p><p>目前主流的方法包括2019年 Houlsby N 等人提出的 AdapterTuning，2021年微软提出的 LORA，斯坦福提出的 Prefix-Tuning，谷歌提出的Prompt Tuning，2022年清华提出的 P-tuning v2。</p><ul><li>Adapter Tuning在模型的某些层之间插入小的适配器模块，而不对原有的大量参数进行修改。适配器模块通常包含一小部分可训练的参数，并通过这些参数来调整模型的输出；但是增加了模型层数，引入了额外的推理延迟</li><li>Prefix-Tuning 每个 Transformer层前面添加一个可学习的前缀（prefix）来实现模型的调整。这个前缀是固定长度的，可视作特定任务的提示（prompt）；但是预留给Prompt 的序列挤占了下游任务的输入序列空间，影响模型性能</li><li>P-tuning v2使用可学习的嵌入来替代硬编码的提示文本；但是很容易导致旧知识遗忘，微调之后的模型，在之前的问题上表现明显变差</li><li>基于上述背景，LORA 得益于前人的一些关于内在维度（intrinsicdimension）的发现：</li></ul><blockquote><p>模型是过参数化的，它们有更小的内在维度，模型主要依赖于这个低的内在维度（lowintrinsic dimension）去做任务适配。</p></blockquote><p>假设模型在任务适配过程中权重的改变量是低秩（lowrank）的，由此提出<strong>低秩自适应</strong>（LoRA）方法。</p><p>LoRA允许我们通过优化适应过程中密集层变化的秩分解矩阵，来间接训练神经网络中的一些密集层，同时保持预先训练的权重不变。</p><h2 id="lora">LoRA</h2><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241204190635.png">LoRA 的思想很简单:</p><ul><li><p>在原始 PLM (Pre-trained Language Model)旁边增加一个旁路，做一个降维再升维的操作，来模拟所谓的intrinsicrank。</p></li><li><p>训练的时候固定 PLM 的参数，只训练降维矩阵与升维矩阵。而模型的输入输出维度不变，输出时将与 PLM的参数叠加。</p></li><li><p>用随机高斯分布初始化 A，用 0 矩阵初始化B，保证训练的开始此旁路矩阵依然是 0 矩阵。 &gt;为什么A用随机高斯分布初始化，B用0矩阵初始化呢？</p></li><li><p>首先增量矩阵<span class="math inline">\(\Delta W = BA\)</span>在训练刚开始肯定是要用0来初始化。</p></li><li><p>如果A用0矩阵初始化，在计算梯度时由于B的梯度有一个因子A，导致B的梯度始终为0，无法被更新，导致梯度传播受阻。</p></li></ul><p>我们也可以理解为因为A是降维矩阵，所以它能够学到一些有用的信息，所以用随机高斯分布初始化；而B是升维矩阵，我们希望它能够保持原始信息，所以用0矩阵初始化。</p><p><strong>秩的选择</strong> 在 Transformer 中，LoRA 主要作用在attention 过程的四个权重矩阵W_Q, W_K, W_V,W_O（也可以选择其中部分）。A，B矩阵的秩r也时远远小于原始的权重大小。对于一般的任务，r=1，2，4，8就足够了。而一些领域差距比较大的任务可能需要更大的r。同时，增加r值变大并不能提升微调的效果，这可能是因为参数量增加需要更多的语料.<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241204204545.png"></p><p><strong>实现</strong> huggingface:https://huggingface.co/docs/peft/main/conceptual_guides/lorallama_factory:https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/sft.html关键参数： - lora_r：秩，控制低秩矩阵的表示能力。 -lora_alpha：缩放因子，控制 LoRA 的影响力。 - lora_dropout：dropout概率，防止过拟合。 - lora_target_modules：指定微调的目标层。 -lora_trainable_only：是否只训练 LoRA 增加的部分。 -lora_init_scale：控制低秩矩阵初始化的尺度。 -lora_layers_to_freeze：指定冻结哪些层。 - lora_lr：设置 LoRA 的学习率。- lora_warmup_steps：指定热身阶段的步数。</p><h2 id="qlora">QLoRA</h2><p>QLoRA（Quantized Low-Rank Adapter）是一种高效的微调方法，是 LoRA的量化版本（什么是 LoRA？）。该调优方法由华盛顿大学发表于论文《QLORA:Efficient Finetuning of Quantized LLMs》。通过降低内存使用，实现在单个GPU 上对大型语言模型进行微调。它可以在单个 48GB GPU 上微调 650亿个参数的模型，并且能够保持完整的 1 6 位微调任务性能。</p><p>QLoRA 核心是使用量化进行微调。其成果有三个。 - 4位标准浮点数量化（4-bit NormalFloat Quantization） - 双重量化（DoubleQuantization） - 分页优化（PagedOptimizer）就是显存不足时将部分梯度检查点转移到内存上。</p><p>为了降低极端的量化对微调造成的不利影响，<strong>QLoRA 借助 LoRA对原模型权重进行更新</strong>，且 LoRA 侧的权重保持高精度。这就是 QLoRA与 LoRA 的关系。 <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241204231609.png">### 基础知识 #### 量化<strong>量化</strong>指的是将连续或高精度的数值转换为较低精度（比如较少的位数）的表示形式的过程。这通常用于减少模型的存储需求和加快其运算速度。例如，将一个FP32 的 tensor 转成 Int8: <span class="math display">\[X^{\text{Int8}} =\text{round}\left(\frac{127}{\text{absmax}(X^{\text{FP32}})} \cdotX^{\text{FP32}}\right)=round(c^{\text{FP32}} \cdotX^{\text{FP32}})\]</span></p><p>其中，c为量化常数，通常是这个张量的特征的绝对值的最大值。逆量化公式则为： <span class="math display">\[\text{dequant}(c^{\text{FP32}}, X^{\text{Int8}})= \frac{X^{\text{Int8}}}{c^{\text{FP32}}}\]</span></p><p>按照量化过程是否以0点为对称点量化又可以分为对称量化和非对称量化。其中对称量化将原浮点数的最小或最大值的绝对值作为映射值的范围，而非对称量化是将原浮点数的最小和最大值映射为量化数据的最小和最大值。在非对称量化中，0的映射也可能会有偏移，因此不一定会被映射到0。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205004225.png"></p><h4 id="分位数量化">分位数量化</h4><p>分位数量化（QuantileQuantization）是隶属于非线性量化。分位数（Quantile）在数学上的定义指的是把顺序排列的一组数据分割为若干个相等块的分割点的数值。在标准正态分布中，对于分布X给定的概率<span class="math inline">\(\alpha\)</span>，如果存在<span class="math inline">\(\mu_\alpha\)</span>使得它的累积分布函数（CDF）<span class="math inline">\(P(X &lt; \mu_\alpha)= \alpha\)</span>，则称<span class="math inline">\(\mu_\alpha\)</span>是标准正态分布的 <span class="math inline">\(\alpha\)</span>分位数，因为CDF表示的是概率值小于<span class="math inline">\(\mu_\alpha\)</span>的阴影部分的面积，因此具有严格递增的特性，所以它一定存在反函数。CDF的反函数的一个重要作用是用来生成服从该随机分布的随机变量。假设a是[0,1)区间上均匀分布的一个随机变量，那么<span class="math inline">\(F_{X}^{-1}(a)\)</span>服从分布<span class="math inline">\(X\)</span>。 <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205004852.png"></p><p><strong>分位数量化是通过分位数将张量分成了大小相同的若干个块，这样我们得到更加均匀的量化特征</strong>，对于4比特量化，我们希望需要找到15个分位数来将这个曲线下面的面积（积分）等分成16份。两个分位数的中点便是模型量化到这个区间映射的值<span class="math inline">\(q_i\)</span>。 <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205005255.png"></p><p>由于大模型参数通常服从正态分布，因此有： <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205015121.png">其中<span class="math inline">\(Q\)</span>是CSF的反函数。</p><h4 id="分块k位量化">分块k位量化</h4><p>常规的量化方法的局限性是当 tensor 中有一个非常大的数字（一般称为outlier）时，导致大多数值被压缩到较小的范围，精度损失大，影响最终的量化结果。</p><p>因此，Block-wise k-bit Quantization 方法就是把 tensor 分割成 B块，每块有自己的量化常数c，独自量化。从而解决模型参数的极大极小的异常值的问题。分块量化的另外一个好处是减少了核之间的通信，可以实现更好的并行性，并充分利用硬件的多核的能力。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205010158.png"></p><h3 id="位标准浮点量化---nf4">4位标准浮点量化---NF4</h3><p>4-bit NormlFLoat 量化是结合了分位数量化和分块量化，使用上面介绍的分位数量化方法我们可以将FP2精度量化到4bit的精度，但是直接这么用的一个问题是不能保证高精度的0一定被映射到低精度的0，但是0点又是深度学习中一个重要的值，例如在模型稀疏化，数据padding的时候一般都是使用0来完成。</p><p>假设offset的值是0.99，我们可以通过下面的代码片段计算出它的16个 <span class="math inline">\(q_i\)</span>。 </p><pre class="line-numbers language-python" data-language="python"><code class="language-python">offset = 0.99num_bins = 16quantile = norm.ppf(torch.linspace(1 - offset, offset, num_bins + 1)).tolist() # 将[1-offset,offset]区间等分为16份tmp = [(quantile[1:][idx] + val) / 2 for idx, val in enumerate(quantile[:-1])] # 计算分位数r_max, r_min = tmp[-1], tmp[0]S = (r_max - r_min)/(1 - (-1))Z = 1 - r_max / SQ = [x/S + Z for x in tmp] # 分位数量化到[-1,1]print (Q)&gt;&gt;&gt; Q = [-1.0, -0.680534899946304, -0.5217156169574965, -0.4015399993912077, -0.299784167882981, -0.20835410767681603, -0.12291223249970012, -0.040639059218818274, 0.04063881956774142, 0.12291199284862328, 0.20835391124150712, 0.2997839714476721, 0.40153976366883704, 0.5217154126647753, 0.6805348056573558, 1.0]</code></pre><p></p><p>这种方式的一个问题是0的映射值不是0，如果我们考虑奇数个bin，0是可以有个映射值但是却无法充分利用4比特的16位的信息。为了确保零点映射到0并且使用4位数据类型的全部16位，我们通过估计正负两个范围的分位数来创建一个非对称的数据类型：负数部分映射其中7位，正数部分映射8位，0占据1位，总共用满了4位数的16位。另外我们也可以使用对称的量化，其中正数和负数均使用7位，0占用2个位。我这里和论文介绍的略有不同，论文说的是正数部分取9个值，负数部分取8个值，不过它们都会取到0，所以合并时再去掉一个重复的0，这两个说法其实是一样的，只是实现方式略有差异。</p><p>接下来根据作者的源码来看下量化分位数如何计算的。其中核心代码片段摘抄如下。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">from scipy.stats import normimport torchdef create_normal_map(offset=0.9677083, use_extra_value=True, num_bins=16):    # INT8 : num_bins = 256    # INT4 : num_bins = 16    if use_extra_value:        # one more positive value, this is an asymmetric type        v1 = norm.ppf(torch.linspace(offset, 0.5, 9)[:-1]).tolist() # 正数部分        v2 = [0]*(num_bins-15) ## we have 15 non-zero values in this data type        v3 = (-norm.ppf(torch.linspace(offset, 0.5, 8)[:-1])).tolist() #负数部分        v = v1 + v2 + v3    else:        v1 = norm.ppf(torch.linspace(offset, 0.5, 8)[:-1]).tolist()        v2 = [0]*(num_bins-14) ## we have 14 non-zero values in this data type        v3 = (-norm.ppf(torch.linspace(offset, 0.5, 8)[:-1])).tolist()        v = v1 + v2 + v3    values = torch.Tensor(v)    values = values.sort().values    values /= values.max()    assert values.numel() == num_bins    return values    Q = create_normal_map()&gt;&gt;&gt; Q = [-1.0, -0.6961928009986877, -0.5250730514526367, -0.39491748809814453, -0.28444138169288635, -0.18477343022823334, -0.09105003625154495, 0.0, 0.07958029955625534, 0.16093020141124725,0.24611230194568634, 0.33791524171829224, 0.44070982933044434, 0.5626170039176941, 0.7229568362236023, 1.0]函数create_normal_map有两个入参：offset和use_extra_value。其中offset的作用是确定分位数的始末值。use_extra_value用来控制是使用对称量化还是非对称量化。v1计算正数部分，v3计算负数部分。v2直接将0映射到0，并且根据要量化的单位计算0的个数。源码是使用NF4来表示8比特的量化，如果是使用4比特的量化，我们将里面的256改成16就行。接下来最后几行用来将量化值归一化到[-1,1]。接下来我们举一个具体的实例。假设一个张量有16个值，它的被分成了4块：```pythoninput_blocked_tensor = [[-1.28645003578589, -1.817660483275528, 9.889441349505042, 0.010208034676132627], [ -15.009014631551885, 1.4136255086268115, -7.815595761491153, 10.766760590950263],  [-0.731406153917959, 3.468224595908726, 2.445252541840315, -8.970824523299282],  [-9.641638854625175, 7.696158363188889, -5.323939281255154, 5.97160401402024]]</code></pre>根据每个块的特征的绝对值的最大值，我们为每个块保存一个量化常数，它的计算方式是每个块中特征的绝对值中最大的那个：<pre class="line-numbers language-python" data-language="python"><code class="language-python">c1 = max(|-1.28645003578589|, |-1.817660483275528|, |9.889441349505042|, |0.010208034676132627|) = 9.889441349505042c2 = max(|-15.009014631551885|, |1.4136255086268115|, |-7.815595761491153|, |10.766760590950263|) = 15.009014631551885c3 = max(|-0.731406153917959|, |3.468224595908726|, |2.445252541840315|, |-8.970824523299282|) = 8.970824523299282c4 = max(|-9.641638854625175|, |7.696158363188889|, |-5.323939281255154|, |5.97160401402024|) = 9.641638854625175</code></pre><p></p><p>最后我们便可以计算这个张量的量化值了。例如第一个值-1.28645003578589，它除以这个块的量化常数c1后得到-0.13008318572517502，接下来我们要按照分位数的值来量化这个值。-0.13008318572517502在Q中最接近的值是-0.12291223249970012，这个值在Q中对应的索引是6，因此这个值被量化后的值是6。同理我们可以得到这个输入张量所有的值量化后的结果。<strong>在模型保存时，除了要保存量化后的值，我们还要保存每个块对应的量化常数c_i</strong>，因为这个值在我们进行反量化时需要用到。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">[[6, 5, 15, 7],[0, 8, 2, 14],[6, 11, 10, 0],[0, 14, 2, 13]]</code></pre>在反量化时，我们以量化结果作为索引，从Q中查找到它对应的分位数，再乘以为每个块保存的量化常数c_i，便可以得到最终结果。<pre class="line-numbers language-python" data-language="python"><code class="language-python">[[-0.9004339933799617, -1.8273060011889755, 9.889441349505042, 0.0], [-15.009014631551885, 1.1944218804231184,  -7.880829111886221,  10.850869732860506], [-0.816793898052648, 3.0313783372030603, 2.2078302737800004, -8.970824523299282], [-9.641638854625175, 6.970488722350373, -5.062564734402345, 5.424549965245643]]</code></pre><p></p><h3 id="双量化">双量化</h3><p>在上面我们介绍到，当我们保存模型时我们不仅要保存量化后的结果，还要保存每个块的量化常数。虽然量化后的参数只有4bit的精度，但是这个量化常量的精度是float32。</p><p>**在QLoRA中，每个块的大小是64，块中的每个值占4比特。这相当于为了存储量化常数，模型要额外占用32/（64*4）=12.5% 的显存。**</p><p>QLoRA的双重量化就是对这个量化常数再做一次8bit的量化，在进行量化常数的量化时，QLoRA以每256个量化常数为一组再做一次量化。因此它额外增加的内存消耗有两部分组成，一部分是量化后的8bit的第一层的量化常数，它额外增加的显存占比是8/（64x4）= 3.125%，第二部分是为量化常数做量化的第二层的32bit的量化常数，它额外增加的显存占比是32/（256x64x4）=0.049%，额外显存增加只有3.17%。</p><h3 id="分页优化器">分页优化器</h3><p>分页优化是针对梯度检查点做的进一步优化，以防止在显存使用峰值时发生显存OOM的问题。QLoRA分页优化其实就是当显存不足是，将保存的部分梯度检查点转移到CPU内存上，和计算机的内存数据转移到硬盘上的常规内存分页一个道理。</p><h3 id="实现">实现</h3><p>要使用 HuggingFace 进行 QLoRA 微调，您需要安装BitsandBytes 库和 PEFT库。BitsandBytes 库负责 4 位量化以及整个低精度存储和高精度计算部分。PEFT库将用于 LoRA 微调部分。 </p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_modelfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfigmodel_id = "EleutherAI/gpt-neox-20b"bnb_config = BitsAndBytesConfig(    load_in_4bit=True,    bnb_4bit_use_double_quant=True,    bnb_4bit_quant_type="nf4",    bnb_4bit_compute_dtype=torch.bfloat16) # setup bits and bytes configmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={"":0})model.gradient_checkpointing_enable()model = prepare_model_for_kbit_training(model) # prepares the whole model for kbit trainingconfig = LoraConfig(    r=8,     lora_alpha=32,     target_modules=["query_key_value"],     lora_dropout=0.05,     bias="none",     task_type="CAUSAL_LM")model = get_peft_model(model, config) # Now you get a model ready for QLoRA training</code></pre>在模型的微调过程中，虽然主干参数以 4-bit精度存储，但计算（例如前向传播、反向传播和梯度计算）可以使用更高精度的数据类型<code>bnb_4bit_compute_dtype</code>，以减少由于低精度存储引起的累积误差。<p></p><p>参考资料： &gt; https://arxiv.org/pdf/2106.09685</p><blockquote><p>https://arxiv.org/abs/2305.14314</p></blockquote><blockquote><p>https://zhuanlan.zhihu.com/p/623543497</p></blockquote><blockquote><p>https://mingchao.wang/ShYWOOwr/</p></blockquote><blockquote><p>https://zhuanlan.zhihu.com/p/690739797</p></blockquote><blockquote><p>https://zhuanlan.zhihu.com/p/666234324</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> LoRA </tag>
            
            <tag> PEFT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型优化--PagedAttention</title>
      <link href="/2024/12/03/da-mo-xing-you-hua-pagedattention/"/>
      <url>/2024/12/03/da-mo-xing-you-hua-pagedattention/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>基于KV Cache的大模型推理过程通常分为两个阶段：Prefill 和Decoding。</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205200821.png"></p><p>在 Prefill 阶段，推理引擎将整段 prompt输入模型进行前向计算。如果引入了 KV Cache 技术，prompt 经 Wk 和 Wv计算得到的结果（即 K 和 V）会被存储到 K Cache 和 V Cache中。这样，在后续 token 的 Attention 计算中，无需重复计算 K 和V，从而显著节约计算时间。</p><p>进入 Decoding 阶段后，推理引擎会基于 Prefill阶段的结果，逐步生成响应，每次输出一个 token。如果同样使用了 KVCache，每生成一个 token，都会将其对应的 K 和 V值存入缓存，加速后续推理。</p><p>下图展示了一个13B的模型在A10040GB的gpu上做推理时的显存占用分配（others表示forward过程中产生的activation的大小，这些activation你可以认为是转瞬即逝的，即用完则废，因此它们占据的显存不大），从这张图中我们可以直观感受到推理中KVcache对显存的占用。因此，<strong>如何优化KVcache，节省显存，提高推理吞吐量，就成了LLM推理框架需要解决的重点问题。</strong><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205201020.png"></p><h2 id="kv-cache的常规存储分配">KV Cache的常规存储分配</h2><p>由于推理所生成的序列长度大小无法事先预知，所以大部分推理框架都会按batch_size x max_seq_len这样的固定尺寸来分配。在请求到来时，预先在显存中申请一块连续的区域。然而，这种“静态”显存分配策略，显存利用率是很低的。</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205201213.png"></p><p>我们假设 max_seq_len =8，所以当第1条请求(prompt1)过来时，我们的推理框架为它安排了(1,8)大小的连续存储空间。</p><p>当第2条请求（prompt2）过来时，同样也需要1块(1,8)大小的存储空间。但此时prompt1所在的位置上，只剩3个空格子了，所以它只能另起一行做存储。对prompt3也是同理。</p><p>问题：</p><ul><li>浅色块：prefill阶段prompt的KVcache，是无论如何都会被使用的空间，它不存在浪费。</li><li>中色块：decode阶段的KVcache，其中<eos>表示序列生成的截止符。虽然这些中色块最终都会被我们用上，但是在decode阶段一个个token生成时，我们并不能预知哪些块会被最终用上，导致一种“潜在的浪费”，称为<strong>预留碎片（reservation fragment）</strong>。</eos></li><li>深色块：decode阶段预留空间但是最终没有用上，称为<strong>内部碎片（internalfragment）</strong>。</li><li>灰色块：不是预留的KVcache的一部分，且最终也没有被用上，称这些灰色块为<strong>外部碎片（externalfragment）</strong>。想象一下，此时新来了一条prompt4，它也要求显存中的8个格子作为KVcache。此时你的显存上明明有9个空格子，但因为它们是不连续的碎片，所以无法被prompt4所使用。这时prompt4的这条请求只好在队列中等待，直到gpu上有足够显存资源时再进行推理。</li></ul><p>观察整个KV cache排布，你会发现它们的毛病在于太过<strong>“静态化”</strong>。当你无法预知序列大小时，你为什么一定要死板地为每个序列预留KVcache空间呢？为什么不能做得更动态化一些，即“用多少占多少”呢？这样我们就能减少上述这些存储碎片，使得每一时刻推理服务能处理的请求更多，提高吞吐量，这就是vLLM在做的核心事情，我们先通过一张实验图来感受下vLLM在显存利用上的改进效果（VS其它推理框架）： <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205202611.png"></p><p>不难发现，相比于别的推理框架，vLLM几乎能做到将显存完全打满。</p><h2 id="pagedattention">PagedAttention</h2><p>vLLM通过一种名为PagedAttention的技术，动态地为请求分配KVcache显存，提升显存利用率。</p><p>整体上来说，PagedAttention的设计灵感来自操作系统中虚拟内存的分页管理技术。</p><h3 id="操作系统的虚拟内存">操作系统的虚拟内存</h3><p>虚拟内存分页是一种操作系统管理内存的技术，它将物理内存抽象为一个连续的虚拟地址空间，使每个进程都像拥有独立的内存。分页通过将内存划分为<strong>固定大小的页（虚拟内存）和页框（物理内存）进行管理</strong>。页表记录虚拟页到物理页框的映射关系。若虚拟页不在物理内存中，会发生页缺失（PageFault），操作系统会从磁盘调入对应页。分页提高了内存利用率，并支持进程隔离与动态内存扩展。</p><h4 id="分段式内存管理">分段式内存管理</h4><p>在分段式内存管理中，虚拟内存会尽量为每个进程在物理内存上找到一块连续的存储空间，让进程加载自己的全部代码、数据等内容。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205213618.png"></p><p>在这个例子中，3个进程的虚拟内存各自为它们在物理内存上映射了一块连续的存储空间。在某一时刻，我释放了进程2，同时想运行进程4。这时我尴尬地发现，虽然物理内存上有640M的空间剩余，但因为是碎片化的，我的进程4无法加载进去，因此它只能等待。</p><p>在这个情况下，如果我硬要运行进程4，也是有办法的：我可以先把进程3从物理内存上交换（swap）到磁盘上，然后把进程4装进来，然后再把进程3从磁盘上加载回来。通过这种方法我重新整合了碎片，让进程4能够运行。</p><p>但这种办法的显著缺点是：如果进程3过大，同时内存到磁盘的带宽又不够，整个交换的过程就会非常卡顿。这就是分段式内存管理的缺陷。</p><h4 id="分页式内存管理">分页式内存管理</h4><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205213826.png"></p><p>虚拟内存的分页管理技术总结起来就是：</p><ul><li>将物理内存划分为固定大小的块，我们称每一块为页（page）。从物理内存中模拟出来的虚拟内存也按相同的方式做划分</li><li>对于1个进程，我们不需要静态加载它的全部代码、数据等内容。我们想用哪部分，或者它当前跑到哪部分，我们就动态加载这部分到虚拟内存上，然后由虚拟内存帮我们做物理内存的映射。</li><li>对于1个进程，虽然它在物理内存上的存储不连续（可能分布在不同的page中），但它在自己的虚拟内存上是连续的。通过模拟连续内存的方式，既解决了物理内存上的碎片问题，也方便了进程的开发和运行。</li></ul><h3 id="pagedattention-工作流程">PagedAttention 工作流程</h3><p>假设现在你向推理引擎发送请求，prompt 为 Four score and seven yearsago our，你希望模型进行续写。PagedAttention 的运作流程如下 <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241206002752.png"></p><h4 id="prefill-阶段">Prefill 阶段：</h4><p><strong>划分逻辑块</strong>：vLLM 在接收到这条 prompt后，会根据设定的块大小 B（本例中 B=4），将 prompt划分为若干逻辑块（Logical KV Blocks）。由于该 prompt 包含 7 个token，因此 vLLM 使用两个逻辑块（block 0 和 block 1）来存储它们的 KV值。在逻辑块 1 中，目前仅存储了 "years"、"ago" 和 "hour" 这 3 个 token的 KV 值，还有 1 个位置为空，称为保留位（Reservation）。</p><p><strong>划分物理块</strong>：完成逻辑块的划分后，这些逻辑块会映射到物理块，即实际存储KV 值的空间。映射关系通过一张 block table（块表）记录，其主要内容包括：- <strong>逻辑块与物理块的映射关系</strong>（Physical BlockNumber）：例如，逻辑块 0 映射到物理块 7。 -<strong>物理块已填满的槽位数量</strong>（# Filled）：例如，在 Prefill阶段，物理块 7 的 4 个槽位已填满，而物理块 1 的 4 个槽位中填满了 3个。</p><p>系统正常计算 prompt 的 KV值后，根据上述划分关系，将这些值填入对应的物理块中。</p><h4 id="decode-阶段">Decode 阶段：</h4><p>在使用 KV cache 计算 attention 并生成第一个词 "fathers"时，不难发现，计算过程中使用的是<strong>逻辑块</strong>，即从形式上看，这些token 是连续的。与此同时，vLLM 通过后台的 block table映射关系，从对应的物理块中获取数据以完成实际计算。通过这种方式，每个请求都能认为自己是在一个连续且充足的存储空间上操作，尽管这些数据在物理存储上并不是连续的。</p><p>基于新生成的词，系统会对逻辑块、物理块和 block table进行更新。例如，对于 block table，vLLM 将其 filled 字段从 3 更新为4。</p><p>当 "fathers" 被写入后，当前逻辑块已装满，因此 vLLM会开辟一个新的逻辑块（逻辑块 2），并同步更新 block table和对应的物理块，以确保后续生成过程能够顺利进行。</p><h4 id="pagedattention在不同解码策略下的运作">PagedAttention在不同解码策略下的运作</h4><p>我们知道，根据实际需求，大模型的解码方式也比较复杂，例如：</p><ul><li><strong>ParallelSampling</strong>：我给模型发送一个请求，希望它对prompt做续写，并给出三种不同的回答。我们管这个场景叫parallelsampling。在这个场景中，我们可以将prompt复制3次后拼接成1个batch喂给模型，让它做推理。但我们也需注意到，这种方式会产生prompt部分KVcache的重复存储。</li><li><strong>Beam Search束搜索</strong>：这是LLM常用的deocde策略之一，即在每个decode阶段，我不是只产生1个token，而是产生topk个token（这里k也被称为束宽）。top k个token必然对应着此刻的topk个序列。我把这topk个序列喂给模型，假设词表的大小为|V|，那么在下一时刻，我就要在k*|V|个候选者中再选出topk，以此类推。不难想象每一时刻我把topk序列喂给模型时，它们的前置token中有大量的KV cache是重复的。</li><li><strong>Sharedprefix</strong>：在某些大模型中，所有请求可能都会共享一个前置信息（比如systemmessage:“假设你是一个有帮助的AI助手...."），这些前置信息没有必要重复存储KVcache</li><li><strong>其余一般场景</strong>：在一些更通用的场景中，虽然两个prompt可能完全没有关系，但它们中某些KVcache却是可以共用的。例如两个prompt的相同位置（position）恰好出现了完全一样的序列，比如它们的结尾都是好想下班。假设这个相同序列已经存在于KVcache中，那也没有必要重复计算和存储了。 <strong>纠正：对于部分序列部分相同的情况，应该是只能作用在layer1的KVcache上，而不能作用在后序的KVcache上。因为计算完layer1的attention后，后面的序列已经不重复了。</strong></li></ul><h5 id="parallel-sampling">Parallel Sampling</h5><p><strong>传统KV cache怎么做</strong>： 假设模型的max_seq_len =2048。传统KVcache可能在显存中分配两块长度是2048的空间。由于prompt一致，这两块2048的空间中存在大量重复的KVcache。</p><p><strong>vLLM怎么做</strong>： <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241206004803.png">假定我们发给模型1个request，这个request中包含2个prompt/sample，记为SampleA1和Sample A2，这两个prompt完全一致，都为Four score and seven years agoour，我们希望模型对这两个prompt分别做续写任务。 - Prefill阶段： -分配逻辑块：对于A1，vLLM为其分配逻辑块block0和block1；对于A2，vLLM为其分配逻辑块block0和block1。需要注意的是，A1的逻辑块和A2的逻辑块是独立的（尽管它们都叫block0和block1），你可以将A1和A2视作操作系统中两个独立运行的进程。-分配物理块：对于A1和A2，虽然逻辑块独立，但因为它们的文字完全相同，所以可以在物理内存上共享相同的空间。所以A1的逻辑块block0/1分别指向物理块block7/1；A2的逻辑块block0/1分别指向物理块block7/1。我们设每个物理块下映射的逻辑块数量为<strong>refcount</strong>，所以对物理块block7/1来说，它们的ref count都为2。 -decode阶段：A1和A2各自做推理，得到第一个token，分别为fathers和mothers。-将生成的token装入逻辑块：对于A1和A2来说，将其生成的token装入各自的逻辑块block1。-触发物理块<strong>copy-on-write</strong>机制：由于fathers/mothers是两个完全不同的token，因此对物理块block1触发复制机制，即在物理内存上新开辟一块空间。此时物理块block1只和A2的逻辑块block1映射，将其refcount减去1；物理块block3只和A1的逻辑块block1映射，将其refcount设为1。</p><p>总结起来，vLLM节省KV cache显存的核心思想是，对于相同数据对应的KVcache，能复用则尽量复用；无法复用时，再考虑开辟新的物理空间。</p><h5 id="beam-search">Beam Search</h5><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241206005411.png"></p><p>我们从右往左来看这张图。虚线位置表示“当前decoding时刻”，beam width =4。图中所有的block皆为逻辑块。</p><p>因为beam width = 4，这意味着根据beamsearch算法，在当前阶段我们生成了top4个概率最大的token（我们记这4个token为beam candidate0/1/2/3），它们分别装在block5，block6，block7和block8中。</p><p>现在我们继续使用beam search算法做decoding，继续找出top4个最可能的next token。经过我们的计算，这top 4 next token，有2个来自beamcandidate 1，有2个来自beam candidate2。因此我们在block6中引出block9和block10，用于装其中两个top 2 nexttoken；对block7也是同理。</p><p>现在，block9/10/11/12中装的top 4 next token，就成为新的beamcandidates，可以按照和上述一样的方式继续做beamsearch算法。而对于block5和block8，它们已经在beamsearch的搜索算法中被淘汰了，后续生成的token也不会和它们产生关系，所以可以清除掉这两个逻辑块，并释放它们对应的物理块的内存空间。</p><p>好，我们继续往左边来看这幅图。block3引出block5/6/7，block4引出block8，这意味着当前这4个top4token，是上一个timestep下candidate1和candidate3相关序列生成的（candidate0和2的block没有画出，是因为它们所在的序列被beamsearch算法淘汰了，因此没有画出的必要）。由于block8已经被淘汰，所以block4也相继被淘汰，并释放对应的物理内存空间。</p><p>由此往左一路推，直到block0为止（block0代表着prompt，因此被beamseach中所有的序列共享）。这一路上，我们都<strong>根据最新时刻的beamsearchdecoding结果，释放掉不再被需要的逻辑块和对应的物理内存空间</strong>，达到节省显存的目的。</p><h3 id="调度和抢占">调度和抢占</h3><p>到目前为止，我们已经解答了“vLLM 如何优化 KV cache的显存分配”这一问题。接下来，我们将探讨另一个关键问题：当采用动态显存分配策略时，虽然表面上可以同时处理更多的prompt，但由于没有为每个 prompt预留充足的显存空间，<strong>如果某一时刻显存被完全占满，而所有正在运行的prompt 都尚未完成推理，系统又该如何应对呢？</strong></p><p>vllm 的调度和抢占原则：</p><ul><li>采用“<strong>先到先服务</strong>”（FCFS）的调度策略来处理所有请求，确保公平性并防止请求饿死。</li><li>当 vLLM需要抢占请求时，它会优先服务最早到达的请求，并优先抢占最新到达的请求。暂停它们的执行，同时将与之相关的KV cache 从 gpu 上释放掉。等 gpu 资源充足时，重新恢复它们的执行。</li></ul><h4 id="swapping交换策略">Swapping交换策略</h4><p>对于被抢占的请求，vLLM要将其KV cache从gpu上释放掉，那么：</p><p><strong>问题1：该释放哪些KV cache？</strong>由前文PagedAttention原理可知，一个请求可能对应多个block。我们既可以选择释放掉部分block，也可以选择释放掉全部block，或者更科学地，我们可以预测一下哪些block被使用的频率最低，然后释放掉这些低频block（但这种方式实现起来难度较大，性价比不是很高）。</p><p>在vLLM中，采取的是all-or-nothing策略，即释放被抢占请求的所有block。</p><p><strong>问题2：要把这些KV cache释放到哪里去？</strong>对于这些被选中要释放的KVblock，如果将它们直接丢掉，那未免过于浪费。vLLM采用的做法是将其从gpu上交换（Swap）到cpu上。这样等到gpu显存充份时，再把这些block从cpu上重载回来。</p><h4 id="recomputing重计算策略">Recomputing重计算策略</h4><p>知道了Swapping机制，重计算的过程也很好理解了：对于有些任务，当它们因为资源不足而被抢占时，可以不做swap，而是直接释放它们的物理块，把它们重新放入等待处理的队列中，等后续资源充足时再重新从prefill阶段开始做推理。</p><p>比如parallelsampling中并行采样数n=1的任务，对于这种小粒度任务，直接重计算（从prefill 阶段重新开始）所需的成本可能比执行 swapping（迁移内存到CPU）并在未来重新加载更低。Swapping 的过程会将内存页（或张量）从 GPU转移到 CPU，这涉及到 PCIe 或 NVLink 通信，速度远低于 GPU内部计算和内存操作。</p><h3 id="分布式管理">分布式管理</h3><p>最后，我们一起来看一下vLLM的整体架构。 <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241206012807.png"></p><p>在分布式场景下，vLLM 的整体运作流程如下：</p><ul><li>中央调度器（Scheduler）：vLLM配备了一个中央调度器，负责计算并管理每张显卡上 KV cache从逻辑块到物理块的映射表（block tables）。</li><li>映射表广播：在进行分布式计算时，Scheduler会将生成的映射表广播到各显卡。</li><li>缓存引擎管理：每张显卡上的 Cache Engine在接收到对应的映射信息后，负责具体管理该显卡上的 KV block。</li></ul><p>上图中给出的例子，是用张量模型并行（megatron-lm）做分布式推理时的情况，所以图中每个worker上写的是modelshard。<strong>在张量并行中，各卡上的输入数据相同，只是各卡负责计算不同head的KVcache</strong>。所以这种情况下，各卡上的逻辑块-物理块的映射关系其实是相同的（用的同一张blocktable），只是各卡上物理块中实际存储的数据不同而已。</p><p>参考资料： &gt; 极智AI |大模型优化技术PagedAttention：https://juejin.cn/post/7290163879287881765</p><blockquote><p>图解大模型计算加速系列之：vLLM核心技术PagedAttention原理：https://zhuanlan.zhihu.com/p/691038809</p></blockquote><blockquote><p>LLM推理优化 -PagedAttention：https://www.yidoo.xyz/paged-attention</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> LLM </tag>
            
            <tag> PagedAttention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型优化--KV Cache</title>
      <link href="/2024/12/02/da-mo-xing-you-hua-kv-cache/"/>
      <url>/2024/12/02/da-mo-xing-you-hua-kv-cache/</url>
      
        <content type="html"><![CDATA[<h2 id="kv-cache-介绍">KV Cache 介绍</h2><p>KVCache是Transformer标配的推理加速功能，transformer官方use_cache这个参数默认是True，但是它只能用于Decoder架构的模型，这是因为Decoder有CausalMask，在推理的时候前面已经生成的字符不需要与后面的字符产生attention，从而使得前面已经计算的K和V可以缓存起来。</p><p>我们先看一下不使用KVCache的推理过程。假设模型最终生成了“遥遥领先”4个字。</p><p>当模型生成第一个“遥”字时，<code>input="&lt;s&gt;"</code>,<code>"&lt;s&gt;"</code>是起始字符。Attention的计算如下： <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202155542.png"></p><p>为了看上去方便，我们暂时忽略scale项<span class="math inline">\(1/\sqrt(d)\)</span>，但是要注意这个scale面试时经常考。</p><p>如上图所示，最终Attention的计算公式如下，（softmaxed表示已经按行进行了softmax）: <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202155748.png"></p><p>当模型生成第二个“遥”字时，<code>input="&lt;s&gt;遥"</code>,Attention的计算如下： <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202155837.png"></p><p>当QK变为矩阵时，softmax 会针对 行 进行计算。写详细一点如下，softmaxed表示已经按行进行了softmax。</p><p><strong>（关键）由于decoder架构的模型有Causal Mask，所以<span class="math inline">\(Q_1\)</span>与<span class="math inline">\(K_2\)</span>的计算结果为<span class="math inline">\(-\infty\)</span>。</strong> <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202204856.png"></p><p>假设<span class="math inline">\(Att_1\)</span>表示 Attention的第一行， <span class="math inline">\(Att_2\)</span>表示 Attention的第二行，则根据上面推导，其计算公式为： <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202205002.png"></p><p>我们发现： - <span class="math inline">\(Q_1\)</span>在第二步参与的计算与第一步是一样的，而且第二步生成的<span class="math inline">\(Att_1\)</span>仅仅依赖于<span class="math inline">\(Q_1\)</span>，与<span class="math inline">\(Q_2\)</span>毫无关系。 - <span class="math inline">\(Att_2\)</span>仅仅依赖于<span class="math inline">\(Q_2\)</span>，与<span class="math inline">\(Q_1\)</span>毫无关系。</p><p>当模型生成第三个“领”字时，<code>input="&lt;s&gt;遥遥"</code>,Attention的计算如下： <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202210952.png">详细的推导参考第二步，其计算公式为： <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202211018.png">同样的，第三步生成的<span class="math inline">\(Att_3\)</span>仅仅依赖于<span class="math inline">\(Q_3\)</span>，与<span class="math inline">\(Q_1\)</span>和<span class="math inline">\(Q_2\)</span>毫无关系。</p><p>当模型生成第四个“先”字时，<code>input="&lt;s&gt;遥遥领"</code>,Attention的计算如下： <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202211156.png"><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202211222.png">和之前类似，不再赘述。</p><p>看上面图和公式，我们可以得出结论： - 当前计算方式存在大量冗余计算 -<span class="math inline">\(Attn_k\)</span>只与<span class="math inline">\(Q_k\)</span>有关 - 推理第<span class="math inline">\(x_k\)</span>个字符时，只需要输入字符<span class="math inline">\(x_{k-1}\)</span>即可。第三个结论的前提是，我们需要把每一步的K和V缓存起来，这样在推理第<span class="math inline">\(x_k\)</span>个字符时，只需要输入字符<span class="math inline">\(x_{k-1}\)</span>计算其<span class="math inline">\(Q_k,K_k,V_k\)</span>, 结合之前保存的KVCache即可得到对应的<span class="math inline">\(Attn_k\)</span>。</p><p>下图展示了使用KV Cache和不使用KV Cache的过程对比： <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202211901.png"></p><p><a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L318C1-L331C97">huggingface的实现</a> </p><pre class="line-numbers language-python" data-language="python"><code class="language-python">if layer_past is not None:        past_key, past_value = layer_past        key = torch.cat((past_key, key), dim=-2)        value = torch.cat((past_value, value), dim=-2)        if use_cache is True:        present = (key, value)    else:        present = None        if self.reorder_and_upcast_attn:        attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)    else:        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)</code></pre> ## KV Cache 步骤 正是因为 Self Attention 中带Maske ，因此，在推理的时候，前面已经生成的 token 不需要与后面的 token计算 Attention ，从而使得前面已经计算的 K 和 V 可以缓存起来。一个典型的带有 KV Cache 的推理过程包含以下两个阶段： 1.预填充阶段：输入一个 prompt 序列，为每个 transformer 层生成 Key Cache 和Value Cache（KV cache）。 2. 解码阶段：使用并更新 KVCache，一个接一个地生成 token，当前生成的 token依赖于之前已经生成的token。 ### 预填充阶段 <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241204023238.png">### 解码阶段 <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241204023814.png"><p></p><p>KVCache采用动态分配缓冲区大小，当超过当前容量时，内存大小会翻倍。这种方法虽然可行，但在GPU上频繁申请和释放内存的开销较大，导致效率较低。目前流行的解决办法是数据拆分与元数据管理：将数据按最小单元存储，并使用元数据记录每一块数据的位置，称为PageAttention。程序在初始化时申请一块较大的显存区域（例如4GB），然后按照 KVCache 的大小将显存划分成多个小块，并记录每个 token在推理过程中需要访问的小块。显存的分配、释放和管理类似于操作系统对物理内存的虚拟化过程。这一思路被vLLM（具体参见论文 Efficient Memory Management for Large Language ModelServing withPagedAttention）所采用，并广泛应用于大规模语言模型的推理中。</p><h2 id="mqa与gqa">MQA与GQA</h2><p>在GPU上部署模型时，我们遵循的原则是：能在一张卡上部署的，就不要跨多张卡；能在一台机器上部署的，就不要跨多台机器。这是因为“卡内通信带宽&gt; 卡间通信带宽 &gt;机间通信带宽”。由于“木桶效应”，模型部署时跨的设备越多，受到设备间通信带宽的制约就越大。</p><p>因此，减少 KV Cache 的目的是为了在更少的设备上推理更长的Context，或者在相同的 Context 长度下实现更大的推理 batchsize，从而提升推理速度或增加吞吐总量。最终目的都是为了降低推理成本。</p><h3 id="mha">MHA</h3><p>MHA（Multi-Head Attention），也就是多头注意力，是 Transformer中的标准 Attention 形式。在数学上，多头注意力 MHA等价于多个独立的单头注意力的拼接。其遵循前面所讲的KV Cache的原理。而后面的 MQA、GQA、都是围绕“如何减少 KV Cache同时尽可能地保证效果”这个主题发展而来的产物。</p><h3 id="mqa">MQA</h3><p>MQA，即“Multi-Query Attention”，2019年由 Google 在论文 <a href="chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/1911.02150">FastTransformer Decoding: One Write-Head is All You Need</a> 中提出。</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241204024825.png">使用 MQA 的模型包括 PaLM、StarCoder、Gemini 等。很明显，MQA 直接将 KVCache 减少到了原来的 1/head_num。</p><p>效果方面，目前看来大部分任务的损失都比较有限。</p><h3 id="gqa">GQA</h3><p>也有人担心 MQA 对 KV Cache的压缩太严重，以至于会影响模型的学习效率以及最终效果。为此，一个 MHA 与MQA 之间的过渡版本 GQA（Grouped-Query Attention）应运而生，出自 2023 年Google 的论文 <a href="https://arxiv.org/abs/2305.13245">GQA: TrainingGeneralized Multi-Query Transformer Models from Multi-HeadCheckpoints</a>。 <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241203015549.png"></p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241204025114.png"></p><table><thead><tr class="header"><th>模型</th><th>参数量</th><th>非Embedding参数量</th><th>GQA</th><th>上下文长度</th></tr></thead><tbody><tr class="odd"><td>Qwen2-0.5B</td><td>0.49B</td><td>0.35B</td><td>√</td><td>32K</td></tr><tr class="even"><td>Qwen2-1.5B</td><td>1.54B</td><td>1.31B</td><td>√</td><td>32K</td></tr><tr class="odd"><td>Qwen2-7B</td><td>7.07B</td><td>5.98B</td><td>√</td><td>128K</td></tr><tr class="even"><td>Qwen2-57B-A14B</td><td>57.41B</td><td>56.32B</td><td>√</td><td>64K</td></tr><tr class="odd"><td>Qwen2-72B</td><td>72.71B</td><td>70.21B</td><td>√</td><td>128K</td></tr></tbody></table><p>在 Llama 2/3-70B 中，GQA 的 g=8 ，其他用了 GQA的同体量模型基本上也保持了这个设置，这并非偶然，而是同样出于推理效率的考虑。我们知道，70B这个体量的模型，如果不进行极端的量化，那么不可能部署到单卡（A100/H10080G）上。单卡不行，那么就能单机了，一般情况下一台机可以装 8张卡，Attention 的每个Head 实际上是独立运算然后拼接起来的，当 g=8时，正好可以每张卡负责计算一组 K、V 对应的 AttentionHead，这样可以在尽可能保证 K、V 多样性的同时最大程度上减少卡间通信。</p><p>下面看一下 GQA 的实验效果。 | 模型 | 推理时间 | 效果 | | ---------- |-------- | ----- | | MHA-Large | 0.37 | 46.0 | | MHA-XXL | 1.51 | 47.2 || MQA-XXL | 0.24 | 46.6 | | GQA-8-XXL | 0.28 | 47.1 |</p><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241204025823.png"></p><p>参考： &gt; https://zhuanlan.zhihu.com/p/708120479</p><blockquote><p>大模型推理加速：看图学KV Cachehttps://zhuanlan.zhihu.com/p/662498827</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> LLM </tag>
            
            <tag> KV Cache </tag>
            
            <tag> MQA </tag>
            
            <tag> GQA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型显存占用分析</title>
      <link href="/2024/11/30/da-mo-xing-xian-cun-zhan-yong-fen-xi/"/>
      <url>/2024/11/30/da-mo-xing-xian-cun-zhan-yong-fen-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="大模型消耗的显存">大模型消耗的显存</h2><p>在详细说明大模型需要消耗的显存大小之前我们需要先明确几个概念。一个就是大模型在不同阶段对显存的消耗是不同的。但是大致可以分为三个阶段或者说三个场景。即大模型<strong>预训练阶段</strong>、大模型<strong>微调阶段</strong>和大模型<strong>推理阶段</strong>。-在<strong>预训练阶段</strong>，大模型通常选择较大规模的数据集获取泛化能力，因此需要较大的批次等来保证模型的训练强大。而模型的权重也是从头开始计算，因此通常也会选择高精度（如32位浮点数）进行训练。需要消耗大量的GPU显存资源。-在<strong>微调阶段</strong>，通常会冻结大部分参数，只训练小部分参数。同时，也会选择非常多的优化技术和较少的高质量数据集来提高微调效果，此时，由于模型已经在预训练阶段进行了大量的训练，微调时的数值误差对模型的影响通常较小。也常常选择16位精度或者混合精度训练。因此通常比预训练阶段消耗更低的显存资源。-在<strong>推理阶段</strong>，通常只是将一个输入数据经过模型的前向计算得到结果即可，因此需要最少的显存即可运行。### 模型权重这部分显存用于存储神经网络模型的参数，包括权重（weights）和偏置（biases）。模型内存是模型在训练和推理过程中都需要的，因为它包含了模型的结构和学习到的知识。在训练过程中，模型内存的大小通常与模型的复杂度和参数数量成正比。### 梯度在模型训练反向传播（Backward）过程中，计算的梯度所占的显存大小。梯度内存的大小与模型的参数数量有关，因为每个参数都需要计算对应的梯度。### 优化器状态优化器内存用于存储优化器状态，这通常包括梯度的一阶和二阶矩（如在Adam优化器中使用的均值和方差估计）优化器内存的大小取决于所使用的优化器类型。例如，Adam优化器需要额外的内存来存储梯度的一阶和二阶矩，而SGD只需要存储梯度信息，无其他优化器内存占用。### 激活值激活内存用于存储神经网络在前向传播过程中计算的中间激活值。这些激活值在反向传播过程中需要被重用，以计算关于模型参数的梯度。激活内存的大小与网络的深度和输入数据大小（batchsize）有关。更深的网络和更大的 batch size 会导致更大的激活内存需求。</p><h2 id="数据精度">数据精度</h2><p>想要计算显存，从“原子”层面来看，就需要知道我们的使用数据的精度，因为精度代表了数据存储的方式，决定了一个数据占多少bit。对于一个1B参数的模型，如果使用FP32精度存储，那么模型权重占用的显存就是1B* 2 = 2GB。</p><h3 id="常见精度类型">常见精度类型</h3><p>浮点数主要是由符号位（sign）、指数位（exponent）和小数位（mantissa）三部分组成。符号位都是1位（0表示正，1表示负），指数位影响浮点数范围，小数位影响精度。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202035151.png">- FP32：32位浮点数，每个数据占4字节 -TF32：<strong>19位浮点数</strong>，每个数据占2字节 -FP16：16位浮点数，每个数据占2字节 - BF16：16位浮点数，每个数据占2字节 -Int8：8位整数，每个数据占1字节 - Int4：4位整数，每个数据占0.5字节</p><h3 id="混合精度训练amp">混合精度训练AMP</h3><p>较低模型精度对于运算效率和显存占用都更友好，但是如果直接使用FP16精度在训练过程中会出现很多问题：- underflow：梯度再乘以学习率会很小，无法用fp16表示 - roundingerror：fp16各个区间之间存在gap，即使梯度可以用fp16表示，但是也没有把法加在fp16的权重上（被舍去）- 模型预测准确度降低 #### FP32权重备份：解决舍入误差问题保留一份FP32的主权重（Master-Weights），同时在训练中使用FP16存储权重、激活、梯度等数据。在参数更新的过程汇总，用FP16更新FP32的主权重。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202040432.png"></p><p>Step1:优化器会先备份一份FP32精度的模型权重，初始化好FP32精度的一阶和二阶动量（用于更新权重）。</p><p>Step2:开辟一块新的存储空间，将FP32精度的模型权重转换为FP16精度的模型权重。</p><p>Step3:运行forward和backward，产生的梯度和激活值都用FP16精度存储。</p><p>Step4:优化器利用FP16的梯度和FP32精度的一阶和二阶动量去更新备份的FP32的模型权重。</p><p>Step5:重复Step2到Step4训练，直到模型收敛。</p><p>我们可以看到训练过程中显存主要被用在四个模块上：</p><ul><li>模型权重本身（FP32+FP16）</li><li>梯度（FP16）</li><li>优化器（FP32）</li><li>激活值（FP16）</li></ul><p>写到这里，我们应该对于分析大模型训练时候的显存问题应该不在话下了（除了动态部分），那么我们就来实测一下，正在阅读的小伙伴也可以先自己尝试计算一下，看看是不是真的懂了。对于llama3.18B模型，FP32和BF16混合精度训练，用的是AdamW优化器，请问模型训练时占用显存大概为多少？</p><p>解：</p><p>模型参数：16（BF16） + 32（PF32）= 48G</p><p>梯度参数：16（BF16）= 16G</p><p>优化器参数：32（PF32） + 32（PF32）= 64G</p><p>不考虑激活值的情况下，总显存大约占用 （48 + 16 + 64） = 128G</p><h4 id="损失缩放解决数据下溢问题">损失缩放：解决数据下溢问题</h4><p>当采用FP16而不是FP32更新梯度时，由于值太小，会造成FP16精度下数据下溢的问题，一些梯度会变为0，导致模型不收敛。故采用在前向过程结束后对损失进行放大，在反向过程结束后对梯度进行缩小。损失缩放可以有两种主要方式：静态损失缩放和动态损失缩放。 -静态损失缩放：在训练开始前，设置一个固定的缩放因子，在训练过程中保持不变。- 动态损失缩放：在训练过程中，根据损失值的大小动态调整缩放因子。 -如果在某轮训练中检测到梯度正常且没有溢出，缩放因子会逐渐增大。 -如果检测到梯度出现 NaN 或 Inf，则缩放因子减小以防止数值不稳定。 ####精度累加此外，研究者还发现，可以在模型训练的过程中，使用FP16进行乘法预算，使用FP32进行累加运算，并将FP32转换为FP16存储。FP32可以弥补损失的精度，减少舍入误差。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202042004.png">如英伟达Volta架构中的TensorCore可以使用FP16混合精度进行加速，采用的是FP16的矩阵乘法，得出全精度乘积，然后使用FP32累加，将该乘积与其他中间乘积累加，减少因FP16带来的精度损失。#### 更为动态的精度缩放方法在英伟达最新的Hopper架构GPU中，英伟达的TensorCore能够自动根据所需的精度进行动态的数据缩放调整，特别是针对Transformer网络架构，能够在数据存入内存前，根据需求改变各种参数精度。<img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main/images/20241202042340.png">Hopper白皮书内容如下： &gt;在 Transformer 模型的每一层，TransformerEngine 都会分析 Tensor Core产生的输出值的统计数据。了解了接下来会出现哪种类型的神经网络层以及它需要什么精度后，TransformerEngine 还会决定将张量转换为哪种目标格式，然后再将其存储到内存中。 FP8的范围比其他数字格式更有限。为了优化使用可用范围，Transformer Engine还使用从张量统计数据计算的缩放因子动态地将张量数据缩放到可表示的范围内。因此，每一层都在其所需的范围内运行，并以最佳方式加速。## 其他显存占用 - KVCache：在推理过程中，大模型需要缓存一些中间结果，以便在处理下一个输入时重用。这些缓存的结果通常称为KVCache。KVCache占用的显存大小与模型的层数、序列长度和每个序列的token数量有关。 -显存碎片：显存碎片是指显存中未被使用的空闲空间，这些空闲空间可能无法被有效利用，导致显存利用率降低。pagedattention机制可以有效减少显存碎片。</p><h3 id="推理与kv-cache-显存">推理与KV cache 显存</h3><p>推理的时候，显存几乎只考虑模型参数本身，除此之外就是现在广泛使用的KVcache也会占用显存。KV cache与之前讲的如何减少显存不一样，KVcache的目的是减少延迟，也就是<strong>为了推理的速度牺牲显存</strong>。#### kv cache介绍 具体可以参考另一篇博客：<a href="https://baoblei.github.io/2024/12/02/da-mo-xing-you-hua-kv-cache/">大模型优化--KVCache</a> KVCache是Transformer标配的推理加速功能，transformer官方use_cache这个参数默认是True，但是它只能用于Decoder架构的模型，这是因为Decoder有CausalMask，在推理的时候前面已经生成的字符不需要与后面的字符产生attention，从而使得前面已经计算的K和V可以缓存起来。</p><p>下图展示了使用KV Cache和不使用KV Cache的过程对比： <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241202211901.png">从图中，我们可以得出结论： - 当前计算方式存在大量冗余计算 - <span class="math inline">\(Attn_k\)</span>只与<span class="math inline">\(Q_k\)</span>有关 - 推理第<span class="math inline">\(x_k\)</span>个字符时，只需要输入字符<span class="math inline">\(x_{k-1}\)</span>即可。第三个结论的前提是，我们需要把每一步的K和V缓存起来，这样在推理第<span class="math inline">\(x_k\)</span>个字符时，只需要输入字符<span class="math inline">\(x_{k-1}\)</span>计算其<span class="math inline">\(Q_k,K_k,V_k\)</span>, 结合之前保存的KVCache即可得到对应的<span class="math inline">\(Attn_k\)</span>。</p><p><a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L318C1-L331C97">huggingface的实现</a> </p><pre class="line-numbers language-python" data-language="python"><code class="language-python">if layer_past is not None:        past_key, past_value = layer_past        key = torch.cat((past_key, key), dim=-2)        value = torch.cat((past_value, value), dim=-2)        if use_cache is True:        present = (key, value)    else:        present = None        if self.reorder_and_upcast_attn:        attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)    else:        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)</code></pre><p></p><h4 id="kv-cache显存占用">KV Cache显存占用</h4><p>当<strong>sequence特别长</strong>的时候，KVCache其实还是个Memory刺客。</p><p>对于fp16精度保存的KV Cache，其占用的显存大小为： <span class="math display">\[memory = batch\_size * hidden\_size * seq\_length * layer * 2 * 2\]</span> 其中两个2分别表示K和V，fp16精度字节数。</p><p>比如llama 7B模型，batch_size=32, layer=32, dim_size=4096,seq_length=2048, float32类型，则需要占用的显存为 2 * 32 * 4096 * 2048 *32 * 4 / 1024/1024/1024 = 64G。</p><p>为了解决KVCache显存占用问题，研究者提出了<strong>MQA和GQA</strong>。其核心思想是：<strong>共享多头KVCache</strong>。 <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241203015549.png">以GQA为例，我们将hidden_size维度切分为head*head_dim，然后将多个head分成group组，每个group共享一个KV。则总的KVCache显存占用为： <span class="math display">\[memory = batch\_size * group * head\_dim * seq\_length * layer *  2 * 2\]</span> 而MQA则是group=1，即每个head单独保存一个KV。</p><blockquote><p>大模型推理加速：看图学KV Cachehttps://zhuanlan.zhihu.com/p/662498827</p></blockquote><h3 id="lora-与-qlora-训练显存">LoRA 与 QLoRA 训练显存</h3><h4 id="lora">LoRA</h4><p><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241204190635.png">LoRA是在原来的权重矩阵的旁路新建一对低秩的可训练权重，训练的时候只训练旁路，大大降低了训练的权重数量，参数量从dxd 降为 2xdxr。</p><p>有了前面的全参情况下训练的显存分析，现在分析起来就比较通顺了，我们一步一步来，还是以BF16半精度模型Adamw优化器训练为例子，lora部分的参数精度也是BF16，并且设1字节模型参数对应的显存大小<span class="math inline">\(\Phi\)</span>。</p><ul><li>首先是模型权重本身的权重，这个肯定是要加载原始模型和lora旁路模型的，因为lora部分占比小于2个数量级，所以显存分析的时候忽略不计，显存占用<span class="math inline">\(2\Phi\)</span>。</li><li>然后就是优化器部分，优化器也不需要对原模型进行备份了，因为优化器是针对于需要更新参数的模型权重部分进行处理，也就是说优化器只包含Lora模型权重相关的内容，考虑到数量级太小，也忽略不计，故优化器部分占用显存0。</li><li>原始模型都不更新梯度，肯定只需要Lora部分的梯度显存，而这部分占用显存也可以近似为0。 想深入探究的可以去看了一篇<a href="https://baoblei.github.io/2024/12/04/da-mo-xing-gao-xiao-wei-diao-fang-fa-peft-lora-qlora/">博文</a>和<a href="https://zhuanlan.zhihu.com/p/702629428">大模型高效微调-LoRA原理详解和训练过程深入分析</a>。</li></ul><p>总的来说，不考虑激活值的情况下，Lora微调训练的显存占用只有<span class="math inline">\(2\Phi\)</span>，一个7B的模型Lora训练只需要占用显存大约14G左右。验证一下，我们来看LlamaFactory里给出训练任务的显存预估表格： <img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md@main//images/20241205031352.png"></p><h4 id="qlora">QLoRA</h4><p>QLoRA本质上还是对模型的主体进行了量化，以4Bit量化为例，Qlora占用的显存主要就是4Bit量化后的模型本身也就是<span class="math inline">\(0.5\Phi\)</span>，由于A、B矩阵的参数量很小，故忽略不计。</p><h1 id="总结">总结</h1><table><colgroup><col style="width: 27%"><col style="width: 20%"><col style="width: 27%"><col style="width: 12%"><col style="width: 12%"></colgroup><thead><tr class="header"><th>部分显存对应精度（训练）</th><th>全参微调（全FP16）</th><th>全参微调（BF16混合精度）</th><th>LoRA</th><th>QLoRA</th></tr></thead><tbody><tr class="odd"><td>主干模型（模型存储/计算参数）</td><td>FP16/FP16</td><td>BF16/BF16</td><td>BF16/BF16</td><td>NF4/BF16</td></tr><tr class="even"><td>主干模型（梯度）</td><td>FP16</td><td>BF16</td><td>Null</td><td>Null</td></tr><tr class="odd"><td>主干模型（adamw优化器）</td><td>2 x FP16</td><td>3 x FP32</td><td>Null</td><td>Null</td></tr><tr class="even"><td>LoRA部分（可忽略不计）</td><td>Null</td><td>Null</td><td>BF16</td><td>BF16</td></tr><tr class="odd"><td>总和（大约）</td><td>8Byte</td><td>16Byte</td><td>2Byte</td><td>0.5Byte</td></tr></tbody></table><h2 id="huggingface-显存分析工具">huggingface 显存分析工具</h2><p>huggingface提供了一个工具可以方便的查看大模型在不同阶段消耗的显存大小。 <a href="https://huggingface.co/docs/accelerate/usage_guides/model_size_estimator">modelsize estimator</a></p><h2 id="参考资料">参考资料</h2><blockquote><p>https://blog.zhexuan.org/archives/llm-gpu-memory.html</p></blockquote><blockquote><p>https://juejin.cn/post/7352387675837480995</p></blockquote><blockquote><p>https://gitcode.csdn.net/662a062ca2b051225566cf63.html</p></blockquote><blockquote><p>https://hub.baai.ac.cn/view/16045</p></blockquote><blockquote><p>https://zhuanlan.zhihu.com/p/624740065</p></blockquote><blockquote><p>NVIDIA H100 Tensor Core GPUArchitecture：https://nvdam.widen.net/s/9bz6dw7dqr/gtc22-whitepaper-hopper</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLMs </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> KV Cache </tag>
            
            <tag> AI </tag>
            
            <tag> LoRA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo博客搭建</title>
      <link href="/2024/11/28/hexo-zhu-ti-next-pei-zhi/"/>
      <url>/2024/11/28/hexo-zhu-ti-next-pei-zhi/</url>
      
        <content type="html"><![CDATA[<h2 id="基础搭建">基础搭建</h2><h3 id="hexo">Hexo</h3><p>选一个博客框架，hexo是静态网站框架，基于nodejs，可以生成静态网页，部署到github上。需要提前安装<strong>git</strong>，<strong>nodejs</strong>。 - check gitversion: <code>git --version</code> - check nodejs version:<code>node -v</code> - check npm version: <code>npm -v</code> 安装 cnmp(optional) -<code>npm install -g cnpm --registry=https://registry.npm.taobao.org</code>- check cnmp version: <code>cnpm -v</code></p><p>安装 <strong>hexo-cli</strong> -<code>cnpm install -g hexo-cli</code> - check hexo version:<code>hexo -v</code></p><h3 id="初始化blog目录">初始化blog目录</h3><ul><li><code>mkdir blog | cd blog</code></li><li><code>sudo hexo init</code> # clone hexo-starter repo</li><li><code>hexo s</code> # start blog, default port 4000</li></ul><h3 id="添加博客">添加博客</h3><ul><li><code>hexo n "newblog"</code> # add new blog, save in /source/_posts<ul><li>edit blog in /source/_posts/newblog.md</li></ul></li><li><code>hexo clean</code> # clean cache</li><li><code>hexo g</code> # generate static files</li><li><code>hexo s</code> # start blog, default port 4000</li></ul><h3 id="github部署">github部署</h3><ul><li>build blog address:<ul><li>add new token in github</li><li>github new repo: https://<username>.github.io/</username></li></ul></li><li><code>cnpm install hexo-deployer-git --save</code><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml">deploy:    type: git    repo: https://&lt;token&gt;@github.com/&lt;username&gt;/&lt;username&gt;.github.io.git    branch: master</code></pre></li><li><code>hexo d</code> # deploy blog</li></ul><h2 id="主题">主题</h2><h3 id="安装">安装</h3><ul><li>theme 推荐：<ul><li>https://github.com/litten/hexo-theme-yilia.git</li><li>https://github.com/theme-next/hexo-theme-next.git</li></ul></li><li>clone theme into themes folder, e.g.<code>git clone https://github.com/theme-next/hexo-theme-next.git themes/next</code></li><li>edit _config.yml, set theme to next <pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml">theme: next</code></pre></li><li><code>hexo c</code>&amp;&amp;<code>hexo g</code>&amp;&amp;<code>hexo d</code># clean cache, generate static files, deploy blog</li></ul><h3 id="配置主题">配置主题</h3><p>大部分配置在<code>_config.yml</code> 文件中，可以参考<a href="https://theme-next.js.org/docs/getting-started/">官方文档</a> ####基础 ##### scheme - Scheme 是 NexT 提供的一种特性，借助于 Scheme，NexT为你提供多种不同的外观。同时，几乎所有的配置都可以 在 Scheme 之间共用。- muse - 默认 Scheme，这是 NexT 最初的版本，黑白主调，大量留白 - Mist -Muse 的紧凑版本，整洁有序的单栏外观 - Pisces - 双栏Scheme，小家碧玉似的清新 Scheme 的切换通过更改 <code>_config.yml</code>文件，搜索 scheme 关键字。 你会看到有三行 scheme 的配置，将你需用启用的scheme 前面注释 # 去除即可。 ##### language ##### menu菜单配置包括三个部分，第一是菜单项（名称和链接），第二是菜单项的显示文本，第三是菜单项对应的图标。NexT 使用的是 Font Awesome 提供的图标，可以在 <a href="https://fontawesome.com/icons">Font Awesome</a> 查看。</p><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml">menu:  home: / || home  archives: /archives || archive  tags: /tags || tags  categories: /categories || th</code></pre> #### 其他 主题美化是个逐渐积累的过程,后期可以在相册、挂件、评论、搜索、sitemap、rss等方面进行配置。一些工具插件： <strong>gallery page</strong>--justified gallery<p></p>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> next </tag>
            
            <tag> 个人静态博客搭建 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
