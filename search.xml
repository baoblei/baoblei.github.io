<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>大模型优化--KV Cache</title>
      <link href="/2024/12/02/da-mo-xing-you-hua-kv-cache/"/>
      <url>/2024/12/02/da-mo-xing-you-hua-kv-cache/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjYzMzkzOQ==&amp;mid=2247486250&amp;idx=1&amp;sn=060cb277a22558f8f2bae6578df75240&amp;chksm=ceed0816f99a81004a65fe94fa837efb12327138166e0238bc16822287aa24e341a854785f30&amp;token=861524234&amp;lang=zh_CN#rd">https://mp.weixin.qq.com/s?__biz=Mzg3MjYzMzkzOQ==&amp;mid=2247486250&amp;idx=1&amp;sn=060cb277a22558f8f2bae6578df75240&amp;chksm=ceed0816f99a81004a65fe94fa837efb12327138166e0238bc16822287aa24e341a854785f30&amp;token=861524234&amp;lang=zh_CN#rd</a><br><a href="https://juejin.cn/post/7290163879287881765">https://juejin.cn/post/7290163879287881765</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> KV Cache </tag>
            
            <tag> 显存优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型显存占用分析</title>
      <link href="/2024/11/30/da-mo-xing-xian-cun-zhan-yong-fen-xi/"/>
      <url>/2024/11/30/da-mo-xing-xian-cun-zhan-yong-fen-xi/</url>
      
        <content type="html"><![CDATA[<h1 id="大模型消耗的显存"><a href="#大模型消耗的显存" class="headerlink" title="大模型消耗的显存"></a>大模型消耗的显存</h1><p>在详细说明大模型需要消耗的显存大小之前我们需要先明确几个概念。<br>一个就是大模型在不同阶段对显存的消耗是不同的。但是大致可以分为三个阶段或者说三个场景。即大模型<strong>预训练阶段</strong>、大模型<strong>微调阶段</strong>和大模型<strong>推理阶段</strong>。<br>在<strong>预训练阶段</strong>，大模型通常选择较大规模的数据集获取泛化能力，因此需要较大的批次等来保证模型的训练强大。而模型的权重也是从头开始计算，因此通常也会选择高精度（如32位浮点数）进行训练。需要消耗大量的GPU显存资源。<br>在<strong>微调阶段</strong>，通常会冻结大部分参数，只训练小部分参数。同时，也会选择非常多的优化技术和较少的高质量数据集来提高微调效果，此时，由于模型已经在预训练阶段进行了大量的训练，微调时的数值误差对模型的影响通常较小。也常常选择16位精度或者混合精度训练。因此通常比预训练阶段消耗更低的显存资源。<br>在<strong>推理阶段</strong>，通常只是将一个输入数据经过模型的前向计算得到结果即可，因此需要最少的显存即可运行。</p><p>不同阶段的显存占用主要体现在以下几个方面：</p><h2 id="模型权重"><a href="#模型权重" class="headerlink" title="模型权重"></a>模型权重</h2><p>这部分显存用于存储神经网络模型的参数，包括权重（weights）和偏置（biases）。模型内存是模型在训练和推理过程中都需要的，因为它包含了模型的结构和学习到的知识。在训练过程中，模型内存的大小通常与模型的复杂度和参数数量成正比。</p><h2 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h2><p>在模型训练反向传播（Backward）过程中，计算的梯度所占的显存大小。梯度内存的大小与模型的参数数量有关，因为每个参数都需要计算对应的梯度。</p><h2 id="优化器状态"><a href="#优化器状态" class="headerlink" title="优化器状态"></a>优化器状态</h2><p>优化器内存用于存储优化器状态，这通常包括梯度的一阶和二阶矩（如在Adam优化器中使用的均值和方差估计）优化器内存的大小取决于所使用的优化器类型。例如，Adam优化器需要额外的内存来存储梯度的一阶和二阶矩，而SGD只需要存储梯度信息，无其他优化器内存占用。</p><h2 id="激活值"><a href="#激活值" class="headerlink" title="激活值"></a>激活值</h2><p>激活内存用于存储神经网络在前向传播过程中计算的中间激活值。这些激活值在反向传播过程中需要被重用，以计算关于模型参数的梯度。激活内存的大小与网络的深度和输入数据大小（batch size）有关。更深的网络和更大的 batch size 会导致更大的激活内存需求。</p><h1 id="数据精度"><a href="#数据精度" class="headerlink" title="数据精度"></a>数据精度</h1><p>想要计算显存，从“原子”层面来看，就需要知道我们的使用数据的精度，因为精度代表了数据存储的方式，决定了一个数据占多少bit。对于一个1B参数的模型，如果使用FP32精度存储，那么模型权重占用的显存就是1B * 2 = 2GB。</p><h2 id="常见精度类型"><a href="#常见精度类型" class="headerlink" title="常见精度类型"></a>常见精度类型</h2><p>浮点数主要是由符号位（sign）、指数位（exponent）和小数位（mantissa）三部分组成。 符号位都是1位（0表示正，1表示负），指数位影响浮点数范围，小数位影响精度。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md/images/20241202035151.png"></p><ul><li>FP32：32位浮点数，每个数据占4字节</li><li>TF32：<strong>19位浮点数</strong>，每个数据占2字节</li><li>FP16：16位浮点数，每个数据占2字节</li><li>BF16：16位浮点数，每个数据占2字节</li><li>Int8：8位整数，每个数据占1字节</li><li>Int4：4位整数，每个数据占0.5字节</li></ul><h2 id="混合精度训练AMP"><a href="#混合精度训练AMP" class="headerlink" title="混合精度训练AMP"></a>混合精度训练AMP</h2><p>较低模型精度对于运算效率和显存占用都更友好，但是如果直接使用FP16精度在训练过程中会出现很多问题：</p><ul><li>underflow：梯度再乘以学习率会很小，无法用fp16表示</li><li>rounding error：fp16各个区间之间存在gap，即使梯度可以用fp16表示，但是也没有把法加在fp16的权重上（被舍去）</li><li>模型预测准确度降低</li></ul><h3 id="FP32权重备份：解决舍入误差问题"><a href="#FP32权重备份：解决舍入误差问题" class="headerlink" title="FP32权重备份：解决舍入误差问题"></a>FP32权重备份：解决舍入误差问题</h3><p>保留一份FP32的主权重（Master-Weights），同时在训练中使用FP16存储权重、激活、梯度等数据。在参数更新的过程汇总，用FP16更新FP32的主权重。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md/images/20241202040432.png"></p><h3 id="损失缩放：解决数据下溢问题"><a href="#损失缩放：解决数据下溢问题" class="headerlink" title="损失缩放：解决数据下溢问题"></a>损失缩放：解决数据下溢问题</h3><p>当采用FP16而不是FP32更新梯度时，由于值太小，会造成FP16精度下数据下溢的问题，一些梯度会变为0，导致模型不收敛。故采用在前向过程结束后对损失进行放大，在反向过程结束后对梯度进行缩小。<br>损失缩放可以有两种主要方式：静态损失缩放和动态损失缩放。</p><ul><li>静态损失缩放：在训练开始前，设置一个固定的缩放因子，在训练过程中保持不变。</li><li>动态损失缩放：在训练过程中，根据损失值的大小动态调整缩放因子。<ul><li>如果在某轮训练中检测到梯度正常且没有溢出，缩放因子会逐渐增大。</li><li>如果检测到梯度出现 NaN 或 Inf，则缩放因子减小以防止数值不稳定。</li></ul></li></ul><h3 id="精度累加"><a href="#精度累加" class="headerlink" title="精度累加"></a>精度累加</h3><p>此外，研究者还发现，可以在模型训练的过程中，使用FP16进行乘法预算，使用FP32进行累加运算，并将FP32转换为FP16存储。FP32可以弥补损失的精度，减少舍入误差。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md/images/20241202042004.png"><br>如英伟达Volta架构中的Tensor Core可以使用FP16混合精度进行加速，采用的是FP16的矩阵乘法，得出全精度乘积，然后使用FP32累加，将该乘积与其他中间乘积累加，减少因FP16带来的精度损失。</p><h3 id="更为动态的精度缩放方法"><a href="#更为动态的精度缩放方法" class="headerlink" title="更为动态的精度缩放方法"></a>更为动态的精度缩放方法</h3><p>在英伟达最新的Hopper架构GPU中，英伟达的Tensor Core能够自动根据所需的精度进行动态的数据缩放调整，特别是针对Transformer网络架构，能够在数据存入内存前，根据需求改变各种参数精度。<br><img src="https://cdn.jsdelivr.net/gh/baoblei/imgs_md/images/20241202042340.png"><br>Hopper白皮书内容如下：</p><blockquote><p>在 Transformer 模型的每一层，Transformer Engine 都会分析 Tensor Core 产生的输出值的统计数据。了解了接下来会出现哪种类型的神经网络层以及它需要什么精度后，Transformer Engine 还会决定将张量转换为哪种目标格式，然后再将其存储到内存中。 FP8 的范围比其他数字格式更有限。为了优化使用可用范围，Transformer Engine 还使用从张量统计数据计算的缩放因子动态地将张量数据缩放到可表示的范围内。因此，每一层都在其所需的范围内运行，并以最佳方式加速。</p></blockquote><h1 id="其他显存占用分析"><a href="#其他显存占用分析" class="headerlink" title="其他显存占用分析"></a>其他显存占用分析</h1><ul><li>KV Cache：在推理过程中，大模型需要缓存一些中间结果，以便在处理下一个输入时重用。这些缓存的结果通常称为KV Cache。KV Cache占用的显存大小与模型的层数、序列长度和每个序列的token数量有关。</li><li>显存碎片：显存碎片是指显存中未被使用的空闲空间，这些空闲空间可能无法被有效利用，导致显存利用率降低。paged attention机制可以有效减少显存碎片。</li></ul><h1 id="huggingface-显存分析工具"><a href="#huggingface-显存分析工具" class="headerlink" title="huggingface 显存分析工具"></a>huggingface 显存分析工具</h1><p>huggingface 提供了一个工具可以方便的查看大模型在不同阶段消耗的显存大小。<br><a href="https://huggingface.co/docs/accelerate/usage_guides/model_size_estimator">model size estimator</a></p><blockquote><p><a href="https://blog.zhexuan.org/archives/llm-gpu-memory.html">https://blog.zhexuan.org/archives/llm-gpu-memory.html</a></p></blockquote><blockquote><p><a href="https://juejin.cn/post/7352387675837480995">https://juejin.cn/post/7352387675837480995</a></p></blockquote><blockquote><p><a href="https://gitcode.csdn.net/662a062ca2b051225566cf63.html">https://gitcode.csdn.net/662a062ca2b051225566cf63.html</a></p></blockquote><blockquote><p><a href="https://hub.baai.ac.cn/view/16045">https://hub.baai.ac.cn/view/16045</a></p></blockquote><blockquote><p>NVIDIA H100 Tensor Core GPU Architecture：<a href="https://nvdam.widen.net/s/9bz6dw7dqr/gtc22-whitepaper-hopper">https://nvdam.widen.net/s/9bz6dw7dqr/gtc22-whitepaper-hopper</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> 大模型 </tag>
            
            <tag> LLM </tag>
            
            <tag> 显存 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo博客搭建</title>
      <link href="/2024/11/28/hexo-zhu-ti-next-pei-zhi/"/>
      <url>/2024/11/28/hexo-zhu-ti-next-pei-zhi/</url>
      
        <content type="html"><![CDATA[<h1 id="基础搭建"><a href="#基础搭建" class="headerlink" title="基础搭建"></a>基础搭建</h1><h2 id="Hexo"><a href="#Hexo" class="headerlink" title="Hexo"></a>Hexo</h2><p>选一个博客框架，hexo是静态网站框架，基于nodejs，可以生成静态网页，部署到github上。<br>需要提前安装<strong>git</strong>，<strong>nodejs</strong>。</p><ul><li>check git version: <code>git --version</code></li><li>check nodejs version: <code>node -v</code></li><li>check npm version: <code>npm -v</code><br>安装 cnmp (optional) </li><li><code>npm install -g cnpm --registry=https://registry.npm.taobao.org</code></li><li>check cnmp version: <code>cnpm -v</code></li></ul><p>安装 <strong>hexo-cli</strong></p><ul><li><code>cnpm install -g hexo-cli</code></li><li>check hexo version: <code>hexo -v</code></li></ul><h2 id="初始化blog目录"><a href="#初始化blog目录" class="headerlink" title="初始化blog目录"></a>初始化blog目录</h2><ul><li><code>mkdir blog | cd blog</code></li><li><code>sudo hexo init</code>    # clone hexo-starter repo</li><li><code>hexo s</code>            # start blog, default port 4000</li></ul><h2 id="添加博客"><a href="#添加博客" class="headerlink" title="添加博客"></a>添加博客</h2><ul><li><code>hexo n "newblog"</code>  # add new blog, save in /source/_posts<ul><li>edit blog in /source/_posts/newblog.md</li></ul></li><li><code>hexo clean</code>        # clean cache</li><li><code>hexo g</code>            # generate static files</li><li><code>hexo s</code>            # start blog, default port 4000</li></ul><h2 id="github部署"><a href="#github部署" class="headerlink" title="github部署"></a>github部署</h2><ul><li>build blog address:<ul><li>add new token in github</li><li>github new repo: https://<username>.github.io/</username></li></ul></li><li><code>cnpm install hexo-deployer-git --save</code>  <pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml">deploy:    type: git    repo: https://&lt;token&gt;@github.com/&lt;username&gt;/&lt;username&gt;.github.io.git    branch: master</code></pre></li><li><code>hexo d</code>            # deploy blog</li></ul><h1 id="主题"><a href="#主题" class="headerlink" title="主题"></a>主题</h1><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><ul><li>theme 推荐：<ul><li><a href="https://github.com/litten/hexo-theme-yilia.git">https://github.com/litten/hexo-theme-yilia.git</a></li><li><a href="https://github.com/theme-next/hexo-theme-next.git">https://github.com/theme-next/hexo-theme-next.git</a></li></ul></li><li>clone theme into themes folder, e.g. <code>git clone https://github.com/theme-next/hexo-theme-next.git themes/next</code></li><li>edit _config.yml, set theme to next  <pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml">theme: next</code></pre></li><li><code>hexo c</code>&amp;&amp;<code>hexo g</code>&amp;&amp;<code>hexo d</code>            # clean cache, generate static files, deploy blog</li></ul><h2 id="配置主题"><a href="#配置主题" class="headerlink" title="配置主题"></a>配置主题</h2><p>大部分配置在<code>_config.yml</code> 文件中，可以参考<a href="https://theme-next.js.org/docs/getting-started/">官方文档</a></p><h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><h4 id="scheme"><a href="#scheme" class="headerlink" title="scheme"></a>scheme</h4><ul><li>Scheme 是 NexT 提供的一种特性，借助于 Scheme，NexT 为你提供多种不同的外观。同时，几乎所有的配置都可以 在 Scheme 之间共用。</li><li>muse - 默认 Scheme，这是 NexT 最初的版本，黑白主调，大量留白</li><li>Mist - Muse 的紧凑版本，整洁有序的单栏外观</li><li>Pisces - 双栏 Scheme，小家碧玉似的清新<br>Scheme 的切换通过更改 <code>_config.yml</code> 文件，搜索 scheme 关键字。 你会看到有三行 scheme 的配置，将你需用启用的 scheme 前面注释 # 去除即可。</li></ul><h4 id="language"><a href="#language" class="headerlink" title="language"></a>language</h4><h4 id="menu"><a href="#menu" class="headerlink" title="menu"></a>menu</h4><p>菜单配置包括三个部分，第一是菜单项（名称和链接），第二是菜单项的显示文本，第三是菜单项对应的图标。 NexT 使用的是 Font Awesome 提供的图标，可以在 <a href="https://fontawesome.com/icons">Font Awesome</a> 查看。</p><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml">menu:  home: / || home  archives: /archives || archive  tags: /tags || tags  categories: /categories || th</code></pre><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>主题美化是个逐渐积累的过程, 后期可以在相册、挂件、评论、搜索、sitemap、rss等方面进行配置。<br>一些工具插件：<br><strong>gallery page</strong>–justified gallery</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> next </tag>
            
            <tag> 静态网站搭建 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
